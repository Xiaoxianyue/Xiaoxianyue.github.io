---
title: çŸ¥è¯†ç‚¹å›é¡¾
icon: alias
date: 2025-4-15 16:48:16
author: XiaoXianYue
isOriginal: true
category: 
    - å¤§ä¸‰ä¸‹
    - å¤§æ•°æ®
tag:
    - å¤§ä¸‰ä¸‹
    - å¤§æ•°æ®
sticky: false
star: false
article: true
timeline: true
image: false
navbar: true
sidebarIcon: true
headerDepth: 5
lastUpdated: true
editLink: false
backToTop: true
toc: true
---

## é¢„æµ‹è€ƒé¢˜

### è®ºè¿°é¢˜

#### Question 1

> å¤§æ•°æ®çš„åŸºæœ¬ç‰¹å¾ï¼Ÿç‰¹å¾å«ä¹‰ï¼Ÿå¤§æ•°æ®æ˜¯ä»€ä¹ˆï¼Ÿ

**Volume** : Refers to massive amount of data **collected and generated** ay unprecedented scales.

**Velocity**: Fast data generation and **processing** speed. â€˜One second rulesâ€™ means providing data analysis result within 1 second level **time range**, otherwise the data will lose it value.

**Veracity**: it is hard to determine the accuracy and quality of data.

**Variety**: Structured, semi-structured, unstructured data such as document files and video and **audio** files  from web pages, social media, **sensors, smart terminal**.

**Value**: the data value density is relatively low. It is necessary to combine business logic and machine learning **algorithm** to mine the data value.



There is  currently no universally recognized definition of big data. it is a collection  of data that can not be captured managed and processed by conventional software tools within a certain time frame. It is a massive, high-growth, and diverse data that requires newly processing model to have stronger decision-making, insight discovery and process optimization capabilities.



#### Question 2

> å¤§æ•°æ®å¤„ç†åˆ†æçš„ä¸»è¦æµç¨‹

1. Data **acquisition**: it is the **foundation** of the data analysis, severing as the first step to acquire various information from different sources like web pages, social media and **forum**. The process is : sending requests, receiving responds, parsing content, saving data.
2. Data cleaning : is the fundamental task of data preprocess, make data suitable to analysis and application. It is a critical step, ensuring the **integrity** **consistency**, **timeliness** and **effectiveness** of data to support the subsequent data handle. The process is : analyzing, defining checking rules, detecting, handling, evaluation
3. Data transformation and merge: Data merge refers to **stack** multiple datasets into one dataset, collect multiple datasets into desired dataset, and map transform **discretize** the data. Data transform means **converting** data from one **format**, structure and representation to another to ensure the analysis **modeling** and integration with other data. 
4. Data annotation: aims to learn a model that can provide a labeled sequence as prediction of observed sequence. The input is â€¦ and output is â€¦ There are text annotation and image annotation and includes sequence tagging, relationship annotation, attribute annotation and category annotation, keypoint, **rectangle**, **polygon**, regionalâ€¦
5. Data analysis: extracting hidden information from large amount of data and identify the **inherent pattern** of studied objects. People understand, **judge**, make decision and take action based on data analysis. It includes **descriptive**, **diagnostic**, predictive  and directive analysis.
6. Data visualization: refers to the process of using computer **graphics** to transform **massive**, **dynamic**, high dimensional to visual format to support data analysis and **interaction**. It includes **scientific** and information  **visualization**, revealing the key **insight** through **intuitive** visuals.
7. Data storage: using computer hardware and software to store and **apply** data. In big data era, the data type and volume have grown **beyond** traditional capabilities, requiring new **technologies** such as distributed file systems and databases.

### ç®€ç­”é¢˜

#### Question 1

> csvæ ¼å¼

CSV is a kind of file format that stores tabular files. CSV is composed of records(rows), each record is a line of text. Every record contains fields, separated by commas.

#### Question 2

> Hadoop åŸºæœ¬ç»„ä»¶

1. Hadoop Distributed file systems: store data by splitting them into blocks and ensure the fault tolerance. Name nodes is the central sever managing the metadata and namespace and Data nodes store the data and operate the Name nodesâ€™ instructions.
2. Mapreduce: uses map and reduce tasks to **perform parallel data computation with key- value pairs.** JobTracker handles job **decomposition**, task **scheduling** and resource management, TaskTracker executes task and **report** status.
3. YARN. Manage cluster resource and schedules processing tasks. Resourcemanager manage resources, Nodemanager **execute** tasks, ApplicationMaster send **request** and container carries the **workload**.

#### Question 3

> æ•°æ®åˆ†æç®—æ³•

K-means

Randomly choose K **centroids**

Assign each data point to the nearest centroid.

Recompute the centroids as the mean of the assigned points

Repeat steps 2,3 until centroids no longer  change or a maximum number of iteration is reached.



## Question 1

> ä»€ä¹ˆæ˜¯å¤§æ•°æ®ï¼Ÿå¤§æ•°æ®ç‰¹å¾ï¼Ÿå¤§æ•°æ®åº”ç”¨æ–¹é¢ï¼Ÿå¤§æ•°æ®ç»“æ„ï¼Ÿ

1. ä»€ä¹ˆæ˜¯å¤§æ•°æ®

There is currently no universally recognized definition of big data. Big data refers to a collection of data that cannot be captured, managed, and processed using conventional software tools within a certain time frame. It is a massive, high growth, and diverse information asset that requires new processing models to have stronger decision-making, insight discovery, and process optimization capabilities.

å¤§æ•°æ®ç›®å‰è¿˜æ²¡æœ‰ç»Ÿä¸€å…¬è®¤çš„å®šä¹‰ã€‚å®ƒé€šå¸¸æŒ‡çš„æ˜¯é‚£ç§ä¸èƒ½é€šè¿‡ä¼ ç»Ÿè½¯ä»¶å·¥å…·åœ¨è§„å®šæ—¶é—´å†…å®Œæˆæ•æ‰ã€ç®¡ç†å’Œå¤„ç†çš„æ•°æ®é›†åˆã€‚è¿™ç±»æ•°æ®èµ„äº§å…·æœ‰åºå¤§çš„è§„æ¨¡ã€é«˜é€Ÿå¢é•¿å’Œå¤šæ ·åŒ–çš„ç‰¹å¾ï¼Œéœ€è¦æ–°å‹çš„å¤„ç†æ¨¡å¼ï¼Œä»¥æå‡å†³ç­–ã€æ´å¯Ÿå’Œæµç¨‹ä¼˜åŒ–çš„èƒ½åŠ›ã€‚

2. å¤§æ•°æ®ç‰¹å¾

Big data is characterized by several key features,   which can be summarized as the "5Vs": Volume, Velocity, Variety, Veracity, and Value. 

å¤§æ•°æ®çš„ç‰¹å¾é€šå¸¸è¢«æ¦‚æ‹¬ä¸ºâ€œ5ä¸ªVâ€ï¼šåˆ†åˆ«æ˜¯ä½“ç§¯ï¼ˆVolumeï¼‰ã€é€Ÿåº¦ï¼ˆVelocityï¼‰ã€å¤šæ ·æ€§ï¼ˆVarietyï¼‰ã€å‡†ç¡®æ€§æˆ–å¯ä¿¡åº¦ï¼ˆVeracityï¼‰ä»¥åŠä»·å€¼ï¼ˆValueï¼‰ã€‚

- Volume: In 2003, it will take 10 years for humans to complete the sorting of 3 billion base pairs; after 10 years, the gene analyzer can complete it just in 15 minutes. Refers to the massive amount of data generated and collected at unprecedented scales.
- Variety: Structured, semi-structured, and unstructured data such as document files, audio and video files from web pages, social media, sensors, and smart terminals.
- Velocity: Fast data generation and processing speed. The "1-second rule" refers to providing analysis results within a second level time range, otherwise the data will lose its value.
- Veracity: the large amount of data makes it difficult to determine the accuracy and reliability (data quality) of the data.
- Value: The data value density is relatively low, and it is necessary to combine business logic and machine learning algorithms to mine the data value.



## Question 2

> æ•°æ®æ¸…æ´—å½“ä¸­ä¼šå¯èƒ½åŒ…å«å“ªäº›æ•°æ®æ¸…æ´—çš„ä»»åŠ¡ï¼Ÿ

The basic process of data cleaning consists of five stepsï¼š**analyzing data, defining data cleaning rules, checking dirty data, processing dirty data, and evaluating data quality.**

<img src="./question.assets/image-20250415193532554.png" alt="image-20250415193532554" style="zoom:33%;" />



### Data cleaning process

- **Analyzing data**: understand the structure, content of raw data  to identify the quality issue.
- **Define cleaning rules**: how to detect and process dirty data in raw data. For example, how to detect outliers, how to handle outliers, how to handle missing values, etc.
- **Detect dirty data**: identify and mark dirty data using statistical analysis, data visualization, rule checking, model predictionâ€¦
- **Handling dirty data**: Taking corresponding measures to make rule data clean, accurate.
    - Consistency check is mainly focused on 
        - æ£€æŸ¥æ¯ä¸ªå˜é‡çš„åˆç†å–å€¼èŒƒå›´ï¼Œ
        - å˜é‡é—´çš„ç›¸äº’å…³ç³»ï¼Œ
        - æ£€æŸ¥å˜é‡åç§°æ˜¯å¦è§„èŒƒï¼Œ
        - æ˜¯å¦å­˜åœ¨å†²çªï¼Œ
        - å€¼æ˜¯å¦ç¬¦åˆè¦æ±‚ï¼Œ
        - è®°å½•ä¸­æ˜¯å¦å­˜åœ¨æ‹¼å†™é”™è¯¯ã€‚
        - SPSSã€SASã€Excel ç­‰æä¾›äº†ä¸€äº›åŸºæœ¬æ–¹æ³•æ¥è‡ªåŠ¨è¯†åˆ«æ¯ä¸ªå€¼æ˜¯å¦è¶…å‡ºèŒƒå›´ã€‚
        - æ•°æ®å½¢å¼ä¸­å¯èƒ½å‡ºç°å€¼ä¸å€¼ä¹‹é—´é€»è¾‘ä¸ä¸€è‡´çš„æƒ…å†µã€‚
    - ç¼ºå¤±å€¼å¤„ç†æ–¹æ³•ï¼š

        - **delete missing values**: This method  deletes data containing missing values directly, which is suitable for the situations where there are few missing values in the dataset and it does not affect the overall data analysis. 
        - **Fill in the missing values**: This method replaces the missing value  with another appropriate value. This method is suitable for situations where there are many missing values in the data or where deleting data affects overall data analysis.
            - ï¼ˆ1ï¼‰Mean imputationï¼šç”¨å‡å€¼å¡«è¡¥æ•°å€¼å‹æ•°æ®çš„ç¼ºå¤±å€¼ã€‚è¿™ç§æ–¹æ³•ç®€å•æ˜“è¡Œï¼Œä½†å®¹æ˜“é€ æˆå‡å€¼çš„æ¼‚ç§»ï¼Œå¿½ç•¥æ•°æ®çš„å˜å¼‚æ€§ã€‚
            - ï¼ˆ2ï¼‰Median imputationï¼šç”¨ä¸­ä½æ•°å¡«è¡¥æ•°å€¼å‹æ•°æ®çš„ç¼ºå¤±å€¼ã€‚
            - ï¼ˆ3ï¼‰Mode imputationï¼šç”¨ä¼—æ•°ï¼ˆå³å‡ºç°æ¬¡æ•°æœ€å¤šçš„å€¼ï¼‰å¡«è¡¥ç¼ºå¤±å€¼ã€‚
            - ï¼ˆ4ï¼‰Regression imputationï¼šé€šè¿‡å»ºç«‹å›å½’æ¨¡å‹æ¥é¢„æµ‹ç¼ºå¤±å€¼ã€‚
            - ï¼ˆ5ï¼‰Interpolation imputation: ä½¿ç”¨æ•°å­¦æ’å€¼æ–¹æ³•ä¼°è®¡ç¼ºå¤±å€¼ï¼Œç„¶åè¿›è¡Œå¡«è¡¥ã€‚çº¿æ€§æ’å€¼
            - ï¼ˆ6ï¼‰Special value : åœ¨æŸäº›ç‰¹å®šæƒ…å†µä¸‹ï¼Œå¯ä»¥ä½¿ç”¨ç‰¹æ®Šå€¼ï¼ˆä¾‹å¦‚ 0 æˆ– 1ï¼‰æ›¿æ¢ç¼ºå¤±å€¼ï¼Œä½†åŠ¡å¿…ç¡®ä¿è¿™äº›ç‰¹æ®Šå€¼èƒ½å¤Ÿä¸å…¶ä»–æ•°æ®åŒºåˆ†å¼€æ¥ã€‚
            - ï¼ˆ7ï¼‰ K-nearest neighbor method: åˆ©ç”¨æ ·æœ¬è§‚æµ‹å€¼ä¹‹é—´çš„ç›¸å…³æ€§æ¥å¡«å……ç¼ºå¤±å€¼ã€‚å¦‚æœä¸¤ä¸ªè§‚æµ‹å€¼ç›¸ä¼¼ï¼Œå¹¶ä¸”å…¶ä¸­ä¸€ä¸ªè§‚æµ‹å€¼åœ¨æŸäº›å˜é‡ä¸Šå­˜åœ¨ç¼ºå¤±å€¼ï¼Œåˆ™è¯¥ç¼ºå¤±å€¼å¾ˆå¯èƒ½ä¸å¦ä¸€ä¸ªè§‚æµ‹å€¼ç›¸ä¼¼ã€‚
    - é‡å¤å€¼å¤„ç† duplicate valuesï¼š

        - (1) Delete duplicate values: The simplest method is to directly delete duplicate values.  
        - (2) Merge duplicate values: This method is suitable for situations where duplicate values have similar information and merging them will not affect data analysis.
        - (3) Tag duplicate values:  Do not remove duplicate values, but add an tags on each duplicate record to indicate that it is duplicate
    - Outliersï¼š

        - æ£€æµ‹å¼‚å¸¸å€¼ï¼š

            - Statistical based methods
            - box plot
            - Distance based methods include K-Nearest Neighbors (KNN) and LocalOutlier Factor (LOF).

            - Cluster based methods
            - Machine learning based methods

        - å¤„ç†å¼‚å¸¸å€¼

            - The purpose of handling outliers is to ensure the accuracy and reliability of data analysis and modeling processes. 
            - Delete outliers
            - Replace Outliers
            - Group processing: å°†æ•°æ®åˆ†æˆä¸¤éƒ¨åˆ†ï¼Œä¸€éƒ¨åˆ†åŒ…å«å¼‚å¸¸å€¼ï¼Œå¦ä¸€éƒ¨åˆ†ä¸åŒ…å«ï¼Œå¹¶å¯¹æ¯ä¸€éƒ¨åˆ†æ•°æ®åˆ†åˆ«è¿›è¡Œå¤„ç†ã€‚è¿™å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šå‡å°‘å¼‚å¸¸å€¼å¯¹æ•´ä¸ªæ•°æ®é›†çš„å½±å“ã€‚
- Evaluation data quality
    - Helping to validate the effectiveness of data cleaning and whether data meets the expected quality standards. Find overlook.

## Question 

> ETL

<img src="./question.assets/image-20250415192141084.png" alt="image-20250415192141084" style="zoom:33%;" />

**ETL æ˜¯æ•´ä¸ªæµç¨‹çš„æ ¸å¿ƒå¤„ç†ç¯èŠ‚ï¼ŒåŒ…å«ä¸‰æ­¥ï¼š**

- **Eï¼šExtractï¼ˆæŠ½å–ï¼‰**
     ä»ä¸Šé¢å¤šä¸ªæ•°æ®æºä¸­æŠ½å–å‡ºéœ€è¦çš„æ•°æ®ï¼›
- **Tï¼šTransformï¼ˆè½¬æ¢ï¼‰**
     å¯¹æ•°æ®è¿›è¡Œæ¸…æ´—ã€æ ‡å‡†åŒ–ã€æ ¼å¼è½¬æ¢ç­‰å¤„ç†ï¼›
- **Lï¼šLoadï¼ˆè£…è½½ï¼‰**
     å°†å¤„ç†åçš„æ•°æ®åŠ è½½è¿›æ•°æ®ä»“åº“ã€‚

è¿™ä¸ªè¿‡ç¨‹ç¡®ä¿**æ•°æ®å˜å¾—ç»“æ„åŒ–ã€å¹²å‡€ï¼Œå¹¶å…·å¤‡ç»Ÿä¸€æ ‡å‡†**ã€‚

## Question

> æ•°æ®æ ‡å‡†åŒ–

ç›®çš„ï¼š

It is mainly used to process raw data structurally to conform to specific data models, standards, or conventions .

å®ƒä¸»è¦ç”¨äºå¯¹åŸå§‹æ•°æ®è¿›è¡Œç»“æ„å¤„ç†ï¼Œä½¿å…¶ç¬¦åˆç‰¹å®šçš„æ•°æ®æ¨¡å‹ã€æ ‡å‡†æˆ–è§„èŒƒã€‚

Ch transforé‚£å¼  P4

- Conversion type of dataï¼šconverting a string type to a numerical type for mathematical operations.
- Data structure adjustmentï¼šFor example, splitting a field containing complex data into multiple fields.
- ==Standardization of data== : Data standardization refers to the unified representation or unit for data to ensure consistency. For example, converting data with different units to the same unit, or unifying the representation of data into a specific standard format
- Standardization of fieldsï¼šåŒä¸€å­—æ®µå‘½åæˆ–æ ¼å¼





## Question 3

> æ•°æ®çš„å‡†å¤‡è¿‡ç¨‹ï¼Œæ¶‰åŠåˆ°å¯¹æ•°æ®å½¢å¼çš„æ“ä½œï¼Œå¦‚åˆå¹¶ï¼Œè¿½åŠ å¸¸ç”¨çš„æ–¹æ³•ï¼ŸPythonç›¸å…³æ“ä½œ

<img src="./question.assets/image-20250421155404698.png" alt="image-20250421155404698" style="zoom: 50%;" />

<img src="./question.assets/image-20250418203829976.png" alt="image-20250418203829976" style="zoom:80%;" />

<img src="./question.assets/image-20250421160056045.png" alt="image-20250421160056045" style="zoom:50%;" />

**æ·»åŠ è¡Œæ·»åŠ åˆ—**:

<img src="./question.assets/image-20250415212358323.png" alt="image-20250415212358323" style="zoom: 80%;" />

```python
df.loc["è‹±è¯­"] = np.random.randint(60,100,4)
df.loc["è‹±è¯­",:] = np.random.randint(60,100,4)
```

<img src="./question.assets/image-20250418204118411.png" alt="image-20250418204118411" style="zoom: 33%;" />

```python
df[:."å¼ è²"] = np.random.randint(60,100,4)
```

<img src="./question.assets/image-20250415213114823.png" alt="image-20250415213114823" style="zoom:80%;" />

åªèƒ½æ’å…¥åˆ—ï¼š

```python
df.insert(2,"éƒå»º", np.random.randint(60,100,4))
```

<img src="./question.assets/image-20250415213733737.png" alt="image-20250415213733737" style="zoom:80%;" />

### åˆå¹¶

`concat()`, `merge()`, å’Œ `join()` æ˜¯ pandas ä¸­å¸¸ç”¨çš„æ•°æ®åˆå¹¶å‡½æ•°ã€‚

`concat()`ï¼šæŒ‰è¡Œæˆ–æŒ‰åˆ—æ‹¼æ¥ Series æˆ– DataFrameã€‚

`merge()`ï¼šæ ¹æ®ä¸¤ä¸ªè¡¨çš„**åˆ—**è¿›è¡Œè¿æ¥åˆå¹¶ï¼ˆç±»ä¼¼ SQL ä¸­çš„ joinï¼‰ã€‚

`join()`ï¼šæ ¹æ®ä¸¤ä¸ªè¡¨çš„**ç´¢å¼•**è¿›è¡Œè¿æ¥åˆå¹¶ã€‚



#### concat()

| å‚æ•°å               | å«ä¹‰                                                         |
| -------------------- | ------------------------------------------------------------ |
| **objs**             | è¦è¿æ¥çš„æ•°æ®å¯¹è±¡åºåˆ—                                         |
| **axis**             | æŒ‡å®šè¿æ¥çš„è½´ï¼Œé»˜è®¤æ˜¯ 0ï¼ˆå³æ·»åŠ è¡Œï¼‰  1ï¼ˆæ·»åŠ åˆ—ï¼‰              |
| **join**             | è¿æ¥æ–¹å¼ï¼Œé»˜è®¤æ˜¯ "outer"ï¼ˆå¹¶é›†ï¼‰ï¼Œä¹Ÿå¯ä»¥è®¾ç½®ä¸º "inner"ï¼ˆäº¤é›†ï¼‰ |
| **ignore_index**     | æ˜¯å¦å¿½ç•¥åŸç´¢å¼•ã€‚é»˜è®¤ Falseï¼Œè®¾ä¸º True æ—¶é‡æ–°ç”Ÿæˆæ•´æ•°ç´¢å¼•     |
| **keys**             | å¤šå±‚ç´¢å¼•çš„å¤–å±‚æ ‡ç­¾                                           |
| **levels / names**   | æ„å»ºå¤šå±‚ç´¢å¼•æ—¶ï¼ŒæŒ‡å®šå±‚çº§é¡ºåºå’Œåç§°                           |
| **verify_integrity** | æ˜¯å¦æ£€æŸ¥è¿æ¥åæ˜¯å¦æœ‰é‡å¤ç´¢å¼•ï¼Œé»˜è®¤ False                     |
| **copy**             | æ˜¯å¦å¤åˆ¶æ•°æ®ï¼Œé»˜è®¤ Trueï¼Œè®¾ä¸º False æ—¶èŠ‚çœå†…å­˜ä½†éœ€æ³¨æ„æ•°æ®ç»“æ„å˜åŒ– |

==1æ˜¯æ·»åŠ åˆ—ï¼Œinnerä¸é‡å¤çš„è¡Œåˆ æ‰ï¼›0æ˜¯æ·»åŠ è¡Œï¼Œä¸é‡å¤çš„åˆ—åˆ æ‰==

```python
pd.concat([df1,df2],axis=1,join='outer') # 1è¡¨ç¤ºæ·»åŠ äº†åˆ—
```

<img src="./question.assets/image-20250415221118841.png" alt="image-20250415221118841" style="zoom:33%;" />

```python
pd.concat([dfl,df2],axis=l,join='inner') # æ²¡ç”¨çš„è¡Œåˆ æ‰
```

<img src="./question.assets/image-20250415221200995.png" alt="image-20250415221200995" style="zoom:33%;" />

```python
pd.concat([df1, df2], axis=0, join='outer') # 0è¡¨ç¤ºæ·»åŠ äº†è¡Œ
```

<img src="./question.assets/image-20250415221504955.png" alt="image-20250415221504955" style="zoom:33%;" />

```python
pd.concat([df1, df2], axis=0, join='inner') # æ²¡ç”¨çš„åˆ—åˆ æ‰
```

<img src="./question.assets/image-20250415221644898.png" alt="image-20250415221644898" style="zoom:33%;" />

#### append

<img src="./question.assets/image-20250415222736239.png" alt="image-20250415222736239" style="zoom: 50%;" />



#### Merge() 

| å‚æ•°                         | å«ä¹‰                                                         |
| ---------------------------- | ------------------------------------------------------------ |
| **left/right**               | å·¦/å³ä¾§å‚ä¸åˆå¹¶çš„ DataFrame                                  |
| **how**                      | åˆå¹¶æ–¹å¼ï¼š'inner'ï¼ˆäº¤é›†ï¼Œé»˜è®¤ï¼‰ï¼Œ'outer'ï¼ˆå¹¶é›†ï¼‰ï¼Œ'left'ï¼ˆå·¦è¿æ¥ï¼‰ï¼Œ'right'ï¼ˆå³è¿æ¥ï¼‰ |
| **on**                       | æŒ‡å®šç”¨äºåˆå¹¶çš„åˆ—åï¼Œå¿…é¡»åœ¨ä¸¤ä¸ªè¡¨ä¸­éƒ½å­˜åœ¨                     |
| **left_on / right_on**       | åˆ†åˆ«æŒ‡å®šå·¦å³è¡¨ä¸­ç”¨äºåˆå¹¶çš„åˆ—å                               |
| **left_index / right_index** | ä½¿ç”¨ç´¢å¼•ä½œä¸ºè¿æ¥é”®                                           |
| **sort**                     | æ˜¯å¦æŒ‰ç…§è¿æ¥é”®æ’åºï¼Œé»˜è®¤ True                                |
| **suffixes**                 | é‡ååˆ—çš„åç¼€ï¼Œé»˜è®¤ `_x`, `_y`                                |
| **indicator**                | æ˜¾ç¤ºè¡Œçš„æ¥æºï¼ˆä»…å·¦/å³/åŒæ—¶å­˜åœ¨ï¼‰ï¼Œè¾“å‡º `_merge` åˆ—           |



```python
df1 = pd.DataFrame({'åŸå¸‚':['åŒ—äº¬','ä¸Šæµ·','å¹¿å·'],'æ¸©åº¦':[22, 27, 32]})
df2 = pd.DataFrame({'åŸå¸‚':['åŒ—äº¬','ä¸Šæµ·','å¹¿å·'],'æ¹¿åº¦':[69, 78, 81]})
df = pd.merge(df1, df2, on='åŸå¸‚')  # ä»¥â€œåŸå¸‚â€ä¸ºè¿æ¥é”®åˆå¹¶ä¸¤ä¸ª DataFrame

df1 = pd.DataFrame({'åŸå¸‚':['åŒ—äº¬','ä¸Šæµ·','å¹¿å·','æˆéƒ½'],'æ¸©åº¦':[21,24,32,26]})
df2 = pd.DataFrame({'åŸå¸‚':['åŒ—äº¬','ä¸Šæµ·','æ­¦æ±‰'],'æ¹¿åº¦':[69, 78,80]})
df = pd.merge(df1, df2, on='åŸå¸‚')
```

- Output:

```python
åŸå¸‚   æ¸©åº¦  æ¹¿åº¦
åŒ—äº¬   22   69
ä¸Šæµ·   27   78
å¹¿å·   32   81

åŸå¸‚   æ¸©åº¦  æ¹¿åº¦
åŒ—äº¬   21   69
ä¸Šæµ·   24   78
```



```python
df1 = pd.DataFrame({'åŸå¸‚':['åŒ—äº¬','ä¸Šæµ·','å¹¿å·','æˆéƒ½'],'æ¸©åº¦':[21,24,32,26]})
df2 = pd.DataFrame({'åŸå¸‚':['åŒ—äº¬','ä¸Šæµ·','æ­¦æ±‰'],'æ¹¿åº¦':[69, 78,80]})
df = pd.merge(df1, df2, on='åŸå¸‚', how='left')
```

ä»¥å·¦è¡¨ä¸ºä¸»è¿›è¡Œè¿æ¥ï¼Œä¿ç•™å·¦è¡¨å…¨éƒ¨å†…å®¹

<img src="./question.assets/image-20250415224205696.png" alt="image-20250415224205696" style="zoom: 50%;" />



```python
df = pd.merge(df1, df2, on='åŸå¸‚', how='outer', indicator=True)
```

`how='outer'`ï¼šå¤–è¿æ¥ï¼ˆouter joinï¼‰â†’ ä¿ç•™ä¸¤ä¸ªè¡¨ä¸­æ‰€æœ‰æ•°æ®

`indicator=True`ï¼šä¼šæ·»åŠ  `_merge` åˆ—ï¼Œè¯´æ˜æ¯æ¡æ•°æ®æ¥æºäºå“ªä¸ªè¡¨

<img src="./question.assets/image-20250415224357574.png" alt="image-20250415224357574" style="zoom: 50%;" />



#### join() P66

| å‚æ•°                  | å«ä¹‰                                                |
| --------------------- | --------------------------------------------------- |
| **other**             | è¦åˆå¹¶çš„å¦ä¸€ä¸ª DataFrame æˆ– Seriesï¼Œæˆ–å®ƒä»¬çš„åˆ—è¡¨    |
| **on**                | ç”¨äºè¿æ¥çš„åˆ—åï¼ˆå¯é€‰ï¼‰                              |
| **how**               | è¿æ¥æ–¹å¼ï¼š'left'ï¼ˆé»˜è®¤ï¼‰ã€'right'ã€'outer'ã€'inner' |
| **lsuffix / rsuffix** | ç”¨äºé‡ååˆ—çš„åç¼€å¤„ç†                                |
| **sort**              | æ˜¯å¦æŒ‰é”®æ’åºï¼Œé»˜è®¤ Falseï¼ˆå³ä¿æŒåŸé¡ºåºï¼‰            |

<img src="./question.assets/image-20250415225210030.png" alt="image-20250415225210030" style="zoom:50%;" />

````python
# åˆ›å»ºå·¦ä¾§ DataFrame
left = pd.DataFrame({'Key': ['A', 'B'], 'Value': [1, 2]})

# åˆ›å»ºå³ä¾§ DataFrame
right = pd.DataFrame({'Key': ['A', 'C'], 'Data': [3, 4]})

# åŸºäº 'Key' åˆ—è¿›è¡Œè¿æ¥ï¼Œå…ˆè®¾ç½®ç´¢å¼•
result = left.set_index('Key').join(right.set_index('Key'), on='Key', how='left')
print(result)
````

![image-20250415225800115](./question.assets/image-20250415225800115.png)

```python
# å·¦è¡¨ï¼šç´¢å¼•æ˜¯ Aã€Bï¼Œåˆ—åæ˜¯ 'Value'
left = pd.DataFrame({'Value': [1, 2]}, index=['A', 'B'])

# å³è¡¨ï¼šç´¢å¼•æ˜¯ Aã€Cï¼Œåˆ—åä¹Ÿæ˜¯ 'Value'
right = pd.DataFrame({'Value': [3, 4]}, index=['A', 'C'])

# ä½¿ç”¨ outer æ–¹å¼åˆå¹¶ï¼Œè‡ªåŠ¨å¯¹é½ç´¢å¼•ï¼Œåˆ—åå†²çªæ—¶åŠ åç¼€
result = left.join(right, how='outer', lsuffix='_left', rsuffix='_right')
print(result)
```

- åˆå¹¶æ–¹å¼æ˜¯ `outer`ï¼šå¹¶é›†ï¼Œä¿ç•™å·¦å³è¡¨æ‰€æœ‰ç´¢å¼•ã€‚
- ç´¢å¼• A åŒæ—¶å‡ºç°åœ¨ä¸¤è¡¨ â†’ å¯¹åº”å€¼åˆ†åˆ«æ¥è‡ªå·¦å³è¡¨ã€‚
- ç´¢å¼• B åªåœ¨å·¦è¡¨ â†’ å³ä¾§å¡« NaNã€‚
- ç´¢å¼• C åªåœ¨å³è¡¨ â†’ å·¦ä¾§å¡« NaNã€‚
- åˆ—åå†²çªï¼šä¸¤è¾¹éƒ½æœ‰ `Value`ï¼ŒåŠ ä¸Šåç¼€ `_left` å’Œ `_right` æ¥åŒºåˆ†ã€‚

![image-20250415234553940](./question.assets/image-20250415234553940.png)

#### combine

æœ‰é‡åˆæƒ…å†µï¼š

![image-20250415235903261](./question.assets/image-20250415235903261.png)

æ²¡é‡åˆï¼š

![image-20250416000043759](./question.assets/image-20250416000043759.png)





## Question 4

> æ•°æ®æ ‡æ³¨çš„å½¢å¼ï¼Ÿ



#### Sequence tagging

**Name Entity Recognition** aims to **identify specific entities** (Name Entity) in text and the category which they belong to.

ğŸ“™ **å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰**æ˜¯è¯†åˆ«æ–‡æœ¬ä¸­å…·æœ‰ç‰¹å®šå«ä¹‰çš„å®ä½“ï¼Œå¹¶å°†å…¶åˆ†ç±»ï¼Œå¦‚äººåã€åœ°åã€ç»„ç»‡åç­‰ã€‚

**Part of speech tagging (POS)** is a form of text data tagging that can annotate entity names, entity attributes, and entity relationships of text content.
 ğŸ“™ **è¯æ€§æ ‡æ³¨ï¼ˆPOSï¼‰** æ˜¯ä¸€ç§æ ‡æ³¨å®ä½“ã€å±æ€§å’Œå…³ç³»çš„æ–¹å¼ã€‚

**Rhythm annotation** is to mark the position of rhythm symbols.
ğŸ“™ **éŸµå¾‹æ ‡æ³¨**æ˜¯æ ‡æ³¨è¯­éŸ³ä¸­çš„åœé¡¿ã€é‡éŸ³ç­‰éŸµå¾‹ä¿¡æ¯ã€‚

**Intention understanding** data is the process of collecting various user queries, categorizing them by domain, and labeling each sentence's intention, slot, and slot value.
ğŸ“™ **æ„å›¾è¯†åˆ«**æ˜¯é€šè¿‡é‡‡é›†ç”¨æˆ·çš„æŸ¥è¯¢è¯­å¥ï¼Œå¯¹å…¶è¿›è¡Œåˆ†ç±»ï¼ˆå¦‚é¢†åŸŸè¯†åˆ«ï¼‰ï¼Œå¹¶æ ‡æ³¨æ¯å¥çš„**æ„å›¾**ã€**æ§½ä½**åŠå…¶**æ§½å€¼**ã€‚

![image-20250418214305953](./question.assets/image-20250418214305953.png)



#### Relationship annotation

**Relationship annotation is marking the syntactic and semantic associations of complex sentences.**

ğŸ“™ **å…³ç³»æ ‡æ³¨**æ˜¯å¯¹å¤æ‚å¥å­ä¸­çš„**è¯­æ³•å’Œè¯­ä¹‰å…³ç³»**è¿›è¡Œæ ‡æ³¨ã€‚

<img src="./question.assets/image-20250416094710735.png" alt="image-20250416094710735" style="zoom:33%;" />

#### Attribute annotation

**Attribute annotation is the attributes annotation of objects in the text, and sentiment annotation is also a key content of attribute annotation, which labels the emotions corresponding to the original text.**
ğŸ“™ å±æ€§æ ‡æ³¨æ˜¯å¯¹æ–‡æœ¬ä¸­å¯¹è±¡çš„å±æ€§è¿›è¡Œæ ‡æ³¨ï¼Œè€Œæƒ…æ„Ÿæ ‡æ³¨ä¹Ÿæ˜¯å±æ€§æ ‡æ³¨çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå®ƒä¸ºåŸå§‹æ–‡æœ¬å¯¹åº”çš„æƒ…æ„Ÿæ‰“æ ‡ç­¾ã€‚

å±æ€§æ ‡æ³¨ä¸ä»…è¯†åˆ«â€œç”¨æˆ·åœ¨è¯´ä»€ä¹ˆâ€ï¼Œè¿˜è¦æ ‡å‡ºâ€œç”¨æˆ·åœ¨è¡¨è¾¾ä»€ä¹ˆæƒ…ç»ªã€é’ˆå¯¹å“ªä¸ªå±æ€§â€ï¼Œæ¯”å¦‚â€œå¯¹å”®åå¾ˆæ„¤æ€’â€ã€‚

![image-20250416095300930](./question.assets/image-20250416095300930.png)

ä¾‹å­ï¼š

<img src="./question.assets/image-20250416095614873.png" alt="image-20250416095614873" style="zoom:67%;" />



| å›¾ä¸­åˆ†æé¡¹       | æ ‡æ³¨å†…å®¹                                  | å¯¹åº”çš„æ•°æ®æ ‡æ³¨ç±»å‹             | å±äºçš„ç»†åŒ–ä»»åŠ¡ï¼ˆå­ç±»å‹ï¼‰                                   |
| ---------------- | ----------------------------------------- | ------------------------------ | ---------------------------------------------------------- |
| **åˆ†è¯**         | ä»Šå¤© / åŒ—äº¬ / æ˜¯ä¸æ˜¯ / ä¸‹é›¨               | **Sequence Tagging**           | åˆ†è¯ï¼ˆWord Segmentationï¼‰                                  |
| **è¯æ€§æ ‡æ³¨**     | ä»Šå¤©/nl åŒ—äº¬/ns æ˜¯ä¸æ˜¯/v ä¸‹é›¨/v           | **Sequence Tagging**           | è¯æ€§æ ‡æ³¨ï¼ˆPart-of-Speech Taggingï¼‰                         |
| **å‘½åå®ä½“æ ‡æ³¨** | ä»Šå¤©/TIME åŒ—äº¬/LOC                        | **Sequence Tagging**           | å‘½åå®ä½“è¯†åˆ«ï¼ˆNamed Entity Recognition, NERï¼‰              |
| **æ„å›¾æ ‡æ³¨**     | æ„å›¾ï¼šæŸ¥è¯¢æ°”è±¡-é›¨ï¼Œæ—¶é—´ï¼šä»Šå¤©ï¼ŒåŸå¸‚ï¼šåŒ—äº¬ | **Attribute Annotation**       | æ„å›¾è¯†åˆ«ï¼ˆIntent Understandingï¼‰ã€æ§½ä½æŠ½å–ï¼ˆSlot Fillingï¼‰ |
| **æƒ…æ„Ÿåˆ†æ**     | ä¸­æ€§                                      | **Attribute Annotation**       | æƒ…æ„Ÿæ ‡æ³¨ï¼ˆSentiment Annotationï¼‰                           |
| **è‹±æ–‡ç¿»è¯‘**     | Is it raining in Beijing today            | **å…¶ä»–æ‹“å±•ä»»åŠ¡ï¼ˆå¸¸ç”¨äºäº¤ä»˜ï¼‰** | æœºå™¨ç¿»è¯‘ / è¯­ä¹‰å¯¹é½ï¼ˆä¸å±äºæ ¸å¿ƒå››ç±»ï¼Œä½†å¸¸è§ï¼‰              |







## Question 5

> å¤§æ•°æ®åˆ†æï¼ŒåŸºäºåˆ†æç›®æ ‡ï¼Œèšç±»ï¼Œåˆ†ç±»ï¼Œé¢„æµ‹å›å½’ï¼Œçš„ç®—æ³•åŸç†ï¼Œæ²¡æœ‰åšé™åˆ¶ï¼Œç†Ÿç»ƒæŒæ¡ä¸€ç§åŸºæœ¬åŸç†å’ŒåŠŸèƒ½ã€‚

The Kernighan Lin algorithm aims to divide a network into two known sized communities based on greedy principle. 

### Step:

**Initial Partition**: Randomly divide the nodes into two equally sized sets A and B.

**Compute Node Scores**: For each node in A and B, calculate its internal edge weight (within the group) and external edge weight (to the other group).

**Select Node Pairs to Swap**: Identify a pair of nodes (one from each group) whose swap would yield the greatest reduction in total cross-group edge weight (i.e., cut value).

**Perform Swap**: Swap the selected nodes and record the gain from each swap.

**Repeat Optimization**: Repeat steps 2 to 4 until no further improvement can be made.

### Example

å»è‰ç¨¿æœ¬ä¸Šå¤ä¹ 





## Question 6

>  Python ä¸­çš„ request æ–¹æ³•èƒ½å®Œæˆä»€ä¹ˆæ ·çš„åŠŸèƒ½ï¼Œè·å–ä»€ä¹ˆä¸œè¥¿ï¼Ÿ

![image-20250418095931838](./question.assets/image-20250418095931838.png)

- **In addition, the Requests library also provides other types of request methods, such as PUT, DELETE, HEAD, and OPTIONS.**
     æ­¤å¤–ï¼Œ`requests` åº“è¿˜æ”¯æŒå…¶ä»–å‡ ç§è¯·æ±‚æ–¹æ³•ï¼Œä¾‹å¦‚ PUTã€DELETEã€HEAD å’Œ OPTIONSã€‚

<img src="./question.assets/image-20250418153144128.png" alt="image-20250418153144128" style="zoom: 67%;" />

<img src="./question.assets/image-20250418112404709.png" style="zoom:67%;" />

| å±æ€§           | PUT                                  | POST                                 |
| -------------- | ------------------------------------ | ------------------------------------ |
| **ç”¨é€”**       | ä¿®æ”¹æˆ–æ›¿æ¢èµ„æº                       | åˆ›å»ºæ–°èµ„æº                           |
| **å¹‚ç­‰æ€§**     | âœ… æ˜¯å¹‚ç­‰æ“ä½œï¼ˆå¤šæ¬¡ç›¸åŒè¯·æ±‚ç»“æœä¸å˜ï¼‰ | âŒ éå¹‚ç­‰æ“ä½œï¼ˆæ¯æ¬¡å¯èƒ½ç”Ÿæˆä¸åŒèµ„æºï¼‰ |
| **ä½¿ç”¨åœºæ™¯**   | æ›´æ–°ç”¨æˆ·ä¿¡æ¯ã€ä¸Šä¼ æ–‡ä»¶ï¼ˆè¦†ç›–å¼ï¼‰     | æäº¤è¡¨å•ã€æ–°å»ºç”¨æˆ·ã€è¯„è®ºç­‰           |
| **èµ„æºè·¯å¾„**   | é€šå¸¸æ˜¯æ˜ç¡®æŒ‡å®šçš„èµ„æºè·¯å¾„             | é€šå¸¸æ˜¯æœåŠ¡å™¨è‡ªåŠ¨åˆ†é…æ–°èµ„æºä½ç½®       |
| **è¯·æ±‚ä½“å†…å®¹** | åŒ…å«å®Œæ•´çš„èµ„æºæ•°æ®                   | åŒ…å«ç”¨äºåˆ›å»ºçš„å­—æ®µæ•°æ®               |

```python
response = requests.put('http://httpbin.org/put', data={'key': 'value'})
# PUT æ–¹æ³•ï¼šç”¨äºæ›´æ–°èµ„æº

response = requests.delete('http://httpbin.org/delete')
# DELETE æ–¹æ³•ï¼šç”¨äºåˆ é™¤èµ„æº

response = requests.head('http://httpbin.org/get')
# HEAD æ–¹æ³•ï¼šåªè·å–å“åº”å¤´ï¼ˆä¸åŒ…å«å†…å®¹ï¼‰

response = requests.options('http://httpbin.org/get')
# OPTIONS æ–¹æ³•ï¼šæŸ¥çœ‹æœåŠ¡å™¨æ”¯æŒå“ªäº›è¯·æ±‚æ–¹æ³•

```

- **`response.status_code`: If the status code returned is not 200, this method throws an exception.**
- **`response.status_code`ï¼š** å¦‚æœè¿”å›çš„çŠ¶æ€ç ä¸æ˜¯ 200ï¼ˆå³éæˆåŠŸè¯·æ±‚ï¼‰ï¼Œå¯èƒ½ä¼šæŠ›å‡ºå¼‚å¸¸ã€‚



### post get ä¸åŒ

#### **1. Data Transmission Methodsï¼ˆæ•°æ®ä¼ è¾“æ–¹å¼ï¼‰**ï¼š

- **GET Request**:
     Data is appended to the URL, which means the data is visible and limited in size.
     **GET è¯·æ±‚ï¼š** æ•°æ®é™„åŠ åœ¨ URL åé¢ï¼Œå› æ­¤æ•°æ®æ˜¯å¯è§çš„ï¼Œä¸”å¤§å°å—é™ã€‚

<img src="./question.assets/image-20250418113645121.png" alt="image-20250418113645121" style="zoom:50%;" />

- **POST Request**:
     Data is included in the request body, making it suitable for transmitting large amounts of data or sensitive information.
     **POST è¯·æ±‚ï¼š** æ•°æ®æ”¾åœ¨è¯·æ±‚ä½“ä¸­ï¼Œé€‚åˆä¼ è¾“å¤§é‡æ•°æ®æˆ–æ•æ„Ÿä¿¡æ¯ã€‚

```python
import requests

url = "https://httpbin.org/get"
params = {"search": "python", "page": 1}

response = requests.get(url, params=params)

print("GET è¯·æ±‚ç»“æœï¼š")
print(response.json())
```

```python
import requests

url = "https://httpbin.org/post"
data = {"username": "alice", "password": "123456"}

response = requests.post(url, data=data)

print("POST è¯·æ±‚ç»“æœï¼š")
print(response.json())
```



#### **2. Idempotencyï¼ˆå¹‚ç­‰æ€§ï¼‰**ï¼š

- **GET Request**:
     It is idempotent, meaning multiple identical requests will not change the server state.
     **GET è¯·æ±‚ï¼š** æ˜¯å¹‚ç­‰çš„ï¼Œæ„æ€æ˜¯å¤šæ¬¡å‘é€ç›¸åŒè¯·æ±‚ä¸ä¼šæ”¹å˜æœåŠ¡å™¨çŠ¶æ€ã€‚
- **POST Request**:
     It may alter the server state, such as creating a new resource or updating existing data.
     **POST è¯·æ±‚ï¼š** å¯èƒ½ä¼šæ”¹å˜æœåŠ¡å™¨çŠ¶æ€ï¼Œæ¯”å¦‚åˆ›å»ºæ–°èµ„æºæˆ–æ›´æ–°å·²æœ‰æ•°æ®ã€‚

#### **3. Securityï¼ˆå®‰å…¨æ€§ï¼‰**ï¼š

- **GET Request**:
     Data is exposed in the URL, making it unsuitable for transmitting sensitive information.
     **GET è¯·æ±‚ï¼š** æ•°æ®æš´éœ²åœ¨ URL ä¸­ï¼Œä¸é€‚åˆä¼ è¾“æ•æ„Ÿä¿¡æ¯ã€‚
- **POST Request**:
     Data is contained within the request body, providing relatively higher security.
     **POST è¯·æ±‚ï¼š** æ•°æ®åŒ…å«åœ¨è¯·æ±‚ä½“ä¸­ï¼Œç›¸å¯¹æ›´å®‰å…¨ã€‚

#### **4. Use Casesï¼ˆä½¿ç”¨åœºæ™¯ï¼‰**ï¼š

- **GET**:
     Ideal for retrieving public data, such as search results or webpage content.
     **GET è¯·æ±‚ï¼š** é€‚åˆè·å–å…¬å…±æ•°æ®ï¼Œæ¯”å¦‚æœç´¢ç»“æœã€ç½‘é¡µå†…å®¹ç­‰ã€‚
- **POST**:
     Suitable for submitting data that requires some processing, such as user login credentials or form submissions.
     **POST è¯·æ±‚ï¼š** é€‚åˆæäº¤éœ€è¦å¤„ç†çš„æ•°æ®ï¼Œæ¯”å¦‚ç”¨æˆ·ç™»å½•ä¿¡æ¯ã€è¡¨å•æäº¤ç­‰ã€‚

#### **5. Caching Behaviorï¼ˆç¼“å­˜è¡Œä¸ºï¼‰**ï¼š

- **GET Request**:
     Results can be cached, improving access speed.
     **GET è¯·æ±‚ï¼š** è¿”å›ç»“æœå¯ä»¥è¢«ç¼“å­˜ï¼Œæé«˜è®¿é—®é€Ÿåº¦ã€‚
- **POST Request**:
     Results are typically not cached.
     **POST è¯·æ±‚ï¼š** è¿”å›ç»“æœä¸€èˆ¬ä¸ä¼šè¢«ç¼“å­˜ã€‚

#### **6. Data Limitationsï¼ˆæ•°æ®é™åˆ¶ï¼‰**ï¼š

- **GET Request**:
     Data size is limited by the URL length constraints.
     **GET è¯·æ±‚ï¼š** æ•°æ®å¤§å°å—é™äº URL é•¿åº¦é™åˆ¶ã€‚
- **POST Request**:
     Data size generally has no restrictions, making it ideal for large data transmissions.
     **POST è¯·æ±‚ï¼š** æ•°æ®å¤§å°é€šå¸¸æ²¡æœ‰é™åˆ¶ï¼Œé€‚åˆä¼ è¾“å¤§æ•°æ®ã€‚

#### **7. HTTP Methodsï¼ˆHTTP æ–¹æ³•ï¼‰**

- **`requests.get` uses the HTTP GET method, which is used to retrieve resources from a server.**
     `requests.get` ä½¿ç”¨çš„æ˜¯ HTTP çš„ GET æ–¹æ³•ï¼Œç”¨äºä»æœåŠ¡å™¨è·å–èµ„æºã€‚
- **`requests.post` uses the HTTP POST method, which is used to submit data to a server.**
     `requests.post` ä½¿ç”¨çš„æ˜¯ HTTP çš„ POST æ–¹æ³•ï¼Œç”¨äºå‘æœåŠ¡å™¨æäº¤æ•°æ®ã€‚

```python
# ä½¿ç”¨ GET è¯·æ±‚è·å–æ•°æ®
import requests

response = requests.get('https://api.example.com/data')
print(response.text)

# ä½¿ç”¨ POST è¯·æ±‚æäº¤æ•°æ®
response = requests.post('https://api.example.com/create', data={'key': 'value'})
print(response.text)
```



### post çš„ä¸¤ä¸ªå‚æ•°

| ç±»å‹            | å‚æ•°ä½ç½®       | ä½¿ç”¨æ–¹å¼                        | ç‰¹ç‚¹                       |
| --------------- | -------------- | ------------------------------- | -------------------------- |
| URLæ‹¼æ¥å‚æ•°     | URL            | `requests.get(url?key=value)`   | ç®€å•ä½†ä¸æ¨èç”¨äº POST      |
| Form Data       | è¯·æ±‚ä½“ï¼ˆè¡¨å•ï¼‰ | `requests.post(..., data=data)` | æœ€å¸¸ç”¨ï¼Œæ”¯æŒè¡¨å•ç»“æ„       |
| Request Payload | è¯·æ±‚ä½“ï¼ˆJSONï¼‰ | `requests.post(..., json=data)` | ç°ä»£æ¥å£å¸¸ç”¨ï¼Œæ”¯æŒåµŒå¥—ç»“æ„ |









## Question 8

> CSVæ ¼å¼çš„ç‰¹ç‚¹

P 142

- CSV ( Comma-Separated Values), is a simple and widely-used file format for storing tabular data. **Each line in a CSV file represents a data record, with fields separated by commas.**
- Features: This structure allows easy data exchange between different systems and applications. CSV is commonly supported by many programming languages and tools. Its simplicity and compatibility make it a popular choice for handling structured data efficiently.

### ä½¿ç”¨è§„èŒƒç»†åˆ™

#### 1. Records and Fieldsï¼ˆè®°å½•å’Œå­—æ®µï¼‰

- **A CSV file is composed of records (rows), where each record is a line of text.**
     â¤ ä¸€ä¸ª CSV æ–‡ä»¶ç”±å¤šæ¡è®°å½•ï¼ˆè¡Œï¼‰ç»„æˆï¼Œæ¯æ¡è®°å½•æ˜¯ä¸€è¡Œæ–‡æœ¬ã€‚
- **Each record contains fields (columns), separated by commas.**
     â¤ æ¯æ¡è®°å½•ç”±å¤šä¸ªå­—æ®µï¼ˆåˆ—ï¼‰æ„æˆï¼Œå­—æ®µä¹‹é—´ç”¨é€—å·åˆ†éš”ã€‚
     - Example: Name, Age, City
         â¤ ç¤ºä¾‹ï¼šName, Age, City
     - This record has three fields: "Name", "Age", and "City".
       

#### 2. Special Casesï¼ˆç‰¹æ®Šæƒ…å†µï¼‰

- Commas within Fields: If a field contains a comma, the entire field must be enclosed in double quotes (").
     â¤ å­—æ®µä¸­æœ‰é€—å·æ—¶ï¼Œéœ€è¦ç”¨åŒå¼•å·æŠŠæ•´ä¸ªå­—æ®µåŒ…è£¹èµ·æ¥ã€‚
    - Example: `"John, Doe", 30, "New York"`
         â¤ ç¤ºä¾‹ï¼šå­—æ®µ "John, Doe" è¢«åŒå¼•å·åŒ…è£¹ã€‚
- Newlines within Fields: If a field contains a newline character, it must also be enclosed in double quotes.
     â¤ å¦‚æœå­—æ®µä¸­åŒ…å«æ¢è¡Œç¬¦ï¼Œä¹Ÿå¿…é¡»ç”¨åŒå¼•å·æ‹¬èµ·æ¥ã€‚
    - Example: `"Multiline\nField", Data`
         â¤ ç¤ºä¾‹ï¼šå«æœ‰å¤šè¡Œçš„å­—æ®µåŒæ ·ç”¨åŒå¼•å·æ‹¬èµ·æ¥ã€‚

- Quotes within Fieldsï¼ˆå­—æ®µä¸­å«æœ‰å¼•å·ï¼‰
    - If a field contains a double quote, it must be expressed by doubling the quote.
         â¤ å¦‚æœå­—æ®µä¸­åŒ…å«å¼•å·ï¼Œéœ€è¦å°†å¼•å·å†™æˆä¸¤ä¸ªåŒå¼•å·ã€‚
    - Example: `"She said, ""Hello""" , 25, City`
         â¤ ç¤ºä¾‹ï¼šShe said, "Hello"ï¼Œ25ï¼ŒCity

#### 3. Header Rowï¼ˆè¡¨å¤´ï¼‰

- The first line of a CSV file often contains headers that describe the content of each column.
     â¤ CSV æ–‡ä»¶çš„ç¬¬ä¸€è¡Œé€šå¸¸æ˜¯è¡¨å¤´ï¼Œç”¨äºè¯´æ˜æ¯ä¸€åˆ—çš„å«ä¹‰ã€‚
- **Example**: ID, Name, Email, Phone
     â¤ ç¤ºä¾‹ï¼šID, Name, Email, Phone

#### 4. Optional Whitespaceï¼ˆå¯é€‰ç©ºæ ¼ï¼‰

- Spaces after commas are generally ignored, but spaces within fields are preserved.
     â¤ é€—å·åçš„ç©ºæ ¼é€šå¸¸ä¼šè¢«å¿½ç•¥ï¼Œä½†å­—æ®µå†…éƒ¨çš„ç©ºæ ¼ä¼šè¢«ä¿ç•™ã€‚
- **Example**: `Name, Age , City` is equivalent to `Name,Age,City`
     â¤ ç¤ºä¾‹ï¼š`Name, Age , City` ä¸ `Name,Age,City` æ˜¯ä¸€æ ·çš„ã€‚

#### 5. Comment Linesï¼ˆæ³¨é‡Šè¡Œï¼‰

- Some CSV files may include comment lines, which typically start with a # or //.
     â¤ æœ‰äº› CSV æ–‡ä»¶å¯èƒ½åŒ…å«æ³¨é‡Šè¡Œï¼Œé€šå¸¸ä»¥ `#` æˆ– `//` å¼€å¤´ã€‚
- but this is not part of the standard CSV syntax.
     â¤ ä½†è¿™å¹¶ä¸æ˜¯æ ‡å‡† CSV è¯­æ³•çš„ä¸€éƒ¨åˆ†ã€‚



### ç‰¹ç‚¹

```python
Name, Age, City
John Doe, 30, New York
Jane Smith, 25, Los Angeles
"Mike Johnson", 35, "San Francisco"
```

- Lightweight: Small file size, easy to read and write, suitable for storing and transmitting small and medium-sized data.
- Text format: A plain text file that can be opened and edited with any text editor.
- Easy to parse: The structure is simple, and most programming languages can quickly parse and generate CSV data.
- Wide support: Many software (such as Excel, Google Sheets, database systems) support CSV file format.



### ä¼˜ç¼ºç‚¹

**ä¼˜ç‚¹**ï¼š

- æ˜“äºä½¿ç”¨ï¼šæ ¼å¼ç›´è§‚ï¼Œæ˜“äºç†è§£å’Œç¼–è¾‘ã€‚
- å…¼å®¹æ€§å¼ºï¼šæ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€å’Œå·¥å…·ï¼Œä¾‹å¦‚ Pythonã€Excelã€R ç­‰ã€‚
- æ–‡ä»¶å¤§å°å°ï¼šæ²¡æœ‰å¤æ‚çš„å…ƒæ•°æ®æˆ–ç»“æ„ï¼Œæ¯” JSON å’Œ XML æ›´å°ã€‚
- æ˜“äºé˜…è¯»ï¼šå³ä½¿æ²¡æœ‰ä¸“é—¨çš„å·¥å…·ï¼Œä¹Ÿå¯ä»¥é€šè¿‡æ–‡æœ¬ç¼–è¾‘å™¨æŸ¥çœ‹å†…å®¹ã€‚

**ç¼ºç‚¹**ï¼š

- ç¼ºä¹æ ‡å‡†åŒ–ï¼šä¸åŒçš„å·¥å…·å¤„ç† CSV çš„æ–¹å¼å¯èƒ½æœ‰æ‰€ä¸åŒï¼ˆä¾‹å¦‚åˆ†éš”ç¬¦ã€æ¢è¡Œç¬¦ï¼‰ã€‚
- æ— æ³•è¡¨ç¤ºå±‚çº§ç»“æ„ï¼šCSV ä¸ºäºŒç»´ç»“æ„ï¼Œä¸é€‚åˆå­˜å‚¨å¤æ‚çš„åµŒå¥—æ•°æ®ã€‚
- ç¼ºä¹æ•°æ®ç±»å‹ä¿¡æ¯ï¼šæ‰€æœ‰å­—æ®µéƒ½ä»¥æ–‡æœ¬å½¢å¼å­˜å‚¨ï¼Œæ²¡æœ‰æ•°æ®ç±»å‹å®šä¹‰ã€‚
- æ•°æ®å®‰å…¨æ€§ï¼šæ²¡æœ‰å†…ç½®åŠ å¯†æˆ–æƒé™æ§åˆ¶ï¼Œå®¹æ˜“è¢«ç¯¡æ”¹ã€‚

### è¯»å†™

**è¯»å– CSV æ–‡ä»¶**ï¼š

- Python ç¤ºä¾‹ï¼šä½¿ç”¨å†…ç½® `csv` æ¨¡å—æˆ– `pandas` åº“
- JavaScript å¯ç”¨ `PapaParse`ï¼ŒJava ç”¨ Apache Commons CSV

**å†™å…¥ CSV æ–‡ä»¶**ï¼š

- åŒæ ·å¯ä»¥ä½¿ç”¨ Python çš„ `csv` æˆ– `pandas`

```python
import csv

# å†™å…¥çš„æ•°æ®
data = [
    ["Name", "Age", "City"],
    ["Alice", 30, "Beijing"],
    ["Bob", 25, "Shanghai"]
]

# å†™å…¥ CSV æ–‡ä»¶
with open('people.csv', 'w', newline='', encoding='utf-8') as f:
    writer = csv.writer(f)
    writer.writerows(data)
```

```python
import pandas as pd

# å†™å…¥çš„æ•°æ®
data = {
    "Name": ["Alice", "Bob"],
    "Age": [30, 25],
    "City": ["Beijing", "Shanghai"]
}

df = pd.DataFrame(data)

# å†™å…¥ CSV æ–‡ä»¶
df.to_csv("people.csv", index=False, encoding="utf-8")
```



```python
import csv

# è¯»å– CSV æ–‡ä»¶
with open('people.csv', 'r', encoding='utf-8') as f:
    reader = csv.reader(f)
    for row in reader:
        print(row)
```

```python
import pandas as pd

# è¯»å– CSV æ–‡ä»¶
df = pd.read_csv("people.csv", encoding="utf-8")

# æ‰“å°æ¯ä¸€è¡Œ
for index, row in df.iterrows():
    print(row.to_dict())
```



<img src="./question.assets/image-20250418163444809.png" alt="image-20250418163444809" style="zoom:50%;" />

Read CSV file: 

- Python example: Use the built-in CSV module or pandas library.
- Other languages such as JavaScript can use libraries like Papapath, while Java can use Apache Commons CSV.

Writing CSV files:

- Writing operations can also be performed using Python's CSV module or the pandas library







## Question 9

> æœ‰å“ªäº›ç±»å‹çš„çˆ¬è™«ï¼Ÿ

### æŒ‰çˆ¬å–å†…å®¹åˆ†

**1. Focused Web Crawler**
 Crawls web pages related to a specific topic. It filters and prioritizes URLs based on relevance using text similarity or link analysis.

**2. Incremental Web Crawler**
 Only collects newly updated or changed pages. It reduces data volume and needs deduplication.

**3. Deep Web Crawler**
 Accesses hidden pages that require login or form submission. It involves detecting forms, filling them, and navigating deep links.

**4. General Web Crawler (Implied)**
 Unlike the focused crawler, it collects all accessible web pages without targeting specific topics. It is less personalized and less efficient.

### æŒ‰çˆ¬å–ç­–ç•¥åˆ†

- The static crawler : searches data according to some determined rules, which will not change due to changes in webpage structure or text information.
    - Breath first search
    - Depth first search
    - Best first search
- The dynamic crawler : aims to efficiently and quickly complete crawling tasks, and adjust crawling routes in real-time. Fish-Search and Shark-Search are crawlers based on text content; PageRank, HITS,HillTop are dynamic crawlers based on link analysis.

### åº”ç”¨è§’åº¦æ¥åˆ†

Collection type crawler: Continuing the technology of search engine crawling, it is currently the most widely used mode, which limits the scope and intention of crawling to different degrees.

Monitoring type crawler: does not primarily aim to collect information, but utilizes the crawler's ability in content collection and analysis to monitor the information content of the server, thus proposing higher applications for the interaction ability between the crawler and the server



## Question 10

> å¤§æ•°æ®å¤„ç†åˆ†æä¸­çš„ä¸»è¦æµç¨‹ï¼Œè¿‡ç¨‹æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆä¼°è®¡æ˜¯ç®€ç­”é¢˜ï¼‰
>
> å¤„ç†è¿‡ç¨‹ä¸­æœ‰å“ªäº›é—®é¢˜ï¼Œæœ‰ä»€ä¹ˆåŠæ³•è§£å†³ï¼Ÿ

### 1. æ•°æ®çˆ¬å–

It is the **foundation of big data analysis**, acquiring data from various sources such as social media, web pages using web crawler, APIs and so on.

The typical process includes:
 **(1)** Sending requests â†’ **(2)** Receiving responses â†’ **(3)** Parsing content (HTML/JSON) â†’ **(4)** Saving data.

**Common issues** include:

- Data hidden behind JavaScript rendering
- Anti-crawling measures like **User-Agent detection** or **IP blocking**

**Solutions** involve using **custom request headers**, handling **AJAX-loaded data**, or switching to API-based crawling.

![image-20250420102854102](./question.assets/image-20250420102854102.png)





### 2. æ•°æ®æ¸…æ´—

**"Big data cleaning is a fundamental task, ensuring the analysis and application of big data."**

To ensure the data accurate integrity, consistency, timeliness and effectiveness and other requirements to support subsequent data handle.

The cleaning process includes:
 **(1)** Analyzing data â†’ **(2)** Defining cleaning rules â†’ **(3)** Detecting dirty data â†’ **(4)** Handling dirty data â†’ **(5)** Evaluating data quality.

Common issues include **missing values, duplicate records, and outliers**.
 Solutions involve **deleting or imputing missing data**, using **drop_duplicates()** for duplicates, and applying **statistical or machine learning methods** to detect outliers.







### 3. æ•°æ®åˆå¹¶è½¬æ¢

Introduction to Data Transformationï¼š

- Data transformation refers to the process of converting data from one format, structure, or representation to another to make it suitable for analysis, modeling, or integration with other datasets. It is a critical step in data preprocessing, ensuring data quality, consistency, and compatibility across systems.
- It is often necessary to stack multiple datasets into one dataset, connect multiple datasets into the desired dataset, and map, transform, or discretize data values.





### 4. æ•°æ®æ ‡æ³¨

- Text data annotation, as one of the most common types of data annotation, refers to annotating text, including text and symbols, so that computers can understand and recognize them.
- The purpose of labeling problems is to learn a model that can provide labeled sequences as predictions for observed sequences.
- The input of the annotation problem is an observation sequence and the output is a label sequence or state sequence.
- Text data annotation is divided into sequence tagging, relationship annotation, attribute annotation, and category annotation. Using doccano
  - Sequence tagging includes word segmentation, entities, keywords, prosody, intent understanding, etc;
  - Relationship annotation includes pointing relationships, decorate relationships, parallel corpora, etc;
  - Attribute annotation includes text category, news, entertainment, etc;
  - Category annotation includes reading comprehension within the range of passage.

- Image annotation using LabelImg, Labelme
    - keypoint annotation, 
    - rectangular box annotation, 
    - Polygon annotation
    - image segmentation, 
    - 3D box annotation, 
    - attribute annotation



### 5. æ•°æ®åˆ†æ

The purpose of data analysis and data mining is to extract information hidden in large amounts of data, to identify the inherent patterns of the studied object. People understand , judge, make decisions, and take actions based on a series of data analysis.

The purpose of data analysis can be roughly divided into four levels: descriptive data analysis, diagnostic data analysis, predictive data analysis, and directive data analysis.

**Descriptive data analysis** provides a brief summary of what has happened, reflecting the fluctuation and trend of data through descriptive statistical indicators, and observing whether there are any abnormal situations in the data through descriptive data analysis.

**Diagnostic data analysis** goes one step further on the basis of descriptive data analysis, that is, *"how it happened"*. Diagnostic data analysis can discover the causes and outcomes of events.

**Predictive data analysis** is the result of combining descriptive data analysis and diagnostic data analysis to further discover the direction of data and predict what may happen next, that is, *"what may happen"*.

**Directive data analysis** is the process of proposing solutions based on the first three levels, namely *"what should be done"*.



### 6. æ•°æ®å¯è§†åŒ–

**Data visualization** uses computer graphics to transform complex, high-dimensional data into visual formats for analysis and interaction. It includes **scientific** and **information visualization**, helping reveal key insights through intuitive visuals.



### 7. æ•°æ®å­˜å‚¨

**Data storage and management** is the process of using computer hardware and software to store and apply data efficiently. In the big data era, data types (structured, semi-structured, unstructured) and volumes have grown beyond traditional capabilities, leading to new technologies like **distributed file systems** and **distributed databases**.







## Question

> scrapy

### Scrapy çš„æ ¸å¿ƒåŠŸèƒ½

| åŠŸèƒ½                 | ç®€ä»‹                                                     |
| -------------------- | -------------------------------------------------------- |
| ç½‘é¡µæŠ“å–             | å‘èµ·è¯·æ±‚ã€ä¸‹è½½ç½‘é¡µå†…å®¹                                   |
| æ•°æ®æŠ½å–             | é€šè¿‡ XPathã€CSS Selectorã€æ­£åˆ™è¡¨è¾¾å¼ç­‰æ–¹å¼æå–ç»“æ„åŒ–ä¿¡æ¯ |
| æ•°æ®æŒä¹…åŒ–           | æ”¯æŒä¿å­˜ä¸º JSONã€CSVã€XMLã€æ•°æ®åº“ç­‰æ ¼å¼                  |
| é«˜æ€§èƒ½å¹¶å‘           | **åŸºäº Twisted å¼‚æ­¥ I/Oï¼Œé€Ÿåº¦å¿«ã€å¹¶å‘é«˜**                |
| çˆ¬è™«ä¸­é—´ä»¶           | å¯æ’æ‹”æ¨¡å—ï¼Œæ”¯æŒç”¨æˆ·è‡ªå®šä¹‰è¯·æ±‚å¤´ã€ä»£ç†ã€é‡è¯•æœºåˆ¶ç­‰       |
| å¤šé¡¹ç›®æ”¯æŒ           | å¯ä»¥åŒæ—¶ç®¡ç†å¤šä¸ªçˆ¬è™«é¡¹ç›®                                 |
| åˆ†å¸ƒå¼æ”¯æŒï¼ˆå¯æ‹“å±•ï¼‰ | å¯é…åˆ scrapy-redis ç­‰æ’ä»¶å®ç°åˆ†å¸ƒå¼çˆ¬è™«                 |



<img src="./question.assets/image-20250419122208243.png" alt="image-20250419122208243" style="zoom:50%;" />

![image-20250419122241833](./question.assets/image-20250419122241833.png)

<img src="./question.assets/image-20250423004249364.png" alt="image-20250423004249364" style="zoom: 50%;" />

## Question 

> matplotlib

```python
p1 = plt.figure(figsize=(8,6),dpi=80)
```

![image-20250422150732371](./question.assets/image-20250422150732371.png)

<img src="./question.assets/image-20250422151145928.png" alt="image-20250422151145928" style="zoom:67%;" />

<img src="./question.assets/image-20250422151310843.png" alt="image-20250422151310843" style="zoom:80%;" />

<img src="./question.assets/image-20250422151453713.png" alt="image-20250422151453713" style="zoom:67%;" />

![image-20250422151536909](./question.assets/image-20250422151536909.png)



| å›¾ç±»å‹                 | ä¸­æ–‡å              | è¯´æ˜                                            |
| ---------------------- | ------------------- | ----------------------------------------------- |
| `plot()`               | æŠ˜çº¿å›¾              | å±•ç¤ºè¶‹åŠ¿å˜åŒ–ï¼Œé»˜è®¤å‡½æ•°                          |
| `bar()` / `barh()`     | æ¡å½¢å›¾ / æ¨ªå‘æ¡å½¢å›¾ | å±•ç¤ºåˆ†ç±»æ•°æ®çš„å¯¹æ¯”                              |
| `hist()`               | ç›´æ–¹å›¾              | å±•ç¤ºæ•°æ®åˆ†å¸ƒæƒ…å†µ                                |
| `scatter()`            | æ•£ç‚¹å›¾              | å±•ç¤ºç‚¹ä¹‹é—´çš„å…³ç³»ï¼ˆç›¸å…³æ€§ï¼‰                      |
| `pie()`                | é¥¼å›¾                | å±•ç¤ºéƒ¨åˆ†ä¸æ•´ä½“çš„å…³ç³»ï¼ˆæ¯”ä¾‹ï¼‰                    |
| `boxplot()`            | ç®±çº¿å›¾              | å±•ç¤ºæ•°æ®çš„åˆ†å¸ƒã€å¼‚å¸¸å€¼ã€å››åˆ†ä½æ•°                |
| `fill_between()`       | åŒºåŸŸå›¾              | ç”¨äºè¡¨ç¤ºèŒƒå›´å˜åŒ–ï¼ˆå¦‚ç½®ä¿¡åŒºé—´ï¼‰                  |
| `imshow()`             | å›¾åƒå±•ç¤º            | å¯ç”¨äºæ˜¾ç¤ºå›¾ç‰‡ã€çƒ­åŠ›å›¾ç­‰                        |
| `contour()`            | ç­‰é«˜çº¿å›¾            | å¸¸ç”¨äºåœ°ç†ã€å›¾åƒç­‰è¿ç»­ç©ºé—´å˜é‡                  |
| `3D plot`ï¼ˆéœ€ Axes3Dï¼‰ | ä¸‰ç»´å›¾              | ==surface chart, scatter plots and line chart== |

==scatter plot æ•£ç‚¹å›¾==

==line chart  æŠ˜çº¿å›¾==

==Step plot é˜¶æ¢¯å›¾==

==Time series plot æ—¶é—´åºåˆ—å›¾==

==bar chart æŸ±çŠ¶å›¾==

==Radar chart é›·è¾¾å›¾==

==Bubble chart æ°”æ³¡å›¾==

==Pie chart é¥¼å›¾==

==stack column chart== æœ‰æ¯”ä¾‹çš„æŸ±çŠ¶å›¾

==Histogram ç›´æ–¹å›¾==

==Box plot é•¶åµŒå›¾==

==Density plot å¯†åº¦å›¾==

==word cloud==

==Temporal text==

==Force-Directed graph åŠ›å¯¼å‘å›¾==



```python
import matplotlib.pyplot as plt

x = [1, 2, 3, 4, 5]
y = [2, 4, 1, 3, 7]

plt.plot(x, y, label='line')        # æŠ˜çº¿å›¾
plt.xlabel('X axis')                # è®¾ç½®Xè½´æ ‡ç­¾
plt.ylabel('Y axis')                # è®¾ç½®Yè½´æ ‡ç­¾
plt.title('Simple Line Chart')      # è®¾ç½®å›¾æ ‡é¢˜
plt.legend()                        # æ˜¾ç¤ºå›¾ä¾‹
plt.grid(True)                      # æ˜¾ç¤ºç½‘æ ¼
plt.show()                          # æ¸²æŸ“æ˜¾ç¤ºå›¾å½¢
```



## Question 

> jieba

![image-20250422152204352](./question.assets/image-20250422152204352.png)

## Question 

> å¤§æ•°æ®çš„ç»“æ„ç‰¹å¾

### ğŸ§± å¤§æ•°æ®çš„ç»“æ„ç‰¹å¾ï¼šä¸»è¦åˆ†ä¸ºä¸‰ç±»

| ç±»å‹ | åç§°             | è¯´æ˜                                                       | ç¤ºä¾‹                                     |
| ---- | ---------------- | ---------------------------------------------------------- | ---------------------------------------- |
| â‘     | **ç»“æ„åŒ–æ•°æ®**   | Has a clear schema and fields, similar to a database table | è¡¨æ ¼ã€å…³ç³»æ•°æ®åº“ï¼ˆå¦‚ MySQLï¼‰             |
| â‘¡    | **åŠç»“æ„åŒ–æ•°æ®** | There are certain structural tags, but no fixed fields     | JSONã€XMLã€æ—¥å¿—ã€HTML                    |
| â‘¢    | **éç»“æ„åŒ–æ•°æ®** | æ²¡æœ‰å›ºå®šæ ¼å¼ã€æ— æ³•ç›´æ¥å»ºè¡¨                                 | æ–‡æœ¬ã€å›¾ç‰‡ã€éŸ³é¢‘ã€è§†é¢‘ã€é‚®ä»¶ã€ç¤¾äº¤å†…å®¹ç­‰ |

Structured data is organized in a predefined format and structure, typically in tables like SQL or Excel, making it easy to query and analyze. It is highly formatted.

Unstructured data lacks a fixed format and includes diverse content like text, videos, and PDFs, making it hard to process with traditional tools. It holds rich information and is widely used in areas like social media analysis.

Semi-structured data sits between structured and unstructured data, using tags or markers to add some organization. It's easier to parse than unstructured data and is often used in emails, HTML, and log files.















## Question

> å¤§æ•°æ®å­˜å‚¨å’Œç®¡ç†çš„ç®¡ç†ç³»ç»Ÿï¼Œé‡ç‚¹NoSQL

Distributed file system

**NoSQL æ•°æ®åº“**ä¹Ÿå«éå…³ç³»å‹æ•°æ®åº“ï¼Œé€‚ç”¨äºå¤„ç†**æµ·é‡æ•°æ®**ã€**é«˜å¹¶å‘è®¿é—®**ï¼Œä»¥åŠéœ€è¦**é«˜æ‰©å±•æ€§å’Œé«˜å¯ç”¨æ€§**çš„åœºæ™¯ã€‚
å®ƒä¸ä½¿ç”¨å›ºå®šçš„è¡¨ç»“æ„ï¼Œä¹Ÿä¸ä¾èµ– SQL æŸ¥è¯¢è¯­è¨€ï¼Œé€šå¸¸é¿å…ä½¿ç”¨ä¼ ç»Ÿæ•°æ®åº“çš„å¤šè¡¨è¿æ¥ã€‚
å¸¸è§çš„ NoSQL ç±»å‹æœ‰ï¼š**é”®å€¼æ•°æ®åº“**ã€**åˆ—å¼æ•°æ®åº“**ã€**æ–‡æ¡£æ•°æ®åº“** å’Œ **å›¾æ•°æ®åº“**ã€‚æ¯ç§ç±»å‹éƒ½æ“…é•¿è§£å†³å…³ç³»å‹æ•°æ®åº“éš¾ä»¥åº”å¯¹çš„é—®é¢˜ã€‚

==key value databases, column family databases, document databases, and graph databases==.

éµå¾ªï¼šACID

Atomicity åŸå­æ€§ï¼Œäº‹ç‰©ä¸å¯åˆ†å‰²

Consistency ä¸€è‡´æ€§ï¼Œå¿…é¡»ä¿æŒæ‰€æœ‰é¢„å®šä¹‰è§„åˆ™

Isolation éš”ç¦»æ€§ï¼Œå¹¶å‘æ‰§è¡Œçš„äº‹åŠ¡ä¸äº’ç›¸å½±å“

Durability æŒä¹…æ€§ï¼Œå¯¹äºæ•°æ®çš„ä¿®æ”¹æ°¸ä¹…ä¿å­˜

<img src="./question.assets/image-20250422223122633.png" alt="image-20250422223122633" style="zoom: 80%;" />

- Key-Value database ä»£è¡¨ï¼šRedis
- Column family database ä»£è¡¨ï¼šHBase
- æ–‡æ¡£æ•°æ®åº“ï¼šJSON ä»£è¡¨ï¼šMongoDB
- å›¾æ•°æ®åº“ï¼šç¤¾äº¤ç½‘ç»œç­‰ç­‰ ä»£è¡¨ï¼šNeo4j

<img src="./question.assets/image-20250422145413440.png" alt="image-20250422145413440" style="zoom: 50%;" />

<img src="./question.assets/image-20250422150159541.png" alt="image-20250422150159541" style="zoom: 50%;" />

NoSQL databases typically have the following characteristics. 

(1) Flexible scalability

(2) Flexible data model

(3) Closely integrated with cloud computing

Cloud computing has excellent horizontal scalability, allowing for free scaling based on resource usage. Various resources can be dynamically added or removed.



## Question 

> åœ¨å•†ä¸šé¢†åŸŸå¼€å‘å’Œéƒ¨ç½²ç”¨åˆ°çš„ Hadoop é‡Œé¢åŒ…å«çš„åŸºæœ¬ç»„ä»¶æœ‰å“ªäº›ï¼Ÿæ¯ä¸€ä¸ªç»„ä»¶åŠŸèƒ½ï¼Ÿ



The four core components of Hadoop are HDFS, MapReduce, YARN, and Hadoop Common. And many tools and solutions are used to support these main components.

<img src="./question.assets/image-20250422162121593.png" alt="image-20250422162121593" style="zoom: 80%;" />



### å„ä¸ªå­æ¨¡å—åŠŸèƒ½

<img src="./question.assets/image-20250422163406074.png" alt="image-20250422163406074" style="zoom:50%;" />

![image-20250422180801521](./question.assets/image-20250422180801521.png)

<img src="./question.assets/image-20250422180714863.png" alt="image-20250422180714863" style="zoom: 50%;" />



### Hadoop Distributed Files System

æƒ³è±¡ä½ æ˜¯ä¸€å®¶ç‰©æµå…¬å¸ï¼Œè´§å¤ªå¤šï¼Œä¸€å°ç”µè„‘æ ¹æœ¬è£…ä¸ä¸‹ã€‚äºæ˜¯ä½ æŠŠæ•°æ®åˆ‡å—ï¼ˆblockï¼‰ï¼Œ**åˆ†å‘**åˆ°ä¸€å †ç”µè„‘ä¸Šå­˜å‚¨ï¼Œè¿™ä¸ªç³»ç»Ÿå°±å«åš **HDFSï¼ˆHadoop Distributed File Systemï¼‰**ã€‚

HDFS divides files into blocks and stores them in nodes of a distributed architecture.

- å®ƒæ˜¯ **Hadoop æœ€æ ¸å¿ƒçš„ç»„æˆéƒ¨åˆ†**ï¼›
- æ”¯æŒ**åˆ†å¸ƒå¼å­˜å‚¨ä¸è®¿é—®**ï¼›
- æ–‡ä»¶è¢«åˆ‡åˆ†æˆå°å—ï¼Œ**å†—ä½™å­˜å‚¨**ï¼Œé˜²æ­¢ä¸¢æ•°æ®ï¼›
- å®ƒè¿˜èƒ½ä¸å…¶å®ƒç»„ä»¶ï¼ˆæ¯”å¦‚ MapReduceï¼‰**æ— ç¼è¿æ¥**ã€‚

<img src="./question.assets/image-20250422181129718.png" alt="image-20250422181129718" style="zoom: 50%;" />

1. **HDFS** supports distributed storage by splitting files into blocks and storing them across nodes, ensuring fault tolerance through redundancy.
2. **NameNode** is the central server managing file system metadata and namespaces, with support from a Secondary NameNode.
3. **DataNodes** handle actual data storage, reading/writing, and block operations under NameNode's instructions.









### MapReduce

æœ‰äº†ä»“åº“ï¼ˆHDFSï¼‰ï¼Œä½ è¿˜éœ€è¦ä¸€ä¸ª**åŠ å·¥å‚æ¥å¤„ç†æ•°æ®**ï¼Œè¿™å°±æ˜¯ **MapReduce**ï¼š

- â€œ**Map**â€ æ˜¯æŠŠåŸå§‹æ•°æ®åˆ‡æˆä¸€ä¸ªä¸ªå°ä»»åŠ¡ï¼ˆæ¯”å¦‚æŒ‰å…³é”®è¯åˆ†ç±»ï¼‰ï¼›
- â€œ**Reduce**â€ æ˜¯æŠŠåˆ†ç±»ç»“æœæ€»ç»“æ±‡æ€»ï¼ˆæ¯”å¦‚ç»Ÿè®¡æ¯ç±»æœ‰å¤šå°‘ï¼‰ï¼›
- å®ƒ**åŸºäº key-value é”®å€¼å¯¹**æ¥å¹¶è¡Œå¤„ç†æ•°æ®ï¼Œ**æ•ˆç‡é«˜**ï¼›
- MapReduce = **æŠŠå¤æ‚ä»»åŠ¡æ‹†å° + å¤šäººåŒæ—¶å¹² + æœ€åæ±‡æ€»ç»“æœ**ã€‚

<img src="./question.assets/image-20250422183220642.png" alt="image-20250422183220642" style="zoom: 50%;" />

1. **MapReduce** is a core Hadoop component that uses Map and Reduce tasks to perform parallel data computation with key-value pairs.
2. **JobTracker** handles job decomposition, task scheduling, and resource management, while **TaskTracker** executes tasks and reports status.
3. The process involves Map tasks generating intermediate results, which are then processed by Reduce tasks to produce final output.





### YARN

YARN is a core component in Hadoop 2 that manages cluster resources and intelligently schedules processing tasks across the system. RM manages resources, AM initiates requests, NM executes and manages tasks, and Container carries the workload.

ä½ å¯ä»¥æŠŠ Hadoop æ¯”ä½œä¸€åº§å¤§åŸå¸‚ï¼š
 HDFS æ˜¯ä»“åº“ï¼ŒMapReduce æ˜¯å·¥å‚ï¼Œé‚£è°æ¥åˆ†é…ä»“åº“èµ„æºã€è°ƒåº¦å·¥å‚è¿ä½œï¼Ÿç­”æ¡ˆå°±æ˜¯ **YARNï¼ˆYet Another Resource Negotiatorï¼‰**ï¼

![image-20250422170131877](./question.assets/image-20250422170131877.png)





Toolkits and Foundation Packages

- æä¾›ä¸€ç»„**å·¥å…·ç±»å’ŒåŸºç¡€åŒ…**ï¼Œæ¯”å¦‚ I/Oã€ç½‘ç»œã€åºåˆ—åŒ–ç­‰ï¼›
- è®©å„ç§ç¡¬ä»¶å’ŒèŠ‚ç‚¹å¯ä»¥æ— å·®åˆ«åŠ å…¥ Hadoop ç½‘ç»œï¼Œå°±åƒä½ ç”¨ä¸åŒå“ç‰Œçš„æ‰‹æœºéƒ½èƒ½è¿ä¸Š Wi-Fiï¼›
- æ‰€æœ‰æ¨¡å—ï¼ˆHDFSã€YARNã€MapReduceï¼‰éƒ½éœ€è¦è°ƒç”¨å®ƒï¼Œå°±åƒå°åŒºçš„æ°´ç”µç…¤éƒ½å¾—é ç‰©ä¸šç»´æŠ¤ä¸€æ ·ã€‚

ğŸ”§ æ‰€ä»¥ä½ å¯ä»¥ç†è§£ä¸ºï¼š**Hadoop Common æ˜¯ Hadoop å„ä¸ªæ¨¡å—èƒŒåçš„â€œåŸºç¡€ä¿éšœâ€æœåŠ¡ï¼Œæ²¡æœ‰å®ƒå°±åƒæ–­ç”µæ–­ç½‘ï¼Œå•¥ä¹Ÿå¹²ä¸äº†ã€‚**









## Question

<img src="./question.assets/image-20250423224903537.png" alt="image-20250423224903537" style="zoom: 80%;" />





![image-20250423224918112](./question.assets/image-20250423224918112.png)





















