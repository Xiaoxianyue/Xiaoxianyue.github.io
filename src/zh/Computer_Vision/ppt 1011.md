---
title: è®¡ç®—æœºè§†è§‰ L 10 - L 11
icon: python
date: 2024-10-29 14:11:00
author: XiaoXianYue
isOriginal: true
category: 
    - å¤§ä¸‰ä¸Š
    - è®¡ç®—æœºè§†è§‰
tag:
    - å¤§ä¸‰ä¸Š
    - è®¡ç®—æœºè§†è§‰
sticky: false
star: false
article: true
timeline: true
image: false
navbar: true
sidebarIcon: true
headerDepth: 5
lastUpdated: true
editLink: false
backToTop: true
toc: true
---

## Lecture 10



<img src="./ppt 1011.assets/image-20241029142556692.png" alt="image-20241029142556692" style="zoom: 33%;" />

- **å›å½’ä»»åŠ¡ï¼š**

    - Both x and w are continuous value
    - choose Linear regression to compute the Pr(w|x) 
    - æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ª Probability density function to model Pr(w|x) ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦æ£€æµ‹åœ¨ä¸€ä¸ªç‰¹å®šä¸–ç•ŒçŠ¶æ€ä¸‹æ•°æ®çš„åˆ†å¸ƒã€‚
    - é€šè¿‡ **çº¿æ€§å›å½’** æ¥ç›´æ¥å»ºæ¨¡ Pr(x|w)ï¼Œä»è€Œä¼°è®¡æ•°æ®ä¸ä¸–ç•ŒçŠ¶æ€çš„å…³ç³»ã€‚

    

- **åˆ†ç±»ä»»åŠ¡ï¼š**

    - x is continuous while w are categorical, typibally taking value in {0, 1}
    - **Logistic regression** is chosen to compute Pr(wâˆ£x)Pr(w|x)Pr(wâˆ£x), predicting the probability of each category.
    - å¯¹äº Pr(xâˆ£w)ï¼Œä½¿ç”¨ **æ¦‚ç‡å¯†åº¦å‡½æ•°** æ¥è¡¨ç¤ºæ•°æ®åœ¨ç»™å®šç±»åˆ«ä¸‹çš„åˆ†å¸ƒæƒ…å†µã€‚

- **æ€»ç»“ï¼š**

    - å›å½’ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹å…³æ³¨çš„æ˜¯è¿ç»­æ•°å€¼ä¹‹é—´çš„å…³ç³»ï¼Œé€šè¿‡æ¦‚ç‡å¯†åº¦å‡½æ•°å»ºæ¨¡ã€‚
    - åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹å…³æ³¨çš„æ˜¯æ•°æ®å¦‚ä½•åˆ’åˆ†åˆ°ä¸åŒçš„ç±»åˆ«ï¼Œé€šè¿‡é€»è¾‘å›å½’å’Œæ¦‚ç‡å¯†åº¦å‡½æ•°å®ç°åˆ†ç±»é¢„æµ‹ã€‚

    


### 1.1 **What is hidden or latent variables?**

**Key idea:** represent density **Pr(x)** as marginalization of joint density with another **variable h** that we do not see. In other words, hidden variables are unobserved factors within a model that influence the values of observed variables.

<img src="./ppt 1011.assets/image-20241029152312453.png" alt="image-20241029152312453" style="zoom:50%;" />

Also, depend on some parameters

<img src="./ppt 1011.assets/image-20241029152402267.png" alt="image-20241029152402267" style="zoom: 50%;" />

::: details GPT

**éšè—å˜é‡çš„å®šä¹‰**ï¼š

- éšè—å˜é‡æ˜¯æ¨¡å‹ä¸­æœªç›´æ¥è§‚æµ‹åˆ°çš„å˜é‡ï¼Œä½†å®ƒä»¬å½±å“äº†è§‚æµ‹å˜é‡çš„å€¼æˆ–æ•°æ®ç”Ÿæˆè¿‡ç¨‹ã€‚
- ä¾‹å¦‚ï¼Œåœ¨å­¦ç”Ÿçš„è€ƒè¯•æˆç»©æ•°æ®ä¸­ï¼Œâ€œå­¦ç”Ÿçš„å­¦ä¹ èƒ½åŠ›â€å¯ä»¥è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ªéšè—å˜é‡ã€‚æˆ‘ä»¬æ— æ³•ç›´æ¥è§‚æµ‹åˆ°æ¯ä¸ªå­¦ç”Ÿçš„å­¦ä¹ èƒ½åŠ›ï¼Œä½†å®ƒæ˜¾ç„¶å½±å“äº†å­¦ç”Ÿçš„è€ƒè¯•æˆç»©ã€‚

**éšè—å˜é‡çš„ç”¨é€”**ï¼š

- **æ­ç¤ºæ•°æ®ç»“æ„**ï¼šéšè—å˜é‡èƒ½å¤Ÿæ­ç¤ºå‡ºæ•°æ®çš„æ½œåœ¨ç»“æ„ã€‚é€šè¿‡å¼•å…¥è¿™äº›éšè—å˜é‡ï¼Œå¯ä»¥è§£é‡Šè§‚æµ‹æ•°æ®ä¹‹é—´çš„ç›¸å…³æ€§å’Œç»“æ„ã€‚
- **é™ç»´å’Œä¿¡æ¯å‹ç¼©**ï¼šåœ¨å¤æ‚æ•°æ®ä¸­ï¼Œéšè—å˜é‡å¸¸ç”¨äºå°†æ•°æ®ä»é«˜ç»´ç©ºé—´æ˜ å°„åˆ°ä½ç»´ç©ºé—´ã€‚è¿™æ˜¯é™ç»´çš„ä¸€ç§æ–¹å¼ï¼Œå¯ä»¥å‡å°‘æ•°æ®çš„å¤æ‚åº¦ï¼Œä¾‹å¦‚ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ä¸­éšå«çš„ä¸»æˆåˆ†å°±æ˜¯ä¸€ç§éšè—å˜é‡ã€‚
- **åˆ†ç¦»å™ªå£°**ï¼šéšè—å˜é‡æ¨¡å‹æœ‰æ—¶å¯ä»¥å¸®åŠ©åˆ†ç¦»å‡ºæ•°æ®ä¸­çš„å™ªå£°ï¼Œè¿›è€Œæ›´å‡†ç¡®åœ°åæ˜ å‡ºæ•°æ®çš„çœŸå®æ¨¡å¼ã€‚

**å¤„ç†éšè—å˜é‡çš„ç®—æ³•**ï¼š

- **æœŸæœ›æœ€å¤§åŒ–ç®—æ³•ï¼ˆEMç®—æ³•ï¼‰**ï¼šEMç®—æ³•æ˜¯å¸¸ç”¨æ¥ä¼°è®¡å¸¦æœ‰éšè—å˜é‡çš„æ¨¡å‹å‚æ•°çš„ç®—æ³•ã€‚å®ƒåŒ…å«ä¸¤æ­¥ï¼šåœ¨â€œEæ­¥â€ä¸­ï¼Œä¼°è®¡éšè—å˜é‡çš„åˆ†å¸ƒï¼›åœ¨â€œMæ­¥â€ä¸­ï¼ŒåŸºäºä¼°è®¡çš„åˆ†å¸ƒï¼Œæœ€å¤§åŒ–å‚æ•°çš„å¯¹æ•°ä¼¼ç„¶ã€‚

:::

### 1.2 What is **Expectation Maximization**?

::: tabs

@tab PPT

<img src="./ppt 1011.assets/image-20241030104400487.png" alt="image-20241030104400487" style="zoom:50%;" />

@tab GPT 

Expectation Maximization (EM) is an iterative algorithm used in statistics to find maximum likelihood estimates of parameters in models that have latent (hidden) variables or missing data. 

![image-20241029155544401](./ppt 1011.assets/image-20241029155544401.png)

Step:

- Defines **a lower bound** on log likelihood and increases bound iteratively. Lower bound is a *function* of parameters **q** and a set of probability distributions ğ‘ğ‘– (ğ’‰ğ‘– )
- E-Step â€“  Updating the probability distributions {ğ‘ğ‘– ğ’‰ğ‘– }ğ‘– ğ¼ =1 to improve the bound.
- M-Step â€“ updating the parameters ğœƒ to improve the bound.

:::

### 1.3 **What are the advantages of EM?**

ä¼šå‡ºåˆ¤æ–­é¢˜ã€‚

<img src="./ppt 1011.assets/image-20241030104126838.png" alt="image-20241030104126838" style="zoom: 50%;" />

### 1.4 **What is Mixture of Gaussians (MoG)? How to use EM to solve MoG?**

#### What is Mixture of Gaussians (MoG)?

The mixture of Gaussians (MoG) is a prototypical example of a model where learning is suited to the EM algorithm. The data are described as a weighted sum of K normal distributions.

<img src="./ppt 1011.assets/image-20241030090606123.png" alt="image-20241030090606123" style="zoom: 67%;" />



#### How to use EM to solve MoG?

::: tabs

@tab è‡ªå·±æ€»ç»“

1. **Goal**

The goal is "to learn parameters $Î¸={{Î»_1â€¦_k,Î¼_1â€¦_k,Î£_1â€¦_k}}$ from training data $x_1,â€¦,x_I$."

2. **E-Step**

In the E-step, we fix parameters  $Î¸$ , maximize bound with respect to (w.r.t.) distributions $q_i(h_i)$ by calculating "the responsibility of the $k^{th}$ Gaussian for the $i^{th}$ data point.

- For each data point $x_i$, we calculate the posterior probability distribution $Pr(h_i=k | x_i, \theta^{(t)})$.

3. **M-Step**

In the M-step, we fix distributions $q_i(h_i)$ ,maximize bound with respect to w.r.t. parameters Î¸ = ![image-20241030100202835](./ppt 1011.assets/image-20241030100202835.png)

- We compute the updated parameters 
- ![image-20241030094713212](./ppt 1011.assets/image-20241030094713212.png)
- Then, we updated parameter values. 

4. **Iterate Until Convergence**

Alternates E-steps and M-Steps, until the model parameters converge, the (local) maximum of the actual log likelihood is approached.

@tab PPT

![image-20241030100556556](./ppt 1011.assets/image-20241030100556556.png)

:::

### 1.5 **What is t-distributions? How to use EM to solve t-distributions?**

#### What is t-distributions?

The **Student's t-distribution** is described as a distribution robust against data with heavy tails. It is defined as:

![image-20241030101233907](./ppt 1011.assets/image-20241030101233907.png)

where the degrees of freedom $Î½âˆˆ(0,âˆ)$ control the length of the tails; a smaller Î½ implies a heavier tail, meaning more weight in the tails.

#### How to use EM to solve t-distributions?

![image-20241030102923829](./ppt 1011.assets/image-20241030102923829.png)



### **1.6 What is factor analysis? How to use EM to solve factor analysis?**

#### What is factor analysis? 

The probability density function of a factor analyzer is given by:

![image-20241030103623110](./ppt 1011.assets/image-20241030103623110.png)

where the covariance matrix  $\Phi\Phi^T + \Sigma$  contains a sum of two terms. The first term, $\Phi\Phi^T$ describes a full covariance model over the subspace with the factors (columns of matrix $\Phi$) determining the subspace modeled. The factors are latent variables used to explain the correlations between observed variables. The second term, $\Sigma$, is a diagonal matrix that accounts for all remaining variation.

#### How to use EM to solve factor analysis?

![image-20241030104037109](./ppt 1011.assets/image-20241030104037109.png)
