---
title: è®¡ç®—æœºè§†è§‰ L 10 - L 11
icon: python
date: 2024-10-29 14:11:00
author: XiaoXianYue
isOriginal: true
category: 
    - å¤§ä¸‰ä¸Š
    - è®¡ç®—æœºè§†è§‰
tag:
    - å¤§ä¸‰ä¸Š
    - è®¡ç®—æœºè§†è§‰
sticky: false
star: false
article: true
timeline: true
image: false
navbar: true
sidebarIcon: true
headerDepth: 5
lastUpdated: true
editLink: false
backToTop: true
toc: true
---

## Lecture 10



<img src="./ppt 1011.assets/image-20241029142556692.png" alt="image-20241029142556692" style="zoom: 33%;" />

- **å›å½’ä»»åŠ¡ï¼š**

    - Both x and w are continuous value
    - choose Linear regression to compute the Pr(w|x) 
    - æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ª Probability density function to model Pr(w|x) ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦æ£€æµ‹åœ¨ä¸€ä¸ªç‰¹å®šä¸–ç•ŒçŠ¶æ€ä¸‹æ•°æ®çš„åˆ†å¸ƒã€‚
    - é€šè¿‡ **çº¿æ€§å›å½’** æ¥ç›´æ¥å»ºæ¨¡ Pr(x|w)ï¼Œä»è€Œä¼°è®¡æ•°æ®ä¸ä¸–ç•ŒçŠ¶æ€çš„å…³ç³»ã€‚

    

- **åˆ†ç±»ä»»åŠ¡ï¼š**

    - x is continuous while w are categorical, typibally taking value in {0, 1}
    - **Logistic regression** is chosen to compute Pr(wâˆ£x)Pr(w|x)Pr(wâˆ£x), predicting the probability of each category.
    - å¯¹äº Pr(xâˆ£w)ï¼Œä½¿ç”¨ **æ¦‚ç‡å¯†åº¦å‡½æ•°** æ¥è¡¨ç¤ºæ•°æ®åœ¨ç»™å®šç±»åˆ«ä¸‹çš„åˆ†å¸ƒæƒ…å†µã€‚

- **æ€»ç»“ï¼š**

    - å›å½’ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹å…³æ³¨çš„æ˜¯è¿ç»­æ•°å€¼ä¹‹é—´çš„å…³ç³»ï¼Œé€šè¿‡æ¦‚ç‡å¯†åº¦å‡½æ•°å»ºæ¨¡ã€‚
    - åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹å…³æ³¨çš„æ˜¯æ•°æ®å¦‚ä½•åˆ’åˆ†åˆ°ä¸åŒçš„ç±»åˆ«ï¼Œé€šè¿‡é€»è¾‘å›å½’å’Œæ¦‚ç‡å¯†åº¦å‡½æ•°å®ç°åˆ†ç±»é¢„æµ‹ã€‚

    


### 1.1 **What is hidden or latent variables?**

**Key idea:** represent density **Pr(x)** as marginalization of joint density with another **variable h** that we do not see. In other words, hidden variables are unobserved factors within a model that influence the values of observed variables.

<img src="./ppt 1011.assets/image-20241029152312453.png" alt="image-20241029152312453" style="zoom:50%;" />

Also, depend on some parameters

<img src="./ppt 1011.assets/image-20241029152402267.png" alt="image-20241029152402267" style="zoom: 50%;" />

::: details GPT

**éšè—å˜é‡çš„å®šä¹‰**ï¼š

- éšè—å˜é‡æ˜¯æ¨¡å‹ä¸­æœªç›´æ¥è§‚æµ‹åˆ°çš„å˜é‡ï¼Œä½†å®ƒä»¬å½±å“äº†è§‚æµ‹å˜é‡çš„å€¼æˆ–æ•°æ®ç”Ÿæˆè¿‡ç¨‹ã€‚
- ä¾‹å¦‚ï¼Œåœ¨å­¦ç”Ÿçš„è€ƒè¯•æˆç»©æ•°æ®ä¸­ï¼Œâ€œå­¦ç”Ÿçš„å­¦ä¹ èƒ½åŠ›â€å¯ä»¥è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ªéšè—å˜é‡ã€‚æˆ‘ä»¬æ— æ³•ç›´æ¥è§‚æµ‹åˆ°æ¯ä¸ªå­¦ç”Ÿçš„å­¦ä¹ èƒ½åŠ›ï¼Œä½†å®ƒæ˜¾ç„¶å½±å“äº†å­¦ç”Ÿçš„è€ƒè¯•æˆç»©ã€‚

**éšè—å˜é‡çš„ç”¨é€”**ï¼š

- **æ­ç¤ºæ•°æ®ç»“æ„**ï¼šéšè—å˜é‡èƒ½å¤Ÿæ­ç¤ºå‡ºæ•°æ®çš„æ½œåœ¨ç»“æ„ã€‚é€šè¿‡å¼•å…¥è¿™äº›éšè—å˜é‡ï¼Œå¯ä»¥è§£é‡Šè§‚æµ‹æ•°æ®ä¹‹é—´çš„ç›¸å…³æ€§å’Œç»“æ„ã€‚
- **é™ç»´å’Œä¿¡æ¯å‹ç¼©**ï¼šåœ¨å¤æ‚æ•°æ®ä¸­ï¼Œéšè—å˜é‡å¸¸ç”¨äºå°†æ•°æ®ä»é«˜ç»´ç©ºé—´æ˜ å°„åˆ°ä½ç»´ç©ºé—´ã€‚è¿™æ˜¯é™ç»´çš„ä¸€ç§æ–¹å¼ï¼Œå¯ä»¥å‡å°‘æ•°æ®çš„å¤æ‚åº¦ï¼Œä¾‹å¦‚ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ä¸­éšå«çš„ä¸»æˆåˆ†å°±æ˜¯ä¸€ç§éšè—å˜é‡ã€‚
- **åˆ†ç¦»å™ªå£°**ï¼šéšè—å˜é‡æ¨¡å‹æœ‰æ—¶å¯ä»¥å¸®åŠ©åˆ†ç¦»å‡ºæ•°æ®ä¸­çš„å™ªå£°ï¼Œè¿›è€Œæ›´å‡†ç¡®åœ°åæ˜ å‡ºæ•°æ®çš„çœŸå®æ¨¡å¼ã€‚

**å¤„ç†éšè—å˜é‡çš„ç®—æ³•**ï¼š

- **æœŸæœ›æœ€å¤§åŒ–ç®—æ³•ï¼ˆEMç®—æ³•ï¼‰**ï¼šEMç®—æ³•æ˜¯å¸¸ç”¨æ¥ä¼°è®¡å¸¦æœ‰éšè—å˜é‡çš„æ¨¡å‹å‚æ•°çš„ç®—æ³•ã€‚å®ƒåŒ…å«ä¸¤æ­¥ï¼šåœ¨â€œEæ­¥â€ä¸­ï¼Œä¼°è®¡éšè—å˜é‡çš„åˆ†å¸ƒï¼›åœ¨â€œMæ­¥â€ä¸­ï¼ŒåŸºäºä¼°è®¡çš„åˆ†å¸ƒï¼Œæœ€å¤§åŒ–å‚æ•°çš„å¯¹æ•°ä¼¼ç„¶ã€‚

:::

### 1.2 What is **Expectation Maximization**?

::: tabs

@tab PPT

<img src="./ppt 1011.assets/image-20241030104400487.png" alt="image-20241030104400487" style="zoom:50%;" />

@tab GPT 

Expectation Maximization (EM) is an iterative algorithm used in statistics to find maximum likelihood estimates of parameters in models that have latent (hidden) variables or missing data. 

![image-20241029155544401](./ppt 1011.assets/image-20241029155544401.png)

Step:

- Defines **a lower bound** on log likelihood and increases bound iteratively. Lower bound is a *function* of parameters **q** and a set of probability distributions ğ‘ğ‘– (ğ’‰ğ‘– )
- E-Step â€“  Updating the probability distributions {ğ‘ğ‘– ğ’‰ğ‘– }ğ‘– ğ¼ =1 to improve the bound.
- M-Step â€“ updating the parameters ğœƒ to improve the bound.

:::

### 1.3 **What are the advantages of EM?**

ä¼šå‡ºåˆ¤æ–­é¢˜ã€‚

<img src="./ppt 1011.assets/image-20241030104126838.png" alt="image-20241030104126838" style="zoom: 50%;" />

### 1.4 **What is Mixture of Gaussians (MoG)? How to use EM to solve MoG?**

#### What is Mixture of Gaussians (MoG)?

*The mixture of Gaussians (MoG) is a prototypical example of a model where learning is suited to the EM algorithm.* The data are described as a weighted sum of K normal distributions.

<img src="./ppt 1011.assets/image-20241030090606123.png" alt="image-20241030090606123" style="zoom: 67%;" />



#### How to use EM to solve MoG?

::: tabs

@tab è‡ªå·±æ€»ç»“

1. **Goal**

The goal is "to learn parameters $Î¸={{Î»_1â€¦_k,Î¼_1â€¦_k,Î£_1â€¦_k}}$ from training data $x_1,â€¦,x_I$."

2. **E-Step**

In the E-step, we fix parameters  $Î¸$ , maximize bound with respect to (w.r.t.) distributions $q_i(h_i)$ by calculating "the responsibility of the $k^{th}$ Gaussian for the $i^{th}$ data point.

- For each data point $x_i$, we calculate the posterior probability distribution $Pr(h_i=k | x_i, \theta^{(t)})$.

3. **M-Step**

In the M-step, we fix distributions $q_i(h_i)$ ,maximize bound with respect to w.r.t. parameters Î¸ = ![image-20241030100202835](./ppt 1011.assets/image-20241030100202835.png)

- We compute the updated parameters 
- ![image-20241030094713212](./ppt 1011.assets/image-20241030094713212.png)
- Then, we updated parameter values. 

4. **Iterate Until Convergence**

Alternates E-steps and M-Steps, until the model parameters converge, the (local) maximum of the actual log likelihood is approached.

@tab PPT

![image-20241030100556556](./ppt 1011.assets/image-20241030100556556.png)

:::

### 1.5 **What is t-distributions? How to use EM to solve t-distributions?**

#### What is t-distributions?

The **Student's t-distribution** is described as a distribution robust against data with heavy tails. It is defined as:

![image-20241030101233907](./ppt 1011.assets/image-20241030101233907.png)

where the degrees of freedom $Î½âˆˆ(0,âˆ)$ control the length of the tails; a smaller Î½ implies a heavier tail, meaning more weight in the tails.

#### How to use EM to solve t-distributions?

![image-20241030102923829](./ppt 1011.assets/image-20241030102923829.png)



### **1.6 What is factor analysis? How to use EM to solve factor analysis?**

#### What is factor analysis? 

The probability density function of a factor analyzer is given by:

![image-20241030103623110](./ppt 1011.assets/image-20241030103623110.png)

where the covariance matrix  $\Phi\Phi^T + \Sigma$  contains a sum of two terms. The first term, $\Phi\Phi^T$ describes a full covariance model over the subspace with the factors (columns of matrix $\Phi$) determining the subspace modeled. The factors are latent variables used to explain the correlations between observed variables. The second term, $\Sigma$, is a diagonal matrix that accounts for all remaining variation.

#### How to use EM to solve factor analysis?

![image-20241030104037109](./ppt 1011.assets/image-20241030104037109.png)







## Lecture 11

### 1. Directed graphical models

#### 1.1 Difination

- Directed graphical model represents probability distribution that factorizes as a product of conditional probability distributions.

    <img src="./ppt_1011.assets/image-20241105182230747.png" alt="image-20241105182230747" style="zoom: 50%;" />

#### 1.2 To draw the graph:

â€¢ Add one term per node in the graph Pr(xn | xpa[n])

â€¢ If no parents then just add Pr(xn )

- Example:

    ![image-20241105182530875](./ppt_1011.assets/image-20241105182530875.png)

::: tips

Pr(xi | xi çš„æ‰€æœ‰çˆ¶èŠ‚ç‚¹)

:::

#### 1.3 Markov Blanket

**Example 1**

ä»¥ä¸‹å›¾ä¸ºä¾‹ï¼Œç»™å‡ºäº†èŠ‚ç‚¹ $x_8$ çš„ Markov Blanket.

<img src="./ppt_1011.assets/image-20241105194601534.png" alt="image-20241105194601534" style="zoom:33%;" />

- Markov Blanket é€šå¸¸åŒ…æ‹¬è¯¥èŠ‚ç‚¹çš„
    - çˆ¶èŠ‚ç‚¹
    - å­èŠ‚ç‚¹
    - å­èŠ‚ç‚¹çš„å…¶ä»–çˆ¶èŠ‚ç‚¹
- é©¬å°”å¯å¤«æ¯¯çš„æ¦‚å¿µæ„å‘³ç€ **ç»™å®šé©¬å°”å¯å¤«æ¯¯ä¸­çš„èŠ‚ç‚¹ï¼Œ$x_8$ ä¸ç½‘ç»œä¸­å…¶ä»–æ‰€æœ‰èŠ‚ç‚¹æ¡ä»¶ç‹¬ç«‹**ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œé©¬å°”å¯å¤«æ¯¯ä¸­çš„èŠ‚ç‚¹åŒ…å«äº†æ‰€æœ‰ç›´æ¥å½±å“ $x_8$ çš„ä¿¡æ¯ï¼Œä½†å®ƒä»¬æœ¬èº«å¹¶ä¸ä¸€å®šç›¸äº’æ¡ä»¶ç‹¬ç«‹ã€‚
- æ ¹æ®ä¸Šè¿°æ¡ä»¶æ¦‚ç‡çš„å®šä¹‰ï¼Œæˆ‘ä»¬å¯çŸ¥ï¼Œåœ¨ç»™å®š$x_8$çš„ Markov Blanket çš„æ¡ä»¶ä¸‹ï¼Œ$x_8$ ä¸å›¾ä¸­å…¶ä»–èŠ‚ç‚¹æ¡ä»¶ç‹¬ç«‹ã€‚



**Example 2**

<img src="./ppt_1011.assets/image-20241105195250349.png" alt="image-20241105195250349" style="zoom:33%;" />

- æ˜¾ç„¶ï¼Œåœ¨ç»™å®š $x_2$ çš„æ¡ä»¶ä¸‹ï¼Œ$x_1$ å’Œ $x_3$ æ¡ä»¶ç‹¬ç«‹ã€‚æ‰€ä»¥ï¼š
    $Pr(x_1,x_2,x_3)=Pr(x_1)â‹…Pr(x_2âˆ£x_1)â‹…Pr(x_3âˆ£x_2)$







### 2. Undirected graphical models

#### 2.1 Probability distribution factorize as

Markov Random field

<img src="./ppt_1011.assets/image-20241105201959279.png" alt="image-20241105201959279" style="zoom:50%;" />

- ç”¨ Gibbs Distribution è¡¨ç¤º

    **å‰å¸ƒæ–¯åˆ†å¸ƒå½¢å¼**ï¼šæ— å‘å›¾æ¨¡å‹çš„æ¦‚ç‡åˆ†å¸ƒå¯ä»¥å†™æˆå‰å¸ƒæ–¯åˆ†å¸ƒï¼š $Pr(x_1, \dots, x_N) = \frac{1}{Z} \exp\left(-\sum_{c=1}^{C} \psi_c[x_1, \dots, x_N]\right)$

    **æˆæœ¬å‡½æ•° Ïˆ\psiÏˆ**ï¼šé€šè¿‡å®šä¹‰åŠ¿å‡½æ•°çš„è´Ÿå¯¹æ•°æ¥å¼•å…¥**æˆæœ¬å‡½æ•°** $\psi_c[x_1, \dots, x_N] = -\log(\phi_c[x_1, \dots, x_N])$

- å›¢ï¼ˆCliquesï¼‰

    åœ¨æ— å‘å›¾ä¸­ï¼Œæ¯ä¸ªåŠ¿å‡½æ•°ä½œç”¨äºå˜é‡çš„ä¸€ä¸ªå­é›†ï¼Œè¿™ä¸ªå­é›†è¢«ç§°ä¸º**å›¢**ã€‚

    <img src="./ppt_1011.assets/image-20241105202554294.png" alt="image-20241105202554294" style="zoom:50%;" />

â€‹	**å›¢çš„æ„ä¹‰**ï¼šå¯¹äºæ¯ä¸ªå›¢ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰åŠ¿å‡½æ•°ï¼Œç”¨ä»¥æè¿°è¯¥å›¢ä¸­å˜é‡çš„ç›¸äº’ä¾èµ–æ€§ã€‚åœ¨è®¡ç®—æœº	è§†è§‰ç­‰åº”ç”¨ä¸­ï¼ŒåŠ¿å‡½æ•°é€šå¸¸ä½œç”¨äºå˜é‡çš„å­é›†è€Œä¸æ˜¯æ•´ä¸ªå˜é‡é›†ã€‚

â€‹	**é©¬å°”ç§‘å¤«éšæœºåœºï¼ˆMRFï¼‰**ï¼šæ— å‘å›¾æ¨¡å‹é€šå¸¸è¡¨ç¤ºä¸ºé©¬å°”ç§‘å¤«éšæœºåœºï¼Œå®šä¹‰äº†ç¦»æ•£æˆ–è¿ç»­å˜é‡é—´çš„å…³ç³»ã€‚

#### 2.2 To visualize graphical model from factorization

::: info

**è¯†åˆ«å˜é‡**ï¼šå…¬å¼ä¸­çš„ $x_1, x_2, \ldots, x_N$ æ˜¯æ¨¡å‹çš„éšæœºå˜é‡ã€‚æ¯ä¸ªå˜é‡å°†å¯¹åº”æ— å‘å›¾ä¸­çš„ä¸€ä¸ªèŠ‚ç‚¹ã€‚

**è¯†åˆ«åŠ¿å‡½æ•°ï¼ˆPotential Functionsï¼‰**ï¼šå…¬å¼ä¸­çš„åŠ¿å‡½æ•° $\phi_c$ è¡¨ç¤ºç‰¹å®šå˜é‡å­é›†ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚æ¯ä¸ªåŠ¿å‡½æ•°é€šå¸¸ä½œç”¨äºä¸€ç»„å˜é‡ï¼Œè¿™äº›å˜é‡æ„æˆä¸€ä¸ªå›¢ï¼ˆCliqueï¼‰ã€‚å›¢æ˜¯æ— å‘å›¾ä¸­å®Œå…¨è¿æ¥çš„å­å›¾ï¼Œå…¶ä¸­æ¯å¯¹èŠ‚ç‚¹ä¹‹é—´éƒ½æœ‰è¾¹ã€‚

**ç¡®å®šè¾¹çš„è¿æ¥**ï¼šå¦‚æœæŸä¸ªåŠ¿å‡½æ•°ä½œç”¨äºä¸€ç»„å˜é‡ï¼ˆæ¯”å¦‚ $\phi_c[x_1, x_2]$ ä½œç”¨äº $x_1$ å’Œ $x_2$ï¼‰ï¼Œåˆ™åœ¨æ— å‘å›¾ä¸­ç»˜åˆ¶è¿™ä¸¤ä¸ªå˜é‡ä¹‹é—´çš„ä¸€æ¡è¾¹ã€‚å¦‚æœåŠ¿å‡½æ•°æ¶‰åŠå¤šä¸ªå˜é‡ï¼ˆå¦‚ $\phi_c[x_1, x_2, x_3]$ï¼‰ï¼Œåˆ™åœ¨æ— å‘å›¾ä¸­æ„å»ºä¸€ä¸ªåŒ…å«è¿™äº›å˜é‡çš„å®Œå…¨è¿æ¥å­å›¾ã€‚

**é‡å¤æ­¤è¿‡ç¨‹**ï¼šå¯¹äºæ¯ä¸ªåŠ¿å‡½æ•°ï¼ŒæŒ‰ç…§å…¶åŒ…å«çš„å˜é‡ç¡®å®šç›¸åº”çš„å›¢ï¼Œå¹¶åœ¨æ— å‘å›¾ä¸­è¿æ¥ç›¸åº”çš„èŠ‚ç‚¹ï¼Œç›´åˆ°æ‰€æœ‰åŠ¿å‡½æ•°éƒ½è¢«æ˜ å°„åˆ°å›¾ä¸­ã€‚

:::

- Sketch one node per random variable

- For every clique, sketch connection from every node to every other



#### 2.3 Example

- **Example 1**

    <img src="./ppt_1011.assets/image-20241105205055830.png" alt="image-20241105205055830" style="zoom:50%;" />

    **One set of nodes is conditionally independent of another given a third if the third set separates them (i.e. Blocks any path from the first node to the second)**

â€‹	ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨æ— å‘å›¾ä¸­ï¼Œåªè¦ä¸¤ä¸ªèŠ‚ç‚¹ä¸ç›´æ¥ç›¸è¿ï¼Œåœ¨ç»™å®šä¸å…¶ä¸­ä¸€ä¸ªèŠ‚ç‚¹ç›´æ¥ç›¸è¿çš„æ‰€æœ‰èŠ‚	ç‚¹é›†çš„æ¡ä»¶ä¸‹ï¼Œè¿™ä¸¤ä¸ªèŠ‚ç‚¹æ¡ä»¶ç‹¬ç«‹ã€‚

â€‹	å†æ¢å¥è¯è¯´ï¼Œåœ¨æ— å‘å›¾ä¸­ï¼Œç»™å®šä¸€ä¸ªèŠ‚ç‚¹çš„ç›´æ¥é‚»å±…é›†åˆï¼Œè¯¥èŠ‚ç‚¹ä¸æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹æ¡ä»¶ç‹¬ç«‹ã€‚	æ¢å¥è¯è¯´ï¼Œåªè¦çŸ¥é“ä¸€ä¸ªèŠ‚ç‚¹çš„ç›´æ¥é‚»å±…ï¼Œå°±å¯ä»¥å¿½ç•¥å›¾ä¸­æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹çš„ä¿¡æ¯æ¥é¢„æµ‹è¯¥èŠ‚ç‚¹	çš„çŠ¶æ€ã€‚

â€‹	å› æ­¤ï¼Œä¸€ä¸ªèŠ‚ç‚¹çš„ç›´æ¥é‚»å±…é›†åˆå°±æ˜¯å®ƒçš„**é©¬å°”å¯å¤«æ¯¯**ã€‚

- **Example 2**

    <img src="./ppt_1011.assets/image-20241105205134309.png" alt="image-20241105205134309" style="zoom:50%;" />

â€‹	è¿™ä¸ªå›¾çš„æ¦‚ç‡åˆ†å¸ƒå› å­å¼å¯ä»¥æ˜¯ï¼š

$Pr(x_1,x_2,x_3,x_4,x_5)=Z1Ï•1[x_1,x_2,x_3]Ï•2[x_2,x_4]Ï•3[x_3,x_5]Ï•4[x_4,x_5]$	$Pr(x_1, x_2, x_3, x_4, x_5) = \frac{1}{Z} (\phi_1[x_1, x_2] \phi_2[x_2, x_3]) \phi_3[x_1, x_3] \phi_4[x_2, x_4], \phi_5[x_3, x_5] \phi_6[x_4, x_5]$



### 3. MRF and CRF

#### 3.1 Markov Random Fields

- åŸºäº Undirected graphical modell
- Contextual constraints (spatial, temporal) , connect neighbors.
- Neighborhood relations define cliques

**å…¬å¼è¡¨è¾¾: **

$P(Y_i | Y_j \text{ for all } j \neq i) = P(Y_i | N_i)$

å…¶ä¸­ï¼Œ$N_i$è¡¨ç¤ºèŠ‚ç‚¹ $i$ çš„é‚»å±…é›†åˆã€‚å³ç»™å®šèŠ‚ç‚¹ $i$ çš„é‚»å±…é›†åˆ $N_i$ï¼ŒèŠ‚ç‚¹ $Y_i$ ä¸å…¶ä»–èŠ‚ç‚¹æ¡ä»¶ç‹¬ç«‹ã€‚

#### 3.2 Conditional Random Field

- MRF specifies joint distribution on Y
- For any probability distribution, you can condition it on some other variables X

::: tip

**MRFåœ¨æ¡ä»¶ä¸Šçš„æ‰©å±•**ï¼šCRFå¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªåœ¨æŸäº›ç»™å®šæ¡ä»¶ï¼ˆé€šå¸¸æ˜¯ç‰¹å¾å˜é‡é›† Xï¼‰ä¸‹çš„MRFã€‚

**æ¡ä»¶åˆ†å¸ƒ**ï¼šCRFä¸ç›´æ¥å»ºæ¨¡å˜é‡ Y çš„è”åˆåˆ†å¸ƒï¼Œè€Œæ˜¯åœ¨ç»™å®šå…¶ä»–å˜é‡ X çš„æ¡ä»¶ä¸‹å¯¹ Y çš„åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒCRFå¯ä»¥æ›´å¥½åœ°é€‚åº”æœ‰ç›‘ç£å­¦ä¹ é—®é¢˜ã€‚

**CRFçš„å®šä¹‰**ï¼šCRFå¯ä»¥ç†è§£ä¸º**åœ¨ X æ¡ä»¶ä¸‹çš„MRF**ï¼Œå› æ­¤å®ƒçš„ä¾èµ–å…³ç³»ä¸ä»…å—é‚»å±…å½±å“ï¼Œè¿˜å—ç»™å®šçš„æ¡ä»¶å˜é‡ X å½±å“

:::

![image-20241105211454550](./ppt_1011.assets/image-20241105211454550.png)



<img src="./ppt_1011.assets/image-20241105211746313.png" alt="image-20241105211746313" style="zoom:50%;" />











