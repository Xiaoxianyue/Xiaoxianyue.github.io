---
title: RNNâ€”â€”recurrent neural network
icon: alias
date: 2025-06-01 23:03:32
author: XiaoXianYue
isOriginal: true
category: 
    - æ·±åº¦å­¦ä¹ 
tag:
    - æ·±åº¦å­¦ä¹ 
sticky: false
star: false
article: true
timeline: true
image: false
navbar: true
sidebarIcon: true
headerDepth: 5
lastUpdated: true
editLink: false
backToTop: true
toc: true
---

## 1. Introduction

Convolutional neural networks are generally used to process **spatial information**, while recurrent neural networks are generally used to process **sequential information** in the **temporal domain**.

### 1.1 Example 1-Stock series prediction

![image-20250601232435674](./RNN.assets/image-20250601232435674.png)

#### 1. Strategy 01 â€”â€” Auto regressive model

è‡ªå›å½’æ¨¡å‹ï¼Œåªä¾èµ–äºæœ€è¿‘çš„ Ï„ ä¸ªæ—¶é—´æ­¥ï¼Œè€Œä¸æ˜¯æ‰€æœ‰å†å²ä¿¡æ¯ã€‚å› ä¸ºæ¨¡å‹åªä½¿ç”¨å›ºå®šæ•°é‡ï¼ˆÏ„ï¼‰çš„å†å²ä¿¡æ¯ï¼Œæ‰€ä»¥æ— è®ºæ•°æ®å¤šé•¿ï¼Œæ¨¡å‹å¤æ‚åº¦ä¸å˜ã€‚

![image-20250601233505729](./RNN.assets/image-20250601233505729.png)

#### 2. Strategy 02 â€”â€” Latent auto-regressive model

![image-20250601234029459](./RNN.assets/image-20250601234029459.png)

The hidden state at the next time-point is determined by **the hidden state and the input at the previous time-point**.

Finally, the hidden state is used to predict the prices at different time-points.

The hidden state **cannot be seen** if it is not printed, so it is called a hidden variable.

### 1.2 Practice

#### 1. Generate time-price sequence data for stocks

- Generate sequence data using the **sine function** and **the noise function**, with the **maximum time value** being 1000.

![image-20250601234836043](./RNN.assets/image-20250601234836043.png)

ä½¿ç”¨æ­£å¼¦å‡½æ•°å’Œå™ªå£°å‡½æ•°ç”Ÿæˆåºåˆ—æ•°æ®ï¼Œæœ€å¤§æ—¶é—´å€¼ä¸º1000ã€‚

#### 2. Set parameter T

![image-20250601235025631](./RNN.assets/image-20250601235025631.png)

â€¢ æ ¹æ®ç»Ÿè®¡æ¨¡å‹ä¸­çš„è‡ªå›å½’æ¨¡å‹åŸç†ï¼Œå°†å‚æ•°è®¾ç½®ä¸º4ã€‚

â€¢ è¿™æ„å‘³ç€æˆ‘ä»¬éœ€è¦ä½¿ç”¨å‰å››å¤©çš„è‚¡ç¥¨ä»·æ ¼æ¥é¢„æµ‹å½“å‰çš„è‚¡ç¥¨ä»·æ ¼ã€‚

â€¢ ç„¶è€Œè¿™é‡Œä¼šé‡åˆ°ä¸€ä¸ªé—®é¢˜ï¼šå½“tç­‰äº1ã€2æˆ–3æ—¶ï¼Œæˆ‘ä»¬æ— æ³•é¢„æµ‹è‚¡ç¥¨ä»·æ ¼ã€‚

â€¢ è¿™æ—¶å¯ä»¥é‡‡å–çš„æ–¹æ³•æ˜¯å¯¹tç­‰äº1ã€2æˆ–3çš„å€¼è¿›è¡Œå¡«å……ï¼ˆpaddingï¼‰ï¼Œæˆ–è€…ç›´æ¥èˆå¼ƒè¿™äº›æ•°æ®ç‚¹ã€‚

#### 3. Select training data

![image-20250601235648022](./RNN.assets/image-20250601235648022.png)

`batch_size, n_train = 16, 600`
æ‰¹é‡å¤§å°ï¼ˆbatch_sizeï¼‰å’Œè®­ç»ƒæ•°æ®é‡ï¼ˆn_trainï¼‰åˆ†åˆ«è®¾ç½®ä¸º16å’Œ600ã€‚

â€¢ Select the first 600 **input-label pairs** as the training data.
 é€‰æ‹©å‰600ä¸ªè¾“å…¥-æ ‡ç­¾å¯¹ä½œä¸ºè®­ç»ƒæ•°æ®ã€‚

ğŸ’¡ **è§£é‡Š**ï¼š

- è¿™é‡Œå±•ç¤ºäº†åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­ï¼Œé€šå¸¸ä¼šå°†æ—¶é—´æ­¥åˆ’åˆ†ä¸ºè¾“å…¥ï¼ˆå¦‚å‰4å¤©çš„ä»·æ ¼ï¼‰å’Œè¾“å‡ºï¼ˆç¬¬5å¤©çš„ä»·æ ¼ï¼‰ã€‚
- åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†å‰600ä¸ªæ ·æœ¬ï¼ˆè¾“å…¥-æ ‡ç­¾å¯¹ï¼‰ç”¨äºæ¨¡å‹çš„è®­ç»ƒã€‚



#### 4. construct the prediction network

![image-20250601235739442](./RNN.assets/image-20250601235739442.png)

``features = [self.x[i : self.T-self.tau+i] for i in range(self.tau)]``
 â€¢ é€šè¿‡åˆ—è¡¨æ¨å¯¼å¼ç”Ÿæˆç‰¹å¾ï¼š`features = [self.x[i : self.T-self.tau+i] for i in range(self.tau)]`
 ï¼ˆå«ä¹‰ï¼šç”¨è¿‡å»Ï„å¤©çš„æ•°æ®ç”Ÿæˆæ¨¡å‹è¾“å…¥ç‰¹å¾ã€‚ï¼‰

``def get_net():``
 â€¢ å®šä¹‰é¢„æµ‹ç½‘ç»œã€‚

``net = nn.Sequential(``
 â€¢ ä½¿ç”¨PyTorchçš„`nn.Sequential`æ¨¡å—æŒ‰é¡ºåºå †å æ¨¡å‹ã€‚

``nn.Linear(4, 10),``
 â€¢ ä¸€ä¸ªçº¿æ€§å±‚ï¼Œå°†è¾“å…¥çš„4ç»´ç‰¹å¾æ˜ å°„åˆ°10ç»´éšè—å±‚ã€‚

``nn.ReLU(),``
 â€¢ ReLUæ¿€æ´»å‡½æ•°ã€‚

``nn.Linear(10, 1)``
 â€¢ ä¸€ä¸ªçº¿æ€§å±‚ï¼Œå°†10ç»´éšè—å±‚è¾“å‡ºæ˜ å°„ä¸º1ç»´é¢„æµ‹è¾“å‡ºã€‚

``net.apply(init_weights)``
 â€¢ å¯¹ç½‘ç»œæƒé‡è¿›è¡Œåˆå§‹åŒ–ã€‚

``return net``
 â€¢ è¿”å›æ„å»ºå¥½çš„ç½‘ç»œã€‚

#### 5. å¼€å§‹è®­ç»ƒ

![image-20250602000259172](./RNN.assets/image-20250602000259172.png)

#### 6. Testing

![image-20250602000329858](./RNN.assets/image-20250602000329858.png)

When predicting the value at time-point 604, all 600 training data points are utilized.
 â€¢ å½“é¢„æµ‹æ—¶é—´ç‚¹604çš„å€¼æ—¶ï¼Œä½¿ç”¨äº†æ‰€æœ‰600ä¸ªè®­ç»ƒæ•°æ®ç‚¹ã€‚

Then, **looking at the data at time-points after 604, such as 605, 606, etc**., we can see that the single-step prediction still works quite well.
 â€¢ æ¥ç€ï¼Œå½“è§‚å¯Ÿ604ä¹‹åçš„æ—¶é—´ç‚¹ï¼ˆå¦‚605ã€606ç­‰ï¼‰æ—¶ï¼Œå¯ä»¥çœ‹å‡ºå•æ­¥é¢„æµ‹çš„æ•ˆæœä¾ç„¶ä¸é”™ã€‚

That is, the **purple line fits the blue data very well**.
 â€¢ æ¢å¥è¯è¯´ï¼Œç´«è‰²çš„é¢„æµ‹æ›²çº¿å¾ˆå¥½åœ°æ‹Ÿåˆäº†è“è‰²çš„çœŸå®æ•°æ®ã€‚

![image-20250602000538952](./RNN.assets/image-20250602000538952.png)

- Multi-step prediction refers to **predicting data at multiple future time points**.
     å¤šæ­¥é¢„æµ‹æŒ‡çš„æ˜¯åœ¨å¤šä¸ªæœªæ¥æ—¶é—´ç‚¹è¿›è¡Œæ•°æ®é¢„æµ‹ã€‚

- Itâ€™s like predicting the weather 1 day, 3 days, and 7 days in the future. **The greater the number of days apart, the worse the prediction effect**.
     è¿™å°±åƒæ˜¯é¢„æµ‹æœªæ¥1å¤©ã€3å¤©æˆ–7å¤©çš„å¤©æ°”ä¸€æ ·ã€‚é¢„æµ‹çš„æ—¶é—´é—´éš”è¶Šå¤§ï¼Œé¢„æµ‹çš„æ•ˆæœå°±è¶Šå·®ã€‚

- This is actually caused by the accumulation of errors.
     è¿™å®é™…ä¸Šæ˜¯ç”±è¯¯å·®çš„ç´¯ç§¯é€ æˆçš„ã€‚

- For example, each k-step prediction accumulates **data errors**.
     ä¾‹å¦‚ï¼Œæ¯è¿›è¡Œä¸€æ¬¡kæ­¥é¢„æµ‹ï¼Œéƒ½ä¼šç§¯ç´¯æ•°æ®è¯¯å·®ã€‚

- Suppose the first prediction accumulates an error of **m1**.
     å‡è®¾ç¬¬ä¸€æ¬¡é¢„æµ‹äº§ç”Ÿäº†m1çš„è¯¯å·®ã€‚

- Then, when making the next prediction, this error m1 is added to the input, and the data error will also accumulate, becoming **cm1 + data error**, where c is a constant.
     ç„¶åï¼Œåœ¨è¿›è¡Œä¸‹ä¸€æ¬¡é¢„æµ‹æ—¶ï¼Œè¿™ä¸ªm1çš„è¯¯å·®ä¼šè¢«åŠ åˆ°è¾“å…¥ä¸Šï¼ŒåŒæ—¶æ•°æ®è¯¯å·®ä¹Ÿä¼šç§¯ç´¯ï¼Œæœ€ç»ˆè¯¯å·®ä¼šå˜æˆcm1 + æ•°æ®è¯¯å·®ï¼Œå…¶ä¸­cæ˜¯ä¸€ä¸ªå¸¸æ•°ã€‚
- In this example, the **auto regressive model** is used instead of the hidden autoregressive model.
     åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œä½¿ç”¨çš„æ˜¯è‡ªå›å½’æ¨¡å‹ï¼Œè€Œä¸æ˜¯éšè—è‡ªå›å½’æ¨¡å‹ã€‚
     ğŸ‘‰ è§£é‡Šï¼š
     è¿™é‡Œçš„â€œauto regressive modelâ€æ˜¯æŒ‡æ¨¡å‹ç›´æ¥åˆ©ç”¨å†å²è§‚æµ‹å€¼è¿›è¡Œé¢„æµ‹ï¼Œè€Œæ²¡æœ‰å¼•å…¥éšè—çŠ¶æ€ï¼ˆä¾‹å¦‚RNNçš„éšè—å±‚ï¼‰ã€‚

- Judging from the constructed model, although a Multi-Layer Perceptron (MLP) is built as the prediction network, this model directly predicts the current value based on historical data.
     ä»æ‰€æ„å»ºçš„æ¨¡å‹æ¥çœ‹ï¼Œè™½ç„¶é¢„æµ‹ç½‘ç»œé‡‡ç”¨çš„æ˜¯å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ï¼Œä½†è¿™ä¸ªæ¨¡å‹æ˜¯ç›´æ¥åŸºäºå†å²æ•°æ®é¢„æµ‹å½“å‰å€¼çš„ã€‚
     ğŸ‘‰ è§£é‡Šï¼š
     MLPè™½ç„¶æœ‰éšè—å±‚ï¼Œä½†å®ƒä»¬åœ¨è¿™é‡Œåªæ˜¯éçº¿æ€§å˜æ¢ï¼Œå¹¶æ²¡æœ‰å¼•å…¥â€œéšå˜é‡â€æ¦‚å¿µã€‚æ¨¡å‹ä»ç„¶æ˜¯ç›´æ¥åŸºäºå·²çŸ¥çš„è¾“å…¥æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚

- **It does not introduce hidden variables** to summarize the historical pattern features and thus indirectly influence the prediction.
     å®ƒæ²¡æœ‰å¼•å…¥éšè—å˜é‡æ¥æ€»ç»“å†å²æ¨¡å¼ç‰¹å¾ï¼Œå› æ­¤ä¸ä¼šé—´æ¥åœ°å½±å“é¢„æµ‹ã€‚
     ğŸ‘‰ è§£é‡Šï¼š
     ä¸RNNä¸åŒï¼ŒRNNä¸­çš„éšè—çŠ¶æ€å¯ä»¥åœ¨æ—¶é—´æ­¥ä¹‹é—´ä¼ é€’ä¿¡æ¯ï¼Œæ•æ‰åºåˆ—ä¾èµ–ï¼Œè€ŒMLPè¿™é‡Œåˆ™ç›´æ¥ä½¿ç”¨è¾“å…¥æ•°æ®ï¼Œå°‘äº†åºåˆ—å†…éƒ¨çš„æ—¶é—´è®°å¿†èƒ½åŠ›ã€‚



## 2. Introduction 02

### 2.1 Example 2 â€”â€” Text prediction

#### 1. Text preprocessing

![image-20250602221501225](./RNN.assets/image-20250602221501225.png)

==è¿™æ˜¯è¾“å‡ºç¬¬ 11 è¡Œæ–‡æœ¬çš„å†…å®¹ï¼Œç„¶åè¿›è¡Œæ‰“å°tokenï¼ˆåˆ†è¯ï¼‰==

![image-20250602221705098](./RNN.assets/image-20250602221705098.png)

==å•è¯ï¼šæ•°å€¼ indices ; è¯­æ–™åº“ corpus==

```python
vocab = Vocab(tokens) #è¡¨ç¤ºä½¿ç”¨ tokens åˆ›å»ºä¸€ä¸ªè¯æ±‡è¡¨
[('unk', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)] # è¿™æ˜¯è¯æ±‡è¡¨ä¸­å‰ 10 ä¸ª token åŠå…¶ç´¢å¼•çš„ç¤ºä¾‹
```

![image-20250602222242154](./RNN.assets/image-20250602222242154.png)

#### 2. Build a language model

![image-20250602222727824](./RNN.assets/image-20250602222727824.png)

ä¼°ç®—ä¸€ä¸ªå¥å­çš„è”åˆæ¦‚ç‡

![image-20250602223033112](./RNN.assets/image-20250602223033112.png)

å³è¾¹è®²çš„æ˜¯ï¼Œè¿™äº›æ¦‚ç‡å°±æ˜¯è¯­è¨€æ¨¡å‹çš„å‚æ•°ã€‚

æ–¹æ³•1â€”â€”ç›´æ¥è¯å¯¹ï¼šdeep learning ä¸€èµ·å‡ºç°çš„æ¦‚ç‡ã€‚

æ–¹æ³•2â€”â€”é©¬å°”ç§‘å¤«æ¨¡å‹ï¼šç®—ä¸‹ä¸€ä¸ªè¯å‡ºç°æ¦‚ç‡çš„æ—¶å€™ï¼Œå¾€å‰è€ƒè™‘å¤šå°‘æ­¥è¿™æ ·ã€‚

æ–¹æ³•1 ä¼šä¸¥é‡é­é‡æ•°æ®ç¨€ç–é—®é¢˜ï¼Œè€Œæ–¹æ³•2åœ¨ n è¾ƒå°æ—¶å¯ä»¥æ›´å¥½åœ°ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚

å·¦è¾¹è®²çš„æ˜¯ï¼Œæ•°æ®ç¨€ç–çš„é—®é¢˜ï¼Œä¸‡ä¸€è¯­æ–™åº“ deep learningè¿™ä¸ªè¯å¯¹å‡ºç°çš„å¾ˆå°‘ï¼Œä¼šå¯¼è‡´æ•´ä¸ªæ¦‚ç‡éƒ½å¾ˆå°ã€‚

#### Practice

![image-20250602224505065](./RNN.assets/image-20250602224505065.png)

éšç€è¯æ•°çš„å¢åŠ ï¼Œé¢‘ç‡ä¸‹é™çš„éå¸¸å¿«

![image-20250602225447661](./RNN.assets/image-20250602225447661.png)

ç„¶åæˆ‘ä»¬è¿›è¡Œè¯ç»„çš„é¢„æµ‹ï¼Œbigramæ˜¯ä¸¤ä¸ªè¯ç»„æˆçš„è¯ç»„ï¼Œtrigramæ˜¯ä¸‰ä¸ªè¯ï¼Œå¯ä»¥çœ‹åˆ°ä¸‰ä¸ªè¯çš„è¯ç»„é¢‘ç‡æ›´ä½äº†ï¼

![image-20250602230229246](./RNN.assets/image-20250602230229246.png)

å½“åºåˆ—å¤ªé•¿è€Œæ¨¡å‹ä¸€æ¬¡æ— æ³•å…¨éƒ¨å¤„ç†æ—¶ï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›å°†è¿™æ ·çš„åºåˆ—æ‹†åˆ†ä»¥ä¾¿æ¨¡å‹æ›´å®¹æ˜“å¤„ç†ã€‚ã€

æˆ‘ä»¬å¦‚ä½•ä»è¿™ä¸ª mini-batch åºåˆ—æ•°æ®ä¸­æå–ç‰¹å¾å’Œæ ‡ç­¾ï¼Ÿ

**è®²è§£**ï¼š

- è¿™é‡Œæåˆ° mini-batch æ˜¯æŒ‡ä¸€æ¬¡æ€§é€å…¥æ¨¡å‹çš„ä¸€æ‰¹å­åºåˆ—ï¼ˆåˆ†æ®µï¼‰ã€‚
- â€œç‰¹å¾â€å°±æ˜¯è¾“å…¥ï¼Œé€šå¸¸æ˜¯æŸæ®µæ–‡æœ¬çš„å‰ n ä¸ªå­—ç¬¦ã€‚
- â€œæ ‡ç­¾â€å°±æ˜¯å¯¹åº”çš„é¢„æµ‹ç›®æ ‡ï¼ˆå¦‚æ–‡æœ¬çš„ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼‰ã€‚

ä¹Ÿå°±æ˜¯è¯´ï¼Œç¥ç»ç½‘ç»œæ¯æ¬¡åªå¤„ç†è·¨è¶Š n ä¸ªæ—¶é—´ç‚¹çš„å­åºåˆ—ã€‚

**è®²è§£**ï¼š

- ä¾‹å¦‚å¦‚æœ n=5ï¼Œå°±è¡¨ç¤ºæ¯ä¸ª mini-batch å­åºåˆ—é•¿åº¦æ˜¯ 5 ä¸ªæ—¶é—´æ­¥ã€‚
- è¿™æ ·æ¨¡å‹è¾“å…¥ç»Ÿä¸€ï¼Œæ–¹ä¾¿å¹¶è¡Œå¤„ç†ã€‚

![image-20250602230939376](./RNN.assets/image-20250602230939376.png)

- n = 5ï¼›æ¯ä¸ªæ—¶é—´ç‚¹å¯¹åº”ä¸€ä¸ªå­—ç¬¦ï¼Œç©ºæ ¼ä¹Ÿç®—ä½œä¸€ä¸ªå­—ç¬¦ã€‚

    - **è®²è§£**ï¼š

        è¿™é‡Œçš„æ—¶é—´æ­¥ï¼ˆtime pointï¼‰å¯¹åº”äºå­—ç¬¦çº§åˆ«ï¼ˆcharacter-levelï¼‰ã€‚

        è¿™æ„å‘³ç€æ¨¡å‹æ˜¯åœ¨å­—ç¬¦çº§åˆ«è€Œä¸æ˜¯å•è¯çº§åˆ«å»ºæ¨¡ã€‚

- å…³äºåˆå§‹ä½ç½®ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©ä»»æ„åˆå§‹ä½ç½®ã€‚æœ‰ä¸¤ä¸ªç­–ç•¥ã€‚

![image-20250602231449307](./RNN.assets/image-20250602231449307.png)

- æ‰¹æ¬¡å¤§å°ï¼ˆBatch sizeï¼‰ï¼šå‡è®¾æˆ‘ä»¬æœ‰1000æ¡è®­ç»ƒæ ·æœ¬ï¼Œå¦‚æœæ¯æ¬¡åªå–‚ç»™æ¨¡å‹1æ¡æ ·æœ¬ï¼Œå°±æ˜¯â€œå•æ¡ è®­ç»ƒâ€ï¼ˆonline learningï¼‰ã€‚ å¦‚æœæ¯æ¬¡ä¸€æ¬¡æ€§å–‚10æ¡æ ·æœ¬è¿›å»ï¼Œå°±æ˜¯â€œæ‰¹é‡è®­ç»ƒâ€ï¼Œå…¶ä¸­è¿™10æ¡æ ·æœ¬å°±æ„æˆäº†ä¸€ä¸ª**æ‰¹æ¬¡**ï¼ˆbatchï¼‰ã€‚

- æ­¥é•¿ï¼ˆInterval nï¼‰ï¼šåœ¨è¯­è¨€æ¨¡å‹é‡Œï¼Œæ­¥é•¿é€šå¸¸æŒ‡çš„æ˜¯åºåˆ—çš„é•¿åº¦ï¼ˆçª—å£é•¿åº¦ï¼‰ï¼Œå³æ¨¡å‹æ¯æ¬¡å¤„ç†çš„æ—¶é—´æ­¥æ•°ã€‚

    - å¦‚æœæ˜¯å­—ç¬¦çº§æ¨¡å‹ï¼šæ­¥é•¿=5å°±è¡¨ç¤ºæ¨¡å‹æ¯æ¬¡è¯»5ä¸ªå­—ç¬¦ã€‚
    - å¦‚æœæ˜¯å•è¯çº§æ¨¡å‹ï¼šæ­¥é•¿=5å°±è¡¨ç¤ºæ¨¡å‹æ¯æ¬¡è¯»5ä¸ªå•è¯ã€‚

- åœ¨**åºåˆ—å»ºæ¨¡**ä¸­ï¼ˆæ¯”å¦‚æ–‡æœ¬ã€æ—¶é—´åºåˆ—ã€éŸ³é¢‘ç­‰ï¼‰ï¼Œæˆ‘ä»¬é€šå¸¸æŠŠæ–‡æœ¬æŒ‰ç…§æ—¶é—´é¡ºåºå±•å¼€æˆä¸€ä¸²æœ‰åºçš„â€œå•å…ƒæ ¼â€ã€‚æ¯ä¸€ä¸ªå•å…ƒæ ¼ï¼Œå°±æ˜¯ä¸€ä¸ª**æ—¶é—´æ­¥**ã€‚ä¸€ä¸ª**å®Œæ•´çš„å¥å­**ï¼ˆæ¯”å¦‚â€œthe time machine by h g wellsâ€ï¼‰æŒ‰ç…§æ—¶é—´æ­¥å±•å¼€æˆ35ä¸ªå­—ç¬¦ï¼ˆæˆ–å•è¯ï¼‰ã€‚

- è¿™35æ­¥è¢«åˆ†é…ç»™3ä¸ªè¾“å…¥åºåˆ—ï¼ˆx1, x2, x3ï¼‰ã€‚

    



![image-20250602234351947](./RNN.assets/image-20250602234351947.png)

ä¸é¡ºåºåˆ†æ®µï¼ˆSequential Partitioningï¼‰ç›¸æ¯”ï¼Œéšæœºé‡‡æ ·ä¸­çš„æ‰¹æ¬¡é¡ºåºç›¸å¯¹éšæœºï¼Œå¹¶ä¸”ä¸ä¸¥æ ¼éµå¾ªè¾“å…¥åºåˆ—å’Œæ‰¹æ¬¡åºåˆ—çš„é¡ºåºã€‚

ğŸ” è§£é‡Šï¼š

- åœ¨é¡ºåºåˆ†æ®µæ—¶ï¼Œæ‰¹æ¬¡æ˜¯ä¸¥æ ¼æŒ‰ç…§æ—¶é—´æ­¥æ¥åˆ†é…çš„ï¼ˆä¿è¯æ—¶é—´è¿ç»­æ€§ï¼‰ã€‚
- ä½†åœ¨éšæœºé‡‡æ ·æ—¶ï¼Œæ‰¹æ¬¡æ˜¯ä»æ•´ä¸ªæ—¶é—´åºåˆ—ä¸­éšæœºé‡‡æ ·çš„ï¼Œé¡ºåºæ˜¯æ‰“ä¹±çš„ã€‚
- è¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼šå¢åŠ è®­ç»ƒçš„å¤šæ ·æ€§ï¼Œå‡å°‘æ¨¡å‹çš„è¿‡æ‹Ÿåˆã€‚

éšæœºé‡‡æ ·ç¤ºä¾‹ï¼Œéƒ¨åˆ†å­åºåˆ—ç›´æ¥ä»æ—¶é—´æ­¥çš„ä¸åŒä½ç½®éšæœºæŠ½å–ï¼Œå¹¶ä¸ä¸¥æ ¼ä¿æŒæ—¶é—´æ­¥çš„é¡ºåºã€‚

- ä¾‹å¦‚ï¼š
    - x1-2ï¼š29,30,31,32,33
    - x2-1ï¼š4,5,6,7,8
    - x3-2ï¼š10,11,12,13,14
- è¿™æ ·çš„åˆ’åˆ†æ–¹å¼å’Œ Sequential Partitioning ä¸åŒï¼Œä¸è¦æ±‚å¿…é¡»â€œæ—¶é—´è¿ç»­â€ã€‚

![image-20250603000202418](./RNN.assets/image-20250603000202418.png)

1. è¾“å…¥å±‚è¾“å…¥å­—ç¬¦
2. éšè—å±‚æ¥æ”¶è¾“å…¥å¹¶è¿æ¥å‰ä¸€æ—¶é—´æ­¥çš„çŠ¶æ€
3. è¾“å‡ºå±‚è¾“å‡ºä¸€ä¸ªé¢„æµ‹åˆ†å¸ƒ
4. ç”¨Softmaxå½’ä¸€åŒ–å¾—åˆ°æ¦‚ç‡
5. æ¨¡å‹æ ¹æ®æ¦‚ç‡é€‰æ‹©ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼ˆæˆ–è¿›è¡Œé‡‡æ ·ï¼‰

![image-20250603000910830](./RNN.assets/image-20250603000910830.png)

Perplexityï¼ˆå›°æƒ‘åº¦ï¼‰è¶Šä½ï¼Œè¡¨ç¤ºæ¨¡å‹å¯¹ä¸‹ä¸€ä¸ª token é¢„æµ‹å¾—è¶Šå‡†ã€‚

- ç†æƒ³æƒ…å†µï¼šå¦‚æœæ¨¡å‹å®Œç¾é¢„æµ‹äº†æ‰€æœ‰ tokenï¼Œå›°æƒ‘åº¦ = 1ã€‚
- æœ€å·®æƒ…å†µï¼šå¦‚æœæ¨¡å‹é¢„æµ‹æ¯ä¸ª token çš„æ¦‚ç‡éƒ½è¶‹è¿‘äº 0ï¼Œå›°æƒ‘åº¦ = âˆï¼ˆæ— ç©·å¤§ï¼‰ã€‚

==å•è¯ï¼šinfinity æ— ç©·å¤§==
$$
\text{Perplexity} = \exp\Bigg(-\frac{1}{n} \sum_{t=1}^{n} \log P(x_t | x_1, \ldots, x_{t-1}) \Bigg)
$$
è¿™å…¬å¼çš„æ„ä¹‰æ˜¯ï¼š

- å…ˆè®¡ç®—æ‰€æœ‰æ—¶é—´æ­¥çš„å¯¹æ•°ä¼¼ç„¶ï¼ˆlog-likelihoodï¼‰ã€‚
- å†å–å¹³å‡ï¼ˆé™¤ä»¥ nï¼‰ã€‚
- å–è´Ÿæ•°ï¼Œå†å–æŒ‡æ•°ï¼Œå¾—åˆ°å›°æƒ‘åº¦ã€‚



### 2.2 RNNçš„ç»“æ„å’ŒåŸç†

![image-20250603001117088](./RNN.assets/image-20250603001117088.png)



ä»¥ä¸‹æ˜¯PPTä¸Šçš„é‡ç‚¹ï¼š

==nï¼šåŒæ—¶è¾“å…¥çš„æ ·æœ¬æ•°ã€‚$d$ï¼šæ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾ç»´åº¦==ã€‚

Wâ‚•â‚• (green dotted line): It is used to describe ==the relationship between the hidden variable at the current time point and the hidden variable at the previous time point==.

Wâ‚•â‚•ï¼ˆç»¿è‰²è™šçº¿ï¼‰ï¼šç”¨äºæè¿°å½“å‰æ—¶é—´æ­¥çš„éšè—å˜é‡ä¸ä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—å˜é‡ä¹‹é—´çš„å…³ç³»ã€‚

The calculation of the hidden variable can ==capture the historical information== of the sequence up to the current moment. By analogy with biological neural networks, it can have a ==memory function==.

The calculation of ==such a formula is cyclic==. The only differences are the different time points and the actual values of the input and hidden states. Therefore, this kind of neural network with hidden states ==based on cyclic calculation is named the recurrent neural network (RNN)==. The layer that performs calculations in an RNN is called ==the recurrent layer==.

At different time points, the network parameters involved in the RNN model are always Wâ‚“â‚•, Wâ‚•â‚•, bh, Wâ‚•q, bq, and the ==parameters of the network model will not increase== as time goes by.

Wâ‚•â‚•ï¼ˆç»¿è‰²è™šçº¿ï¼‰ï¼šç”¨äºæè¿°å½“å‰æ—¶é—´æ­¥çš„éšè—å˜é‡ä¸ä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—å˜é‡ä¹‹é—´çš„å…³ç³»ã€‚

éšè—å˜é‡çš„è®¡ç®—å¯ä»¥æ•æ‰åˆ°å½“å‰æ—¶åˆ»ä¸ºæ­¢çš„åºåˆ—å†å²ä¿¡æ¯ã€‚ç±»æ¯”äºç”Ÿç‰©ç¥ç»ç½‘ç»œï¼Œå®ƒå¯ä»¥å…·æœ‰è®°å¿†åŠŸèƒ½ã€‚

è¿™ç§å…¬å¼çš„è®¡ç®—æ˜¯å¾ªç¯çš„ã€‚å”¯ä¸€çš„åŒºåˆ«åœ¨äºæ—¶é—´æ­¥çš„ä¸åŒä»¥åŠè¾“å…¥å’Œéšè—çŠ¶æ€çš„å®é™…æ•°å€¼çš„ä¸åŒã€‚å› æ­¤ï¼Œè¿™ç§åŸºäºå¾ªç¯è®¡ç®—çš„éšè—çŠ¶æ€çš„ç¥ç»ç½‘ç»œè¢«ç§°ä¸ºå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ã€‚åœ¨RNNä¸­æ‰§è¡Œè®¡ç®—çš„å±‚å«ä½œå¾ªç¯å±‚ã€‚

åœ¨ä¸åŒçš„æ—¶é—´æ­¥ï¼ŒRNNæ¨¡å‹ä¸­æ¶‰åŠçš„ç½‘ç»œå‚æ•°å§‹ç»ˆæ˜¯ Wâ‚“â‚•ã€Wâ‚•â‚•ã€bhã€Wâ‚•q å’Œ bqï¼Œä¸”è¿™äº›ç½‘ç»œæ¨¡å‹çš„å‚æ•°ä¸ä¼šéšç€æ—¶é—´çš„æ¨ç§»è€Œå¢åŠ ã€‚





## 3. Modern RNN

### 3.1 GRU çš„ æœºåˆ¶![image-20250603003300850](./RNN.assets/image-20250603003300850.png)

![image-20250603004016830](./RNN.assets/image-20250603004016830.png)

When Zâ‚œ is close to 1, the model tends to only retain the hidden state $H_{t-1}$ from the previous time step. Conversely, when $Z_{t}$ is close to 0, the hidden state at the current time step t is close to the candidate hidden state $Ä¤_{t}$.

å½“ $Z_t$ æ¥è¿‘ 1 æ—¶ï¼Œæ¨¡å‹å€¾å‘äºåªä¿ç•™ä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ $H_{t-1}$ã€‚ç›¸åï¼Œå½“ $Z_t$ æ¥è¿‘ 0 æ—¶ï¼Œå½“å‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€æ›´æ¥è¿‘å€™é€‰éšè—çŠ¶æ€ $\widetilde{H}_t$ã€‚

If all $Z_{t}$ values are equal to 1, then regardless of the length of the sequence, the hidden states from the starting time step will be retained and passed on until the end of the sequence.

å¦‚æœæ‰€æœ‰ $Z_t$ å€¼éƒ½ç­‰äº 1ï¼Œé‚£ä¹ˆæ— è®ºåºåˆ—çš„é•¿åº¦å¦‚ä½•ï¼Œèµ·å§‹æ—¶é—´æ­¥çš„éšè—çŠ¶æ€éƒ½å°†è¢«ä¿ç•™å¹¶ä¼ é€’åˆ°æ•´ä¸ªåºåˆ—çš„æœ«å°¾ã€‚

#### é—¨æ§æœºåˆ¶ï¼ˆé‡ç‚¹ï¼‰

![image-20250603005216273](./RNN.assets/image-20250603005216273.png)

è¿™é‡Œè¯´çš„æ˜¯ reset gate çš„ä½œç”¨ï¼šé€šè¿‡â€œé€å…ƒç´ ç›¸ä¹˜â€ï¼ŒæŠŠè¿‡å»éšè—çŠ¶æ€çš„ä¿¡æ¯å‰Šå¼±ï¼Œä»è€Œæ›´å…³æ³¨å½“å‰è¾“å…¥çš„ä¿¡æ¯ã€‚

ä»¥ä¸‹æ˜¯PPTå†…å®¹ï¼š

==When Râ‚œ is close to 1, it becomes an ordinary RNN. If Râ‚œ is close to 0, then the candidate hidden state is only related to the current input Xâ‚œ and has nothing to do with the past hidden states. It can be understood that the past memories are forgotten, discarded, and reset==.

å½“ Râ‚œ æ¥è¿‘ 1 æ—¶ï¼Œå®ƒå°±å˜æˆäº†æ™®é€š RNNã€‚å¦‚æœ Râ‚œ æ¥è¿‘ 0ï¼Œé‚£ä¹ˆå€™é€‰éšè—çŠ¶æ€åªä¸å½“å‰è¾“å…¥ Xâ‚œ æœ‰å…³ï¼Œä¸è¿‡å»çš„éšè—çŠ¶æ€æ— å…³ã€‚å¯ä»¥ç†è§£ä¸ºï¼Œè¿‡å»çš„è®°å¿†è¢«é—å¿˜ã€ä¸¢å¼ƒå¹¶é‡ç½®ã€‚

Both Râ‚œ and Zâ‚œ are vectors in the range of (0, 1) obtained through two fully connected layers
using the sigmoid activation function.

Râ‚œ å’Œ Zâ‚œ éƒ½æ˜¯èŒƒå›´åœ¨ (0, 1) ä¹‹é—´çš„å‘é‡ï¼Œ é€šè¿‡ä¸¤ä¸ªå…¨è¿æ¥å±‚ ä½¿ç”¨ sigmoid æ¿€æ´»å‡½æ•°å¾—åˆ°ã€‚

Their formulas are almost the same, except for different weights and biases.

å®ƒä»¬çš„å…¬å¼å‡ ä¹ç›¸åŒï¼Œåªæ˜¯æƒé‡å’Œåç½®ä¸åŒã€‚

The reset gate helps capture short-term dependencies in the sequence.

The update gate helps capture long-term dependencies in the sequence.

- reset gate ä¸»è¦å½±å“å½“å‰æ—¶é—´æ­¥çš„å€™é€‰éšè—çŠ¶æ€ï¼Œæ›´ä¾§é‡çŸ­æœŸä¿¡æ¯ã€‚
- update gate é€šè¿‡å¹³è¡¡ä¿ç•™å’Œæ›´æ–°ï¼Œå®ç°äº†å¯¹é•¿æœŸä¾èµ–çš„å»ºæ¨¡ã€‚



### 3.2 LSTMçš„æœºåˆ¶

![image-20250603175650187](./RNN.assets/image-20250603175650187.png)
$$
\mathbf{C}_t = \mathbf{F}_t \odot \mathbf{C}_{t-1} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t
$$

- å…¬å¼å«ä¹‰ï¼šLSTM ä¸­çš„è®°å¿†å•å…ƒç”±é—å¿˜é—¨ $\mathbf{F}_t$ å’Œè¾“å…¥é—¨ $\mathbf{I}_t$ æ§åˆ¶ã€‚
    - $\mathbf{F}_t \odot \mathbf{C}_{t-1}$ ï¼šä¿ç•™è¿‡å»çš„è®°å¿†ã€‚
    - $\mathbf{I}_t \odot \tilde{\mathbf{C}}_t$ ï¼šå¼•å…¥æ–°ä¿¡æ¯ã€‚

![image-20250603180050176](./RNN.assets/image-20250603180050176.png)

- ==the output gate determines how much needs to be output to obtain the hidden state==.
     è¾“å‡ºé—¨å†³å®šäº†è¦è¾“å‡ºå¤šå°‘ä¿¡æ¯ä»¥è·å¾—éšè—çŠ¶æ€ã€‚
    - å…¶ä¸­ $o_t$ æ˜¯è¾“å‡ºé—¨çš„å€¼ï¼Œ$C_t$ æ˜¯å½“å‰æ—¶åˆ»çš„å€™é€‰è®°å¿†å•å…ƒã€‚

![image-20250603180619939](./RNN.assets/image-20250603180619939.png)

![image-20250603181108680](./RNN.assets/image-20250603181108680.png)

#### å®Œæ•´æµç¨‹å›¾

![image-20250603181956622](./RNN.assets/image-20250603181956622.png)

![image-20250603182400979](./RNN.assets/image-20250603182400979.png)

**è‹±æ–‡åŸæ–‡ï¼š**
 There are three gating mechanisms in LSTM.
 Just like in GRU, the inputs to the gates include the input at the current time step $t$ and the hidden state at the previous time step $t - 1$.

**ä¸­æ–‡ç¿»è¯‘ï¼š**
LSTMä¸­æœ‰ä¸‰ç§é—¨æ§æœºåˆ¶ã€‚
ä¸GRUç±»ä¼¼ï¼Œé—¨æ§çš„è¾“å…¥åŒ…æ‹¬å½“å‰æ—¶é—´æ­¥ $t$ çš„è¾“å…¥å’Œä¸Šä¸€ä¸ªæ—¶é—´æ­¥ $t - 1$ çš„éšè—çŠ¶æ€ã€‚

**è‹±æ–‡åŸæ–‡ï¼š**
 All three gates are processed by fully connected layers with the sigmoid activation function, and the values of these three gates are all in the range of (0, 1).

**ä¸­æ–‡ç¿»è¯‘ï¼š**
è¿™ä¸‰ä¸ªé—¨éƒ½ç”±å¸¦æœ‰sigmoidæ¿€æ´»å‡½æ•°çš„å…¨è¿æ¥å±‚è¿›è¡Œå¤„ç†ï¼Œå¹¶ä¸”å®ƒä»¬çš„è¾“å‡ºå€¼éƒ½åœ¨ï¼ˆ0, 1ï¼‰èŒƒå›´å†…ã€‚



## 4. Deep and Bi- RNN

### 4.1 ä¸ºä»€ä¹ˆè¦æ·±åº¦RNN

In real life, we are ==faced with many very complex nonlinear relationships==. When the neural networks in ==our brains try to understand these nonlinear relationships==, the neural networks are also deep, and it is impossible to construct these nonlinear relationships relying on a single layer.

![image-20250603184129069](./RNN.assets/image-20250603184129069-1748947411451-1.png)

![image-20250603184335261](./RNN.assets/image-20250603184335261.png)

åœ¨å¤šä¸ªæ—¶é—´æ­¥ä¸­ï¼Œè®¡ç®— $H$ çš„å…¬å¼æ˜¯è¿­ä»£è¿›è¡Œçš„ã€‚è¿™åŒ…æ‹¬æ™®é€šRNNä¸­çš„ $H$ã€GRUä¸­çš„ $H$ã€ä»¥åŠLSTMä¸­çš„ $H$ã€‚å°½ç®¡å®ƒä»¬å„è‡ªçš„è®¡ç®—æ–¹æ³•ä¸åŒï¼Œä½†è¿™äº› $H$ å€¼éƒ½æ˜¯åœ¨æ—¶é—´ç»´åº¦ä¸Šè¿­ä»£è®¡ç®—çš„ã€‚

==å•è¯ï¼šiterate è¿­ä»£==

![image-20250603184514907](./RNN.assets/image-20250603184514907.png)

- ==n== is the number of samples in each small batch;

- the dimensionality of each sample is ==d==.

- The hidden state of layer l at this moment takes into account the hidden state of ==the previous layer at the current moment==, as well as the hidden state of ==the current layer at the previous moment==. 

- It depends on the two ==adjacent preceding hidden states==.
- The output result is only based on the hidden state of the last layer corresponding to that moment.

åœ¨å½“å‰æ—¶åˆ»ï¼Œç¬¬ $l$ å±‚çš„éšè—çŠ¶æ€è€ƒè™‘äº†ï¼š

- å½“å‰æ—¶åˆ»çš„ **ä¸Šä¸€å±‚éšè—çŠ¶æ€**ã€‚
- ä¸Šä¸€æ—¶åˆ»çš„ **å½“å‰å±‚éšè—çŠ¶æ€**ã€‚
- å®ƒä¾èµ–äºä¸¤ä¸ªç›¸é‚»çš„å‰åºéšè—çŠ¶æ€ã€‚

è¾“å‡ºç»“æœä»…ä¾èµ–äºè¯¥æ—¶åˆ»æœ€åä¸€å±‚çš„éšè—çŠ¶æ€ã€‚

- If we use the formula for in the Gated Recurrent Unit (GRU) and the formula for in the Long Short-Term Memory (LSTM) to replace the formula in this diagram, that is, by ==adding the gating mechanism==, we can obtain a deep GRU network and a deep LSTM network.
- In the practical process, the only difference from the shallow one is that **the number of hidden layers** is set through the value of **num_layers**.

![image-20250603190251073](./RNN.assets/image-20250603190251073.png)

`vocab_size`: è¯æ±‡è¡¨çš„å¤§å°ï¼Œé€šå¸¸ç”¨äºè®¾ç½®Embeddingå±‚çš„è¾“å…¥ç»´åº¦ã€‚

- `len(vocab)`ï¼šè¡¨ç¤ºæ•°æ®é›†ä¸­ä¸åŒçš„å•è¯/å­—ç¬¦çš„æ•°é‡ã€‚

`num_hiddens`: æ¯ä¸ªLSTMéšè—å±‚çš„å•å…ƒæ•°ï¼ˆéšè—çŠ¶æ€ç»´åº¦ï¼‰ã€‚

- è¿™é‡Œè®¾ä¸º256ï¼Œè¡¨ç¤ºæ¯ä¸ªéšè—çŠ¶æ€çš„ç»´åº¦æ˜¯256ã€‚

`num_layers`: LSTMçš„å±‚æ•°ï¼ˆæ·±åº¦ï¼‰ã€‚

- è¿™é‡Œè®¾ä¸º2ï¼Œè¡¨ç¤ºæˆ‘ä»¬å°†å †å 2å±‚LSTMã€‚



### 4.2 Bi-RNN

![image-20250603190532277](./RNN.assets/image-20250603190532277.png)

#### 1. ç»“æ„

![image-20250603190850142](./RNN.assets/image-20250603190850142.png)

å¯ä»¥çœ‹åˆ°ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒå‘ç»“æ„ï¼Œç”¨æœªæ¥å’Œè¿‡å»ä¿¡æ¯

The key characteristic of this type of bidirectional RNN is that it uses **information from past and future time steps** to obtain an output result. Therefore, it **cannot be used for prediction tasks**. Without knowing the future values, the bidirectional RNN cannot be employed.

è¿™ç§åŒå‘RNNçš„å…³é”®ç‰¹æ€§åœ¨äºï¼Œå®ƒåˆ©ç”¨**è¿‡å»å’Œæœªæ¥æ—¶é—´æ­¥çš„ä¿¡æ¯**æ¥è·å¾—è¾“å‡ºç»“æœã€‚å› æ­¤ï¼Œå®ƒ**ä¸èƒ½ç”¨äºé¢„æµ‹ä»»åŠ¡**ã€‚å¦‚æœæœªæ¥å€¼æœªçŸ¥ï¼Œå°±æ— æ³•ä½¿ç”¨åŒå‘RNNã€‚



## 5. Encoder and Decoder

### 5.1 Relationship between GRU/LSTM

![image-20250604000147261](./RNN.assets/image-20250604000147261.png)

- Both GRU (Gated Recurrent Unit) and LSTM (Long Short-Term Memory network) are **variants** of the Recurrent Neural Network (RNN), and are often used as **basic units** in the encoder-decoder architecture.
     GRUï¼ˆé—¨æ§å¾ªç¯å•å…ƒï¼‰å’Œ LSTMï¼ˆé•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼‰éƒ½æ˜¯å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰çš„**å˜ä½“**ï¼Œå¹¶ä¸”é€šå¸¸ä½œä¸º**åŸºæœ¬å•å…ƒ**åœ¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ä¸­ä½¿ç”¨ã€‚

- In the encoder-decoder, **the encoder utilizes them to encode the input sequence**, compressing the **sequence information into a fixed-length vector**; the decoder then **utilizes them to decode/generate** the target output sequence based on this vector and the partially generated sequence, with their help.
    åœ¨ç¼–ç å™¨-è§£ç å™¨ä¸­ï¼Œ**ç¼–ç å™¨åˆ©ç”¨å®ƒä»¬æ¥å¯¹è¾“å…¥åºåˆ—è¿›è¡Œç¼–ç **ï¼Œå°†åºåˆ—ä¿¡æ¯å‹ç¼©æˆä¸€ä¸ªå›ºå®šé•¿åº¦çš„å‘é‡ï¼›ç„¶åè§£ç å™¨**åˆ©ç”¨å®ƒä»¬æ¥è§£ç /ç”Ÿæˆ**ç›®æ ‡è¾“å‡ºåºåˆ—ï¼ŒåŸºäºè¯¥å‘é‡å’Œéƒ¨åˆ†å·²ç”Ÿæˆçš„åºåˆ—ï¼Œåœ¨å®ƒä»¬çš„å¸®åŠ©ä¸‹å®Œæˆã€‚

- For example, in a machine translation task, **the encoder processes the source language sentence using GRU or LSTM,** and the **decoder generates** the target language sentence accordingly using **GRU or LSTM**.
    ä¾‹å¦‚ï¼Œåœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œç¼–ç å™¨ä½¿ç”¨ GRU æˆ– LSTM æ¥å¤„ç†æºè¯­è¨€å¥å­ï¼Œç„¶åè§£ç å™¨ä½¿ç”¨ GRU æˆ– LSTM æ¥ç”Ÿæˆç›®æ ‡è¯­è¨€å¥å­ã€‚

### 5.2 function and structure

![image-20250605132604940](./RNN.assets/image-20250605132604940.png)

ã€ç¿»è¯‘ã€‘
 åœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œä»æœ¬è´¨ä¸Šè®²ï¼Œæˆ‘ä»¬æ˜¯åœ¨æ‰§è¡Œä¸€ä¸ªæ“ä½œï¼Œå°†ä¸¤ä¸ªä¸åŒé•¿åº¦çš„åºåˆ—ç›¸äº’è½¬æ¢ã€‚

ã€è®²è§£ã€‘
 è¿™æ˜¯å¯¹æœºå™¨ç¿»è¯‘æ ¸å¿ƒä»»åŠ¡çš„æ¦‚æ‹¬ï¼šæŠŠæºè¯­è¨€çš„å¥å­ï¼ˆå¯èƒ½é•¿åº¦ä¸åŒï¼‰è½¬æ¢æˆç›®æ ‡è¯­è¨€çš„å¥å­ï¼ˆåŒæ ·å¯èƒ½é•¿åº¦ä¸åŒï¼‰ã€‚

### 5.3 ç»“æ„

![image-20250605135640296](./RNN.assets/image-20250605135640296.png)

![image-20250605140506217](./RNN.assets/image-20250605140506217.png)

![image-20250605141900091](./RNN.assets/image-20250605141900091.png)

![image-20250605141906391](./RNN.assets/image-20250605141906391.png)ã€ç¿»è¯‘ã€‘
 å¦‚æœå°†å³è¾¹éƒ¨åˆ†è§†ä¸ºä¸€ä¸ªå®Œæ•´ä¸”ç‹¬ç«‹çš„æ™®é€šå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ï¼Œå…¶å·¥ä½œæ–¹å¼å¦‚ä¸‹ï¼šå°†è¾“å…¥åºåˆ—ä¸­çš„â€œil regardentâ€ä¸¤ä¸ªæ³•è¯­å•è¯ï¼Œä»¥åŠç´§è·Ÿåœ¨åºåˆ—èµ·å§‹ç¬¦â€œBOSâ€ä¹‹åçš„å¥å·ï¼Œä½œä¸ºè¾“å…¥ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥é¢„æµ‹åç»­çš„æ ‡è®°ï¼ˆå•è¯)ã€‚

ã€ç¿»è¯‘ã€‘
 åœ¨ä»è¾“å…¥åˆ°æœ€ç»ˆè¾“å‡ºçš„è¿‡ç¨‹ä¸­ï¼Œå¯ä»¥è§‚å¯Ÿåˆ°ä¸€ç§â€œç§»ä½â€ç°è±¡ï¼Œè¿™ä¸æˆ‘ä»¬ä¹‹å‰åœ¨ä»‹ç»å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æ—¶æåˆ°çš„å­—ç¬¦çº§â€œæœºå™¨â€ç§»ä½åŸç†æ˜¯ä¸€è‡´çš„ã€‚

ã€ç¿»è¯‘ã€‘
 cï¼šç¼–ç å™¨çš„å…¨éƒ¨è¾“å‡ºï¼Œç”¨äºå°†å¯å˜é•¿åº¦çš„è¾“å…¥åºåˆ—è½¬æ¢ä¸ºä¸€ä¸ªå›ºå®šé•¿åº¦çš„å‘é‡cã€‚
 ã€è®²è§£ã€‘
 åœ¨seq2seqæ¨¡å‹é‡Œï¼Œç¼–ç å™¨çš„æ‰€æœ‰éšè—çŠ¶æ€å¯ä»¥åˆå¹¶æˆä¸€ä¸ªå‘é‡cï¼ˆæˆ–è€…ç›´æ¥å–æœ€åä¸€ä¸ªéšè—çŠ¶æ€ï¼‰ç”¨äºè¡¨ç¤ºæ•´ä¸ªè¾“å…¥åºåˆ—ã€‚

ã€ç¿»è¯‘ã€‘
 ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸€ä¸ªåŒå‘RNNç½‘ç»œæ¥æ„å»ºç¼–ç å™¨ã€‚
 ã€è®²è§£ã€‘
 åŒå‘RNNå¯ä»¥æ›´å…¨é¢åœ°æ•æ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼šæ—¢è€ƒè™‘äº†å‰å‘çš„æ—¶é—´ä¾èµ–å…³ç³»ï¼Œä¹Ÿè€ƒè™‘äº†åå‘çš„æ—¶é—´ä¾èµ–å…³ç³»ã€‚

![image-20250605143459463](./RNN.assets/image-20250605143459463.png)

åµŒå…¥å±‚çš„ä½œç”¨æ˜¯è·å–è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªè¯çš„ç‰¹å¾å‘é‡ã€‚è¡Œæ•°ç­‰äºâ€œvocab_sizeâ€è¡¨ç¤ºè¾“å…¥è¯æ±‡è¡¨çš„å¤§å°ï¼Œåœ¨æœ¬ä¾‹ä¸­ä¸º10ã€‚åˆ—æ•°ç­‰äºâ€œembedding_sizeâ€=8ï¼Œè¡¨ç¤ºåµŒå…¥å±‚è¾“å‡ºçš„ç‰¹å¾å‘é‡çš„ç»´åº¦ã€‚åœ¨è¿™æ®µä»£ç ä¸­ï¼ŒRNNé‡‡ç”¨äº†GRUç»“æ„ã€‚æ¯ä¸ªéšè—å±‚çš„è¾“å‡ºç»´åº¦è®¾ç½®ä¸º16ã€‚è¿™æ˜¯ä¸€ä¸ªæ·±åº¦GRUç»“æ„ï¼Œå› ä¸ºå®ƒé…å¤‡äº†ä¸¤ä¸ªéšè—å±‚ã€‚è¾“å‡ºæ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º7 * 4 * 16çš„å¼ é‡ã€‚æ•°å­—7è¡¨ç¤º7ä¸ªæ—¶é—´æ­¥ï¼›4å¯¹åº”äºæ‰¹å¤§å°ï¼Œè¡¨ç¤ºä¸€æ¬¡æ€§åŒæ—¶å¤„ç†4ç»„æ•°æ®ï¼›16æ­£æ˜¯æ¯ä¸ªéšè—å±‚çš„è¾“å‡ºç»´åº¦ï¼Œè¡¨ç¤ºæ¯ä¸ªæ—¶é—´æ­¥éšè—å±‚å¤„ç†å®Œæ¯•åè¾“å‡ºçš„æ¯ç»„æ•°æ®çš„ç‰¹å¾ç»´åº¦ã€‚

![image-20250605151308832](./RNN.assets/image-20250605151308832-1749107616594-1.png)

å¦ä¸€ä¸ªè¾“å‡ºï¼Œstateï¼Œæ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º2 * 4 * 16çš„å¼ é‡ã€‚è¿™é‡Œçš„2è¡¨ç¤ºéšè—å±‚çš„å±‚æ•°ï¼Œä»£è¡¨åœ¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥Tæ—¶ï¼Œæ¯ä¸ªéšè—å±‚çš„çŠ¶æ€ã€‚å®ƒè¡¨ç¤ºåœ¨æ‰¹æ¬¡çš„æœ€åä¸€ä¸ªæ—¶é—´æ­¥ï¼Œæ¯ä¸ªéšè—å±‚ä¸­æ‰€æœ‰16ä¸ªéšè—çŠ¶æ€çš„é›†åˆã€‚ã€

==Question: Which parts of the deep RNN network we discussed in the previous class do output and state correspond to?==

![image-20250605151734068](./RNN.assets/image-20250605151734068.png)

**output** çš„ç»´åº¦è§£é‡Šå¦‚ä¸‹ï¼š

| ç»´åº¦ | ä»£è¡¨å«ä¹‰                    | å›¾ä¸­å¯¹åº”                                           |
| ---- | --------------------------- | -------------------------------------------------- |
| 7    | æ—¶é—´æ­¥æ•°ï¼ˆå¥å­é•¿åº¦ï¼‰        | æ¨ªå‘çš„ $H^{(L)}_1$ åˆ° $H^{(L)}_T$ï¼ˆçº¢æ¡†ï¼‰          |
| 4    | æ‰¹å¤§å°ï¼ˆbatch sizeï¼‰        | ä¸åœ¨å›¾ä¸Šç›´æ¥å±•ç¤ºï¼Œæ˜¯åŒä¸€æ—¶é—´æ­¥ä¸Šä¸åŒæ ·æœ¬çš„å¹¶è¡Œå¤„ç† |
| 16   | éšè—çŠ¶æ€ç»´åº¦ï¼ˆhidden sizeï¼‰ | å•ä¸ªæ–¹æ¡†é‡Œçš„å‘é‡ï¼ˆä¾‹å¦‚ $H^{(L)}_t$ çš„å‘é‡ç»´åº¦ï¼‰    |

**state** çš„ç»´åº¦è§£é‡Šå¦‚ä¸‹ï¼š

| ç»´åº¦ | ä»£è¡¨å«ä¹‰                    | å›¾ä¸­å¯¹åº”                                    |
| ---- | --------------------------- | ------------------------------------------- |
| 2    | éšè—å±‚æ•°ï¼ˆL=2ï¼‰             | ç»¿è‰²æ¡†ä¸­æœ€å³ä¾§çš„ $H^{(L)}_T$ å’Œ $H^{(2)}_T$ |
| 4    | æ‰¹å¤§å°ï¼ˆbatch sizeï¼‰        | åŒæ ·æ˜¯æ‰¹å¤„ç†çš„ç»´åº¦                          |
| 16   | éšè—çŠ¶æ€ç»´åº¦ï¼ˆhidden sizeï¼‰ | å•ä¸ªæ–¹æ¡†é‡Œçš„å‘é‡                            |

![image-20250624180217442](./RNN.assets/image-20250624180217442.png)





### 5.4 Vanishing Gradient

#### 1. å‰é¦ˆç¥ç»ç½‘ç»œ

![image-20250605154044098](./RNN.assets/image-20250605154044098.png)

> ç¬¬ä¸€æ­¥ï¼šå¯¹è¾“å‡ºå‡½æ•° f3 ç›¸å¯¹äº f2 æ±‚åå¯¼æ•°ï¼ˆä¹˜ä»¥ w3ï¼‰
>
> ç¬¬äºŒæ­¥ï¼šå¯¹ f2 ç›¸å¯¹äº f1 æ±‚åå¯¼æ•°ï¼ˆä¹˜ä»¥ w2ï¼‰
>
> ç¬¬ä¸‰æ­¥ï¼šå¯¹ f1 ç›¸å¯¹äº w1 æ±‚åå¯¼æ•°
>
> âœ… è¿™ä¸ªå…¬å¼ä½“ç°äº†ç¥ç»ç½‘ç»œä¸­çš„æ¢¯åº¦åå‘ä¼ æ’­ï¼š
>
> - æ¯å±‚çš„æ¢¯åº¦ä¸ä»…ä¾èµ–äºæœ¬å±‚çš„æƒå€¼å’Œåå¯¼æ•°ï¼Œè¿˜è¦ä¹˜ä¸Šå‰é¢æ‰€æœ‰å±‚çš„æ¢¯åº¦ã€‚
> - è¿™ç§å±‚å±‚ç›¸ä¹˜çš„å½¢å¼å°±æ˜¯Backpropagationçš„æ•°å­¦åŸºç¡€ã€‚

#### 2. æ¢¯åº¦æ¶ˆå¤±

- If all the initialized parameters are decimals between 0 and 1, and the derivatives of the function are also decimals between 0 and 1, then after **successive multiplication**, **the values become smaller and smaller**.
     å¦‚æœæ‰€æœ‰åˆå§‹å‚æ•°éƒ½æ˜¯0åˆ°1ä¹‹é—´çš„å°æ•°ï¼Œä¸”å‡½æ•°çš„åå¯¼æ•°ä¹Ÿæ˜¯0åˆ°1ä¹‹é—´çš„å°æ•°ï¼Œé‚£ä¹ˆç»è¿‡å¤šæ¬¡ç›¸ä¹˜åï¼Œå€¼ä¼šè¶Šæ¥è¶Šå°ã€‚
- After **backpropagation**, the **gradient approaches 0 and vanishes**.
     ç»è¿‡åå‘ä¼ æ’­åï¼Œæ¢¯åº¦è¶‹è¿‘äº0å¹¶æ¶ˆå¤±ã€‚

- The consequence is that **the parameter update is too slow and too small**, and the model cannot normally perform backpropagation and learn to update the network parameters.
     å…¶åæœæ˜¯å‚æ•°çš„æ›´æ–°è¿‡æ…¢ä¸”å¹…åº¦è¿‡å°ï¼Œæ¨¡å‹æ— æ³•æ­£å¸¸è¿›è¡Œåå‘ä¼ æ’­å¹¶æ›´æ–°ç½‘ç»œå‚æ•°ã€‚

> æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæ¨¡å‹çš„å‚æ•°æ›´æ–°å¹…åº¦éå¸¸å°ï¼ˆå‡ ä¹å¯ä»¥å¿½ç•¥ä¸è®¡ï¼‰ï¼Œå¯¼è‡´å­¦ä¹ è¿‡ç¨‹ææ…¢ï¼Œç”šè‡³å®Œå…¨åœæ»ã€‚ç”±äºæ¢¯åº¦æ¶ˆå¤±ï¼Œæ¨¡å‹åœ¨åå‘ä¼ æ’­é˜¶æ®µå‡ ä¹æ²¡æœ‰æœ‰æ•ˆçš„æ¢¯åº¦å¯ä»¥ä½¿ç”¨ï¼Œå› æ­¤æ¨¡å‹çš„å­¦ä¹ ï¼ˆæƒé‡çš„æ›´æ–°ï¼‰åŸºæœ¬â€œå¡ä½â€äº†ï¼Œæ— æ³•ç»§ç»­ä¼˜åŒ–ç½‘ç»œå‚æ•°ã€‚
>
> å‡è®¾æœ‰ä¸€ä¸ªç®€å•çš„ä¸‰å±‚ç½‘ç»œï¼Œæ¯å±‚çš„å¯¼æ•°éƒ½å¤§çº¦æ˜¯0.5ã€‚
>  é‚£ä¹ˆï¼š
>
> - ç»è¿‡3å±‚ä¼ æ’­ï¼š0.5 Ã— 0.5 Ã— 0.5 = 0.125
> - å¦‚æœæ˜¯10å±‚ï¼š0.5Â¹â° â‰ˆ 0.001
> - å¦‚æœæ˜¯50å±‚ï¼š0.5âµâ° â‰ˆ 9e-16ï¼ˆå‡ ä¹ä¸º0ï¼‰
>
> æ­¤æ—¶å†ç”¨æ¢¯åº¦ä¸‹é™æ³•æ›´æ–°æƒå€¼ï¼š
>
> new_weight = old_weight - learning_rate Ã— gradient
>  å¦‚æœ gradient â‰ˆ 0ï¼Œé‚£ä¹ˆæƒå€¼å‡ ä¹ä¸ä¼šå˜åŒ–ï¼Œä¹Ÿå°±æ— æ³•è¿›è¡Œå­¦ä¹ ã€‚



#### 3. æ¢¯åº¦çˆ†ç‚¸

- If all the initialized parameters are very large numbers and the derivatives are also greater than 1, **successive multiplication will make them grow larger and larger**, leading to gradient exploding.
- This will **affect the stable convergence** of the model.

ã€ç¿»è¯‘ã€‘

- å¦‚æœæ‰€æœ‰çš„åˆå§‹å‚æ•°éƒ½æ˜¯å¾ˆå¤§çš„æ•°å€¼ï¼Œå¹¶ä¸”å¯¼æ•°ä¹Ÿå¤§äº1ï¼Œé‚£ä¹ˆç»è¿‡å¤šæ¬¡ç›¸ä¹˜åï¼Œå®ƒä»¬ä¼šè¶Šæ¥è¶Šå¤§ï¼Œä»è€Œå¯¼è‡´æ¢¯åº¦çˆ†ç‚¸ã€‚
- è¿™ä¼šå½±å“æ¨¡å‹çš„ç¨³å®šæ”¶æ•›ã€‚



### 4. 

##### Gradient Clipping

![image-20250605161830825](./RNN.assets/image-20250605161830825.png)

The updated gradient is completely aligned with the original **direction** of g. This imparts a certain degree of stability to the model.
 æ›´æ–°åçš„æ¢¯åº¦ä¸åŸå§‹æ¢¯åº¦çš„æ–¹å‘å®Œå…¨ä¸€è‡´ã€‚è¿™ä¸ºæ¨¡å‹æä¾›äº†ä¸€å®šç¨‹åº¦çš„ç¨³å®šæ€§ã€‚



#### 4. ä¸ºå•¥ä¼šæ¢¯åº¦æ¶ˆå¤±

![image-20250605164735525](./RNN.assets/image-20250605164735525.png)

- $h_t$ æ˜¯å½“å‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼Œç”±å½“å‰è¾“å…¥ $x_t$ã€å‰ä¸€æ—¶åˆ»éšè—çŠ¶æ€ $h_{t-1}$ å’Œæƒé‡ $w_h$ è®¡ç®—è€Œå¾—ã€‚
- $o_t$ æ˜¯å½“å‰æ—¶é—´æ­¥çš„è¾“å‡ºï¼Œç”± $h_t$ å’Œæƒé‡ $w_o$ è®¡ç®—è€Œå¾—ã€‚

$$
L(x_1, \ldots, x_T, y_1, \ldots, y_T, w_h, w_o) = \frac{1}{T} \sum_{t=1}^{T} l(y_t, o_t)
$$

æŸå¤±å‡½æ•°ï¼š

- è¡¨ç¤ºä»æ—¶é—´æ­¥ $t=1$ åˆ°æ•´ä¸ªåºåˆ—é•¿åº¦ $T$ï¼Œæ¨¡å‹çš„è¾“å‡º $o_t$ ä¸å¯¹åº”çš„æ ‡ç­¾ $y_t$ çš„å·®å¼‚ã€‚
- è¿™é‡Œ $l(y_t, o_t)$ æ˜¯å•æ­¥æŸå¤±ï¼ˆæ¯”å¦‚MSEæˆ–è€…äº¤å‰ç†µï¼‰ã€‚

è¿›ä¸€æ­¥å±•å¼€ï¼ˆé“¾å¼æ³•åˆ™ï¼‰ï¼š
$$
= \frac{1}{T} \sum_{t=1}^{T} \frac{\partial l(y_t, o_t)}{\partial o_t} \cdot \frac{\partial g(h_t, w_o)}{\partial h_t} \cdot \frac{\partial h_t}{\partial w_h}
$$

- è“è‰²åœˆéƒ¨åˆ†åˆ†åˆ«è¡¨ç¤ºï¼š
    - $\frac{\partial l(y_t, o_t)}{\partial o_t}$ï¼šæŸå¤±å¯¹è¾“å‡ºçš„æ¢¯åº¦
    - $\frac{\partial g(h_t, w_o)}{\partial h_t}$ï¼šè¾“å‡ºå¯¹éšè—çŠ¶æ€çš„æ¢¯åº¦

- å¯¹äºRNNæ¨¡å‹ï¼Œè¿›è¡Œåå‘ä¼ æ’­æ—¶ï¼Œéœ€è¦æ ¹æ®é“¾å¼æ³•åˆ™åˆ†è§£æŸå¤±å‡½æ•°å¯¹$w_h$çš„åå¯¼æ•°ã€‚
- å…¶ä¸­è“è‰²åœˆå‡ºçš„éƒ¨åˆ†ï¼ˆlossåˆ°$o_t$ï¼Œ$o_t$åˆ°$h_t$ï¼‰æ¯”è¾ƒå®¹æ˜“ç›´æ¥è®¡ç®—ã€‚

==ä¸ºä»€ä¹ˆåœ¨è¿™é‡Œå®¹æ˜“å‡ºç°æ¢¯åº¦æ¶ˆå¤±==ï¼Ÿ

- å› ä¸ºRNNçš„éšè—çŠ¶æ€ $h_t$ ä¾èµ–äº $h_{t-1}$ï¼Œ

- æ‰€ä»¥ $\frac{\partial h_t}{\partial w_h}$ éœ€è¦ç»§ç»­å±•å¼€ï¼š
    $$
    \frac{\partial h_t}{\partial w_h} = \frac{\partial h_t}{\partial h_{t-1}} \cdot \frac{\partial h_{t-1}}{\partial w_h}
    $$
    è¿™é‡Œå¦‚æœ $\frac{\partial h_t}{\partial h_{t-1}}$ æ˜¯å°äº1çš„å€¼ï¼Œç»è¿‡å¤šå±‚ç›¸ä¹˜å°±ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ã€‚

- å½“åºåˆ—è¾ƒé•¿æ—¶ï¼Œé“¾å¼ä¹˜ç§¯çš„é¡¹æ•°å°±å¤šï¼Œå°±ä¼šæ›´ä¸¥é‡ã€‚



#### 5. Time Truncation

![image-20250605170737758](./RNN.assets/image-20250605170737758.png)

> Through the simplified model of RNN, we can see that $h_t$ depends on both $h_{t-1}$ and $W_h$, where the calculation of $h_{t-1}$ also relies on $W_h$.
>  é€šè¿‡ç®€åŒ–çš„RNNæ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° $h_t$ åŒæ—¶ä¾èµ–äº $h_{t-1}$ å’Œå‚æ•° $W_h$ï¼Œå¹¶ä¸” $h_{t-1}$ ä¹Ÿä¾èµ–äº $W_h$ã€‚

ã€è®²è§£ã€‘
 è¿™å¥è¯å¼ºè°ƒäº†RNNçš„â€œé€’å½’æ€§â€ï¼š

- $h_t$ ä¸ä»…ç›´æ¥ä¾èµ– $W_h$ï¼Œä¹Ÿé€šè¿‡ $h_{t-1}$ é—´æ¥ä¾èµ– $W_h$ã€‚
- è¿™å°±æ˜¯ä¸ºä»€ä¹ˆé“¾å¼æ³•åˆ™è¦å±‚å±‚å±•å¼€ï¼Œå¯¼è‡´äº†æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸ã€‚

> If we fully calculate the chains corresponding to all t, it is imaginable that the computation will be very slow, and gradient explosion may occur, leading to low model stability.

ã€ç¿»è¯‘ã€‘
 å¦‚æœæˆ‘ä»¬å®Œæ•´è®¡ç®—ä»æ‰€æœ‰æ—¶é—´æ­¥ t çš„é“¾å¼å±•å¼€ï¼Œæƒ³è±¡ä¸€ä¸‹ï¼Œè¿™å°†å¯¼è‡´éå¸¸æ…¢çš„è®¡ç®—é€Ÿåº¦ï¼Œè€Œä¸”å¯èƒ½å‘ç”Ÿæ¢¯åº¦çˆ†ç‚¸ï¼Œå¯¼è‡´æ¨¡å‹ç¨³å®šæ€§ä½ã€‚

> The summation calculation can be truncated at the time point tau, where the summation continues until to approximate the true gradient.

ã€ç¿»è¯‘ã€‘
 æ±‚å’Œè®¡ç®—å¯ä»¥åœ¨æ—¶é—´æ­¥Ï„å¤„æˆªæ–­ï¼Œç„¶åä»t=Ï„åˆ°t=Tç»§ç»­æ±‚å’Œï¼Œä»¥è¿‘ä¼¼çœŸå®çš„æ¢¯åº¦ã€‚

åœ¨å…¬å¼é‡Œï¼Œæœ€åä¸€é¡µæŒ‡å‡ºï¼š
$$
\frac{\partial h_t}{\partial w_h} \rightarrow \frac{\partial h_{t-\tau}}{\partial w_h}
$$

- è¿™æ„å‘³ç€æˆ‘ä»¬åªåå‘ä¼ æ’­æœ€è¿‘çš„ Ï„ ä¸ªæ—¶é—´æ­¥ï¼ˆæ¯”å¦‚ Ï„=5ï¼‰ã€‚
- è¿™æ ·é“¾å¼æ³•åˆ™å°±ä¸éœ€è¦å±•å¼€åˆ° t=1ï¼Œåªéœ€è¦å±•å¼€åˆ° $t-\tau$ã€‚

