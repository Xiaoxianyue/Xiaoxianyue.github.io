---
title: RNNæ¨¡å‹è®­ç»ƒ
icon: alias
date: 2025-4-14 20:49:21
author: XiaoXianYue
isOriginal: true
category: 
    - å¤§ä¸‰ä¸‹
    - ç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ 
tag:
    - å¤§ä¸‰ä¸‹
    - ç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ 
sticky: false
star: false
article: true
timeline: true
image: false
navbar: true
sidebarIcon: true
headerDepth: 5
lastUpdated: true
editLink: false
backToTop: true
toc: tru
---



### åˆå§‹å‚æ•°



```python
import os
import math
import random
import urllib.request
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

# =========================
# è‡ªåŠ¨ä¸‹è½½æ•°æ®
# =========================
file_path = "timemachine.txt"
url = "http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt"
if not os.path.exists(file_path):
    print("Downloading dataset...")
    urllib.request.urlretrieve(url, file_path)

# =========================
# æ•°æ®é¢„å¤„ç†
# =========================
with open(file_path, 'r', encoding='utf-8') as f:
    text = f.read().lower()
text = ''.join([line.strip() for line in text.split('\n') if line.strip()])

# æ„å»ºè¯è¡¨
vocab = sorted(list(set(text)))
vocab_size = len(vocab)
char2idx = {ch: i for i, ch in enumerate(vocab)}
idx2char = {i: ch for i, ch in enumerate(vocab)}
corpus = [char2idx[ch] for ch in text]


# =========================
# è·å– mini-batch æ•°æ®
# =========================
def get_batch(corpus, batch_size, num_steps):
    start = random.randint(0, len(corpus) - batch_size * num_steps - 1)
    inputs, targets = [], []
    for i in range(batch_size):
        idx = start + i * num_steps
        inputs.append(corpus[idx:idx + num_steps])
        targets.append(corpus[idx + 1:idx + num_steps + 1])
    return torch.tensor(inputs), torch.tensor(targets)


# =========================
# RNN æ¨¡å‹å®šä¹‰
# =========================
class RNNModel(nn.Module):
    def __init__(self, vocab_size, hidden_size):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, vocab_size)  # one-hot
        self.rnn = nn.RNN(vocab_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, x, state):
        x = self.embedding(x)
        out, state = self.rnn(x, state)
        out = self.fc(out)
        return out, state

    def init_state(self, batch_size):
        return torch.zeros(1, batch_size, hidden_size)


# =========================
# æ¨¡å‹è®­ç»ƒå‡½æ•°
# =========================
def train(model, corpus, num_epochs, batch_size, num_steps, lr):
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    loss_fn = nn.CrossEntropyLoss()
    perplexities = []

    for epoch in range(1, num_epochs + 1):
        state = model.init_state(batch_size)
        X, Y = get_batch(corpus, batch_size, num_steps)
        Y = Y.reshape(-1)
        logits, state = model(X, state)
        logits = logits.reshape(-1, vocab_size)
        loss = loss_fn(logits, Y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        ppl = math.exp(loss.item())
        perplexities.append(ppl)
        print(f"Epoch {epoch}, Loss {loss.item():.4f}, Perplexity {ppl:.2f}")

    return perplexities


# =========================
# å‚æ•°è®¾ç½® & å¯åŠ¨è®­ç»ƒ
# =========================
hidden_size = 128
batch_size = 32
num_steps = 35
lr = 1e-2
num_epochs = 20

model = RNNModel(vocab_size, hidden_size)
perplexity_list = train(model, corpus, num_epochs, batch_size, num_steps, lr)

# =========================
# ç»˜åˆ¶ Perplexity æŠ˜çº¿å›¾
# =========================
plt.figure(figsize=(8, 5))
plt.plot(perplexity_list, marker='o')
plt.title("Perplexity over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Perplexity")
plt.grid(True)
plt.tight_layout()
plt.savefig("perplexity_plot.png")
plt.show()
```

<img src="./rnn.assets/image-20250414205126986.png" alt="image-20250414205126986" style="zoom:33%;" />



### æ”¹å˜éšè—å±‚ç»´åº¦

æé«˜è‡³ 200ï¼š

<img src="./rnn.assets/image-20250414205210467.png" alt="image-20250414205210467" style="zoom:33%;" />

æé«˜è‡³ 300ï¼š

<img src="./rnn.assets/image-20250414205441825.png" alt="image-20250414205441825" style="zoom:33%;" />

| ç»´åº¦      | å½±å“                                                 |
| --------- | ---------------------------------------------------- |
| å°ï¼ˆ128ï¼‰ | è®°å¿†åŠ›ä¸­ç­‰ï¼Œå­¦ä¹ ç¨³å®šï¼Œä½†ä¸å¤Ÿå¼º                       |
| ä¸­ï¼ˆ200ï¼‰ | è®°å¿†åŠ›åŠ å¼ºï¼Œæ•æ‰ä¾èµ–èƒ½åŠ›æå‡ï¼Œè¡¨ç°æ›´å¥½ âœ…             |
| å¤§ï¼ˆ300ï¼‰ | å‚æ•°æ•°ç›®æ›´å¤šï¼Œè®­ç»ƒéš¾åº¦åŠ å¤§ï¼Œ**å¯èƒ½ä¸å®¹æ˜“æ”¶æ•›**æˆ–éœ‡è¡ |

æˆ‘ä»¬é€‰æ‹©æœ€ä¼˜å‚æ•°ï¼Œè¿›ä¸€æ­¥è°ƒå…¶ä»–å‚æ•°çœ‹çœ‹èƒ½ä¸èƒ½è¶Šæ¥è¶Šå¥½ã€‚



### è°ƒæ•´æ‰¹é‡å¤§å°:

32 è°ƒæ•´ä¸º 16ï¼š

<img src="./rnn.assets/image-20250414210453017.png" alt="image-20250414210453017" style="zoom:33%;" />

32 è°ƒæ•´ä¸º 64ï¼š

<img src="./rnn.assets/image-20250414210536470.png" alt="image-20250414210536470" style="zoom:33%;" />

| Batch Size | æ”¶æ•›é€Ÿåº¦ | æ³¢åŠ¨æ€§   | æœ€ç»ˆ perplexity | ç‰¹ç‚¹                   |
| ---------- | -------- | -------- | --------------- | ---------------------- |
| 16         | å¾ˆå¿«     | æ˜æ˜¾æ³¢åŠ¨ | 10 å·¦å³         | æ¢¯åº¦å™ªå£°å¤§ï¼Œæ›´éšæœº     |
| 32         | ä¸­ç­‰     | åç¨³å®š   | æœ€ä½³å¹³è¡¡ç‚¹ âœ…    | å®ç”¨æ€§å¼º               |
| 64         | ç¨³å®šä½†æ…¢ | æå¹³ç¨³   | ä¸32ç›¸è¿‘        | å ç”¨å†…å­˜é«˜ï¼Œé€‚åˆå¤§æ¨¡å‹ |

æ‰€ä»¥æ‰¹é‡å¤§å°ä¸åœ¨åŸæœ‰åŸºç¡€ä¸Šä¿®æ”¹ã€‚









### ğŸ§® ç»™å®šæ¡ä»¶ï¼š

- è¾“å…¥åºåˆ—é•¿åº¦ï¼š**5**ï¼ˆä¸å½±å“å‚æ•°é‡ï¼Œåªå½±å“è¿ç®—æ­¥æ•°ï¼‰
- è¾“å…¥ç»´åº¦ $d = 1$
- éšè—å±‚æ•°ï¼š**2 å±‚**
- æ¯å±‚éšè—å•å…ƒæ•°ï¼š**3 ä¸ªç¥ç»å…ƒ**
- è¾“å‡ºç»´åº¦ï¼š**1**ï¼ˆæ¯ä¸ªæ—¶é—´æ­¥è¾“å‡º 1 ç»´ï¼‰
- ä½¿ç”¨æ ‡å‡† RNN å•å…ƒï¼ˆå‡è®¾æ˜¯ vanilla RNNï¼‰

------

## âœ… åˆ†å±‚è®¡ç®—å‚æ•°é‡ï¼š

### ğŸ”¹ ç¬¬ä¸€å±‚ RNNï¼ˆLayer 1ï¼‰ï¼š

è¾“å…¥ç»´åº¦ï¼š1
 éšè—ç»´åº¦ï¼š3

RNN çš„å‚æ•°åŒ…æ‹¬ï¼š

- $W_{xh}^{(1)} \in \mathbb{R}^{1 \times 3}$ï¼šè¾“å…¥åˆ°éšè—
- $W_{hh}^{(1)} \in \mathbb{R}^{3 \times 3}$ï¼šéšè—åˆ°éšè—
- $b_h^{(1)} \in \mathbb{R}^{3}$ï¼šåç½®é¡¹

åˆè®¡ï¼š
$$
1 \times 3 + 3 \times 3 + 3 = 3 + 9 + 3 = \boxed{15}
$$

------

### ğŸ”¹ ç¬¬äºŒå±‚ RNNï¼ˆLayer 2ï¼‰ï¼š

è¾“å…¥ç»´åº¦ï¼š**ä¸Šå±‚éšè—è¾“å‡ºæ˜¯ 3**
 éšè—ç»´åº¦ï¼š3

- $W_{xh}^{(2)} \in \mathbb{R}^{3 \times 3}$
- $W_{hh}^{(2)} \in \mathbb{R}^{3 \times 3}$
- $b_h^{(2)} \in \mathbb{R}^{3}$

åˆè®¡ï¼š
$$
3 \times 3 + 3 \times 3 + 3 = 9 + 9 + 3 = \boxed{21}
$$

------

### ğŸ”¹ è¾“å‡ºå±‚ï¼ˆå¯¹æ¯ä¸ªæ—¶é—´æ­¥ï¼‰ï¼š

- è¾“å…¥æ˜¯ç¬¬ 2 å±‚çš„éšè—çŠ¶æ€ï¼š3 ç»´
- è¾“å‡ºæ˜¯ 1 ç»´
- æ‰€ä»¥ï¼š
    - $W_{out} \in \mathbb{R}^{3 \times 1}$
    - $b_{out} \in \mathbb{R}^{1}$

åˆè®¡ï¼š
$$
3 \times 1 + 1 = \boxed{4}
$$

------

## âœ… æ€»å‚æ•°é‡ï¼š

ç¬¬ä¸€å±‚ + ç¬¬äºŒå±‚ + è¾“å‡ºå±‚
$$
15 + 21 + 4 = \boxed{40}
$$

------

âœ… **æœ€ç»ˆæ€»å‚æ•°é‡æ˜¯ï¼š** **40**

å¦‚ä½ ä½¿ç”¨çš„æ˜¯ LSTM æˆ– GRUï¼Œå‚æ•°ä¼šæ›´å¤šï¼Œéœ€è¦åˆ†åˆ«è®¡ç®—é—¨æ§ç»“æ„ã€‚å¦‚æœä½ éœ€è¦è®¡ç®— GRU/LSTM çš„å‚æ•°ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ åˆ—å‡ºæ¥ï¼