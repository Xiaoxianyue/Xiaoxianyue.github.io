import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as e,b as i,o as n}from"./app-84lBMjzT.js";const o="/assets/image-20250622191618177-CIr8XmE8.png",s="/assets/image-20250622191642744-D8YSbenN.png",r="/assets/image-20250622191656878-agZWGsqC.png",l="/assets/image-20250622123746772-BaYrJThN.png",g="/assets/image-20250622123815494-cAQCfuh1.png",p="/assets/image-20250622123823003-CioQ7dgr.png",c="/assets/image-20250622123829053-DKxv7BIB.png",d="/assets/image-20250622123834145-B0LWXIFr.png",m="/assets/image-20250622123916986-DNvZKn_E.png",u="/assets/image-20250622123936839-CjNCvp0C.png",h="/assets/image-20250622123945600-BoR0pmXx.png",f="/assets/image-20250622124228480-BOeE2brB.png",b="/assets/image-20250622125908783-DggOWl-J.png",y="/assets/image-20250622130301542-DWPDGvsO.png",q="/assets/image-20250622130325680-5aolrZeB.png",k="/assets/image-20250622130641174-DdgfHoci.png",x="/assets/image-20250622133119741-CUwwcnAS.png",_="/assets/image-20250624121312718-BwRNJgvf.png",v="/assets/image-20250623011126090-iEoF_D8l.png",w="/assets/image-20250622164217188-DFHan5dv.png",z="/assets/image-20250622172555655-Df8wiPqx.png",T="/assets/image-20250622172723224-DkuBCf-j.png",C={};function P(N,a){return n(),e("div",null,a[0]||(a[0]=[i('<h2 id="introduction" tabindex="-1"><a class="header-anchor" href="#introduction"><span>Introduction</span></a></h2><h3 id="q1" tabindex="-1"><a class="header-anchor" href="#q1"><span>Q1</span></a></h3><blockquote><p>The developmental history of natural language processing technologies ï¼ˆFocus on mastering the three major tasks of natural language processing, as well as the earliest natural language processing task.<strong>ï¼‰</strong></p></blockquote><ul><li>Rule-based (symbolic) approach</li><li>Statistical approach (traditional machine learning)</li><li>Connectionist approach</li><li>Pre-Training</li></ul><p>major tasks:</p><ul><li>Computational Linguistic Tasks</li><li>Information Extraction Tasks</li><li>Natural Language Generation</li></ul><p>the earliest natural language processing taskï¼š</p><ul><li>Machine Translation</li></ul><h3 id="q2" tabindex="-1"><a class="header-anchor" href="#q2"><span>Q2</span></a></h3><blockquote><p>Various levels of linguistic analysis</p></blockquote><table><thead><tr><th>å±‚çº§</th><th>å†…å®¹</th></tr></thead><tbody><tr><td><strong>éŸ³ç³»å­¦ (Phonology)</strong></td><td>å£°éŸ³ç³»ç»Ÿçš„ç»“æ„åˆ†æ</td></tr><tr><td><strong>å½¢æ€å­¦ (Morphology)</strong></td><td>è¯çš„å†…éƒ¨ç»“æ„ï¼Œå¦‚è¯ç¼€ã€è¯æ ¹ç­‰</td></tr><tr><td><strong>å¥æ³•å­¦ (Syntax)</strong></td><td>åˆ†æè¯çš„ç»“æ„ä¸å¥æ³•è§„åˆ™ï¼ŒåŒ…å«ï¼š</td></tr></tbody></table><p>åˆ†è¯ï¼ˆTokenizationï¼‰</p><p>è¯æ€§æ ‡æ³¨ï¼ˆPOS Taggingï¼‰</p><p>æˆåˆ†å¥æ³•åˆ†æï¼ˆConstituent Parsingï¼‰</p><p>ä¾å­˜å¥æ³•åˆ†æï¼ˆDependency Parsingï¼‰</p><ul><li><p><strong>è¯­ä¹‰å­¦ (Semantics)</strong> ç†è§£è¯ä¸å¥å­çš„æ„ä¹‰ï¼Œå¦‚ï¼š</p><ul><li><p>è¯­ä¹‰è§’è‰²æ ‡æ³¨</p></li><li><p>æŒ‡ä»£æ¶ˆè§£</p></li><li><p>æ–‡æœ¬è•´å«ï¼ˆTextual Entailmentï¼‰</p></li></ul></li><li><p><strong>è¯­ç”¨å­¦ (Pragmatics)</strong> ä¸Šä¸‹æ–‡å’Œæ„å›¾ç†è§£<br><strong>ç¯‡ç« åˆ†æ (Discourse)</strong> å¤šå¥ä¹‹é—´çš„ç»“æ„å…³ç³»ï¼Œå¦‚ï¼š</p><ul><li><p>ç¯‡ç« åˆ†æ®µ</p></li><li><p>ç¯‡ç« è¿è´¯æ€§åˆ†æï¼ˆRhetorical Structure Theoryï¼‰ |</p></li></ul></li></ul><h3 id="q3" tabindex="-1"><a class="header-anchor" href="#q3"><span>Q3</span></a></h3><blockquote><p>Current research status and related applications of natural language processing</p></blockquote><p>è‡ªç„¶è¯­è¨€å¤„ç†çš„ç ”ç©¶ç°çŠ¶åŠç›¸å…³åº”ç”¨</p><figure><img src="'+o+'" alt="image-20250622191618177" tabindex="0" loading="lazy"><figcaption>image-20250622191618177</figcaption></figure><figure><img src="'+s+'" alt="image-20250622191642744" tabindex="0" loading="lazy"><figcaption>image-20250622191642744</figcaption></figure><figure><img src="'+r+'" alt="image-20250622191656878" tabindex="0" loading="lazy"><figcaption>image-20250622191656878</figcaption></figure><h2 id="_2-formal-languages-and-automata" tabindex="-1"><a class="header-anchor" href="#_2-formal-languages-and-automata"><span>2. Formal Languages and Automata</span></a></h2><p>Key examination content, focusing on formal grammars and automata. Key focus on relevant calculations and derivations.</p><p>è€ƒè¯•å†…å®¹é‡ç‚¹ï¼šå½¢å¼è¯­æ³•å’Œè‡ªåŠ¨æœºã€‚é‡ç‚¹å…³æ³¨ç›¸å…³è®¡ç®—å’Œæ¨å¯¼ã€‚</p><h3 id="é¢˜ç›®" tabindex="-1"><a class="header-anchor" href="#é¢˜ç›®"><span>é¢˜ç›®</span></a></h3><h4 id="q1-1" tabindex="-1"><a class="header-anchor" href="#q1-1"><span>Q1</span></a></h4><blockquote><p>Definition and types of formal grammars (key examination content)</p></blockquote><figure><img src="'+l+'" alt="image-20250622123746772" tabindex="0" loading="lazy"><figcaption>image-20250622123746772</figcaption></figure><figure><img src="'+g+'" alt="image-20250622123815494" tabindex="0" loading="lazy"><figcaption>image-20250622123815494</figcaption></figure><figure><img src="'+p+'" alt="image-20250622123823003" tabindex="0" loading="lazy"><figcaption>image-20250622123823003</figcaption></figure><figure><img src="'+c+'" alt="image-20250622123829053" tabindex="0" loading="lazy"><figcaption>image-20250622123829053</figcaption></figure><figure><img src="'+d+'" alt="image-20250622123834145" tabindex="0" loading="lazy"><figcaption>image-20250622123834145</figcaption></figure><h4 id="q2-1" tabindex="-1"><a class="header-anchor" href="#q2-1"><span>Q2</span></a></h4><blockquote><p>Relationship between regular grammars and automata (key examination content)</p></blockquote><figure><img src="'+m+'" alt="image-20250622123916986" tabindex="0" loading="lazy"><figcaption>image-20250622123916986</figcaption></figure><figure><img src="'+u+'" alt="image-20250622123936839" tabindex="0" loading="lazy"><figcaption>image-20250622123936839</figcaption></figure><figure><img src="'+h+'" alt="image-20250622123945600" tabindex="0" loading="lazy"><figcaption>image-20250622123945600</figcaption></figure><figure><img src="'+f+'" alt="image-20250622124228480" tabindex="0" loading="lazy"><figcaption>image-20250622124228480</figcaption></figure><h3 id="é‡ç‚¹" tabindex="-1"><a class="header-anchor" href="#é‡ç‚¹"><span>é‡ç‚¹</span></a></h3><ul><li><p>æœ€å·¦æ¨å¯¼æœ€å³æ¨å¯¼</p></li><li><p>CFGçš„å®šä¹‰ï¼ˆä¸Šé¢æœ‰ï¼‰</p></li><li><p>CSGçš„å®šä¹‰</p></li><li><p>Ambiguity of Context-Free Grammars A grammar G is said to be ambiguous if there exists at least one sentence that corresponds to more than one parse tree. Draw the derivation tree of phrases and sentences.</p><ul><li>ä¸Šä¸‹æ–‡æ— å…³æ–‡æ³•çš„æ­§ä¹‰æ€§ï¼šå¦‚æœæ–‡æ³•Gä¸­è‡³å°‘æœ‰ä¸€ä¸ªå¥å­å¯¹åº”å¤šæ£µè§£ææ ‘ï¼Œåˆ™ç§°è¯¥æ–‡æ³•ä¸ºæ­§ä¹‰æ–‡æ³•ã€‚è¯·ç”»å‡ºçŸ­è¯­å’Œå¥å­çš„æ´¾ç”Ÿæ ‘ã€‚</li></ul></li><li><p>æ­£åˆ™æ–‡æ³•æ„å»ºè‡ªåŠ¨æœº</p></li></ul><h2 id="language-model" tabindex="-1"><a class="header-anchor" href="#language-model"><span>Language model</span></a></h2><h3 id="q1-2" tabindex="-1"><a class="header-anchor" href="#q1-2"><span>Q1</span></a></h3><blockquote><p>N-gram models (key examination contentï¼ŒKey focus on probability calculations utilizing 2-gram and 3-gram modelsï¼‰</p></blockquote><p>N-gram æ¨¡å‹ï¼ˆé‡ç‚¹è€ƒè¯•å†…å®¹ï¼Œé‡ç‚¹å…³æ³¨åˆ©ç”¨ 2-gram å’Œ 3-gram æ¨¡å‹è¿›è¡Œçš„æ¦‚ç‡è®¡ç®—ï¼‰</p><figure><img src="'+b+'" alt="image-20250622125908783" tabindex="0" loading="lazy"><figcaption>image-20250622125908783</figcaption></figure><h3 id="q2-2" tabindex="-1"><a class="header-anchor" href="#q2-2"><span>Q2</span></a></h3><blockquote><p>Performance evaluation of language models (key examination content, conceptual memorization</p></blockquote><p>è¯­è¨€æ¨¡å‹ç»©æ•ˆè¯„ä¼°ï¼ˆé‡ç‚¹è€ƒè¯•å†…å®¹ã€æ¦‚å¿µè®°å¿†ï¼‰</p><figure><img src="'+y+'" alt="image-20250622130301542" tabindex="0" loading="lazy"><figcaption>image-20250622130301542</figcaption></figure><figure><img src="'+q+'" alt="image-20250622130325680" tabindex="0" loading="lazy"><figcaption>image-20250622130325680</figcaption></figure><h3 id="q3-1" tabindex="-1"><a class="header-anchor" href="#q3-1"><span>Q3</span></a></h3><blockquote><p>Data smoothing methods (focusing on major data smoothing calculations)</p></blockquote><figure><img src="'+k+'" alt="image-20250622130641174" tabindex="0" loading="lazy"><figcaption>image-20250622130641174</figcaption></figure><h3 id="q4" tabindex="-1"><a class="header-anchor" href="#q4"><span>Q4</span></a></h3><blockquote><p>Language model adaptation methods (for classroom explanation only)</p></blockquote><p>è¯­è¨€æ¨¡å‹è‡ªé€‚åº”æ–¹æ³•ï¼ˆä»…ä¾›è¯¾å ‚è®²è§£ï¼‰</p><h2 id="éšé©¬å°”ç§‘å¤«å’Œæ¡ä»¶éšæœºåœº" tabindex="-1"><a class="header-anchor" href="#éšé©¬å°”ç§‘å¤«å’Œæ¡ä»¶éšæœºåœº"><span>éšé©¬å°”ç§‘å¤«å’Œæ¡ä»¶éšæœºåœº</span></a></h2><h3 id="é¢˜ç›®-1" tabindex="-1"><a class="header-anchor" href="#é¢˜ç›®-1"><span>é¢˜ç›®</span></a></h3><h4 id="q1-3" tabindex="-1"><a class="header-anchor" href="#q1-3"><span>Q1</span></a></h4><blockquote><p>Basic methods of probabilistic graphical models (including Hidden Markov Models)</p></blockquote><p>æ¦‚ç‡å›¾æ¨¡å‹ï¼ˆåŒ…æ‹¬éšé©¬å°”å¯å¤«æ¨¡å‹ï¼‰çš„åŸºæœ¬æ–¹æ³•</p><figure><img src="'+x+'" alt="image-20250622133119741" tabindex="0" loading="lazy"><figcaption>image-20250622133119741</figcaption></figure><h4 id="q2-3" tabindex="-1"><a class="header-anchor" href="#q2-3"><span>Q2</span></a></h4><blockquote><p>Conditional random field models (for theoretical comprehension only</p></blockquote><figure><img src="'+_+'" alt="image-20250624121312718" tabindex="0" loading="lazy"><figcaption>image-20250624121312718</figcaption></figure><table><thead><tr><th>é¡¹ç›®</th><th>HMM</th><th>CRF</th></tr></thead><tbody><tr><td>æ¨¡å‹å…³æ³¨</td><td>æ¨¡æ‹Ÿçƒçš„ç”Ÿæˆè¿‡ç¨‹ï¼ˆçŠ¶æ€è½¬ç§» + ç”Ÿæˆè¾“å‡ºï¼‰</td><td>ç»™å®šçƒçš„é¢œè‰²åºåˆ—ï¼Œåˆ¤æ–­è¢‹å­çš„é€‰æ‹©</td></tr><tr><td>ç»“æ„</td><td>( p(x, y) = p(y)p(x</td><td>y) )</td></tr><tr><td>ç‰¹å¾å»ºæ¨¡</td><td>åªå…è®¸çŠ¶æ€ä¹‹é—´å’ŒçŠ¶æ€-è§‚æµ‹ä¹‹é—´çš„ç®€å•æ¦‚ç‡å…³ç³»</td><td>å…è®¸å®šä¹‰ä»»æ„ç‰¹å¾å‡½æ•°ï¼Œçµæ´»ç»„åˆå†å²å’Œä¸Šä¸‹æ–‡ä¿¡æ¯</td></tr><tr><td>æ¨ç†æ–¹å¼</td><td>ç”Ÿæˆæ¦‚ç‡æœ€å¤§è·¯å¾„</td><td>æ¡ä»¶æ¦‚ç‡æœ€å¤§è·¯å¾„</td></tr></tbody></table><h3 id="é‡ç‚¹-1" tabindex="-1"><a class="header-anchor" href="#é‡ç‚¹-1"><span>é‡ç‚¹</span></a></h3><p>ğŸ”¹ Forward Algorithmï¼ˆå‰å‘ç®—æ³•ï¼‰</p><p><strong>å®šä¹‰ï¼š</strong><br> å‰å‘ç®—æ³•æ˜¯<strong>éšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHidden Markov Model, HMMï¼‰ä¸­çš„æ ¸å¿ƒç®—æ³•ä¹‹ä¸€ï¼Œä¸»è¦ç”¨äºè®¡ç®—ä¸€ä¸ªè§‚æµ‹åºåˆ—å‡ºç°çš„æ¦‚ç‡</strong>ã€‚</p><p><strong>ç®—æ³•æ­¥éª¤ï¼š</strong></p><ol><li><strong>åˆå§‹åŒ–ï¼ˆInitializationï¼‰ï¼š</strong><br> è®¡ç®—åˆå§‹æ—¶åˆ»æ¯ä¸ªéšè—çŠ¶æ€çš„å‰å‘æ¦‚ç‡ã€‚</li><li><strong>é€’æ¨ï¼ˆRecursionï¼‰ï¼š</strong><br> å¯¹äºåç»­çš„æ¯ä¸ªæ—¶åˆ»ï¼Œé€’å½’åœ°è®¡ç®—æ¯ä¸ªéšè—çŠ¶æ€çš„å‰å‘æ¦‚ç‡ã€‚</li><li><strong>ç»ˆæ­¢ï¼ˆTerminationï¼‰ï¼š</strong><br> æ‰€æœ‰å‰å‘æ¦‚ç‡çš„åŠ å’Œå³ä¸ºæ•´ä¸ªè§‚æµ‹åºåˆ—çš„æ€»æ¦‚ç‡ã€‚</li></ol><hr><p>ğŸ”¹ Backward Algorithmï¼ˆåå‘ç®—æ³•ï¼‰</p><p>**æ³¨æ„ï¼š**è¯¥æœ¯è¯­åœ¨ä¸¤ä¸ªä¸åŒé¢†åŸŸä¸­æœ‰ä¸åŒå«ä¹‰ï¼š</p><hr><p>âœ… 1. åœ¨<strong>ç¥ç»ç½‘ç»œ</strong>ä¸­ï¼š</p><p><strong>Backpropagation Algorithmï¼ˆåå‘ä¼ æ’­ç®—æ³•ï¼‰ï¼š</strong></p><ul><li>æ˜¯ä¸€ç§è®¡ç®—æŸå¤±å‡½æ•°å…³äºç¥ç»ç½‘ç»œå‚æ•°çš„<strong>æ¢¯åº¦</strong>çš„ç®—æ³•ã€‚</li><li>å®ƒåŸºäºé“¾å¼æ³•åˆ™ï¼Œä»è¾“å‡ºå±‚å¼€å§‹<strong>é€å±‚å‘åè®¡ç®—æ¢¯åº¦</strong>ï¼Œä»è€Œä¼˜åŒ–æ¨¡å‹å‚æ•°ã€‚</li><li>è¿™æ˜¯æ·±åº¦å­¦ä¹ ä¸­æœ€æ ¸å¿ƒçš„æŠ€æœ¯ä¹‹ä¸€ï¼Œå¹¿æ³›ç”¨äºå„ç§ç¥ç»ç½‘ç»œçš„è®­ç»ƒä¸­ã€‚</li></ul><hr><p>âœ… 2. åœ¨<strong>æ¦‚ç‡å›¾æ¨¡å‹ / HMM</strong> ä¸­ï¼š</p><p><strong>Backward Algorithmï¼ˆåå‘ç®—æ³•ï¼‰ï¼š</strong></p><ul><li>æ˜¯éšé©¬å°”å¯å¤«æ¨¡å‹ä¸­çš„<strong>åŠ¨æ€è§„åˆ’ç®—æ³•</strong>ï¼Œç”¨äº<strong>è®¡ç®—ç»™å®šè§‚æµ‹åºåˆ—çš„æ¡ä»¶ä¸‹ï¼ŒæŸä¸€æ—¶åˆ»å¤„äºç‰¹å®šéšè—çŠ¶æ€çš„æ¦‚ç‡</strong>ã€‚</li><li>é€šå¸¸ä¸å‰å‘ç®—æ³•ç»“åˆä½¿ç”¨ï¼Œèƒ½æœ‰æ•ˆæ±‚è§£ HMM ä¸­çš„å¤šä¸ªæ¦‚ç‡è®¡ç®—ä»»åŠ¡ï¼ŒåŒ…æ‹¬ï¼š <ul><li>åºåˆ—æ¦‚ç‡è¯„ä¼°</li><li>æœ€ä¼˜çŠ¶æ€è·¯å¾„è§£ç </li><li>æ¨¡å‹å‚æ•°å­¦ä¹ </li></ul></li></ul><h2 id="automatic-word-segmentation-named-entity-recognition-and-part-of-speech-tagging" tabindex="-1"><a class="header-anchor" href="#automatic-word-segmentation-named-entity-recognition-and-part-of-speech-tagging"><span>Automatic Word Segmentation, Named Entity Recognition, and Part-of-Speech Tagging</span></a></h2><p>æ¦‚å¿µè®°å¿†ã€‚é‡ç‚¹æ¢è®¨ä¸­æ–‡åˆ†è¯å’Œå‘½åå®ä½“è¯†åˆ«ç­‰æ¦‚å¿µã€å®ƒä»¬çš„ä»»åŠ¡ä»¥åŠä¸»è¦çš„ç ”ç©¶æŒ‘æˆ˜ã€‚</p><h3 id="q1-4" tabindex="-1"><a class="header-anchor" href="#q1-4"><span>Q1</span></a></h3><p><mark>Automatic word segmentation, named entity recognition, and part-of-speech tagging</mark>. These tasks form the foundation of text processing and language understanding, with wide applications in machine translation, information retrieval, question-answering systems, and other scenarios.</p><p>è‡ªç„¶è¯­è¨€å¤„ç† (NLP) é¢†åŸŸçš„ä¸‰å¤§å…³é”®ä»»åŠ¡ï¼šè‡ªåŠ¨åˆ†è¯ã€å‘½åå®ä½“è¯†åˆ«å’Œè¯æ€§æ ‡æ³¨ã€‚è¿™äº›ä»»åŠ¡æ„æˆäº†æ–‡æœ¬å¤„ç†å’Œè¯­è¨€ç†è§£çš„åŸºç¡€ï¼Œå¹¿æ³›åº”ç”¨äºæœºå™¨ç¿»è¯‘ã€ä¿¡æ¯æ£€ç´¢ã€é—®ç­”ç³»ç»Ÿç­‰åœºæ™¯ã€‚</p><blockquote><p>Fundamental issues in Chinese automatic word segmentation</p></blockquote><ol><li>Word standardization Problem: Key issue: Whatâ€™s a â€œwordâ€? <ul><li>Morphemes or phrases</li></ul></li><li>Recognition of Unknown words</li><li>Ambiguity in Segmentation: <ul><li>Overlapping ambiguity</li><li>Combination ambiguity</li></ul></li></ol><p>Automatic word segmentation refers to the process of <mark>dividing continuous character sequences into meaningful word units</mark>, which is particularly <mark>crucial for languages like Chinese that lack clear word boundaries</mark>. For example, segmenting &quot;æˆ‘å–œæ¬¢å­¦ä¹ &quot; into &quot;æˆ‘/å–œæ¬¢/å­¦ä¹ â€œ</p><p>è‡ªåŠ¨åˆ†è¯æ˜¯æŒ‡å°†è¿ç»­çš„å­—ç¬¦åºåˆ—åˆ’åˆ†æˆæœ‰æ„ä¹‰çš„è¯å•å…ƒçš„è¿‡ç¨‹ï¼Œè¿™å¯¹äºåƒä¸­æ–‡è¿™æ ·ç¼ºä¹æ˜ç¡®è¯æ±‡ç•Œé™çš„è¯­è¨€å°¤ä¸ºé‡è¦ã€‚ä¾‹å¦‚ï¼Œå°†â€œæˆ‘å–œæ¬¢å­¦ä¹ â€åˆ†å‰²æˆâ€œæˆ‘/å–œæ¬¢/å­¦ä¹ â€</p><p>Automatic word segmentation: This is a crucial step in natural language processing for dividing unsegmented text into independent word units. It is particularly important for languages like Chinese where words aren&#39;t separated by spaces.</p><p>è‡ªåŠ¨åˆ†è¯ï¼šè¿™æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å…³é”®æ­¥éª¤ï¼Œç”¨äºå°†æœªåˆ†æ®µçš„æ–‡æœ¬åˆ’åˆ†ä¸ºç‹¬ç«‹çš„è¯å•å…ƒã€‚å¯¹äºåƒä¸­æ–‡è¿™æ ·è¯ä¸è¯ä¹‹é—´ä¸å¸¦ç©ºæ ¼åˆ†éš”çš„è¯­è¨€æ¥è¯´ï¼Œè¿™ä¸€ç‚¹å°¤ä¸ºé‡è¦ã€‚</p><blockquote><p>Methods for Chinese word segmentation</p></blockquote><ol><li>æœ€å¤§åŒ¹é…æ³•ï¼ˆMaximum Matchingï¼Œ MMï¼‰</li><li>æœ€å°‘åˆ†è¯æ³•ï¼ˆæœ€çŸ­è·¯å¾„æ³•ï¼‰ï¼ˆShort Path Methodï¼‰</li><li>åŸºäºè¯­è¨€æ¨¡å‹çš„åˆ†è¯æ–¹æ³•ï¼ˆLanguage Model-based Word Segmentationï¼‰</li><li>åŸºäºéšé©¬å°”ç§‘å¤«æ¨¡å‹çš„åˆ†è¯æ–¹æ³•ï¼ˆHidden Markov model-based word segmentationï¼‰</li><li>ç”±å­—æ„è¯ï¼ˆåŸºäºå­—æ ‡æ³¨ï¼‰çš„åˆ†è¯æ–¹æ³•ï¼ˆCharacter-based Tagging Method)</li><li>ç”Ÿæˆå¼æ–¹æ³•å’ŒåŒºåˆ†å¼æ–¹æ³•çš„ç»“åˆï¼ˆCombination of Generative and Discriminative Modelï¼‰</li></ol><blockquote><p>Named entity recognition</p></blockquote><ol><li>å…³äºä¸­å›½å§“åï¼š <ul><li>å§“ååº“åŒ¹é…</li><li>è®¡ç®—æ½œåœ¨å§“åçš„æ¦‚ç‡ä¼°å€¼</li></ul></li><li>åœ°åï¼š <ul><li>å»ºç«‹åœ°åèµ„æºçŸ¥è¯†åº“</li><li>å»ºç«‹è¯†åˆ«è§„åˆ™åº“</li><li><strong>ç»Ÿè®¡æ¨¡å‹</strong></li><li><strong>é€šè¿‡è®­ç»ƒè¯­æ–™é€‰å–é˜ˆå€¼</strong></li><li><strong>åœ°ååˆç­›é€‰</strong></li><li><strong>å¯»æ‰¾å¯ä»¥åˆ©ç”¨çš„ä¸Šä¸‹æ–‡ä¿¡æ¯</strong></li><li><strong>åˆ©ç”¨è§„åˆ™è¿›ä¸€æ­¥ç¡®å®šåœ°å</strong></li></ul></li></ol><p><mark>Named entity recognition aims to identify specific types of entity names from text, such as person names, location names, and organization names</mark>. For instance, identifying &quot; æ å &quot; as a person name and &quot;åŒ—äº¬&quot; as a location name from the sentence &quot;æååœ¨åŒ—äº¬ å·¥ä½œ&quot;.</p><p>å‘½åå®ä½“è¯†åˆ«æ—¨åœ¨ä»æ–‡æœ¬ä¸­è¯†åˆ«ç‰¹å®šç±»å‹çš„å®ä½“åç§°ï¼Œä¾‹å¦‚äººåã€åœ°åã€æœºæ„åç§°ç­‰ã€‚ä¾‹å¦‚ï¼Œä»â€œæååœ¨åŒ—äº¬å·¥ä½œâ€è¿™å¥è¯ä¸­è¯†åˆ«å‡ºâ€œæåâ€ä¸ºäººåï¼Œè¯†åˆ«å‡ºâ€œåŒ—äº¬â€ä¸ºåœ°åã€‚</p><p>Named entity recognition: Abbreviated as NER (Named Entity Recognition), it&#39;s a subtask of information extraction that identifies specific meaningful entities in text, such as person names, locations, organizations, dates, etc. It plays a key role in many applications like search engine optimization and knowledge graph construction.</p><p>å‘½åå®ä½“è¯†åˆ«ï¼šç®€ç§°NERï¼ˆNamed Entity Recognitionï¼‰ï¼Œå®ƒæ˜¯ä¿¡æ¯æå–çš„ä¸€ä¸ªå­ä»»åŠ¡ï¼Œç”¨äºè¯†åˆ«æ–‡æœ¬ä¸­ç‰¹å®šçš„ã€æœ‰æ„ä¹‰çš„å®ä½“ï¼Œä¾‹å¦‚äººåã€åœ°ç‚¹ã€ç»„ç»‡ã€æ—¥æœŸç­‰ã€‚å®ƒåœ¨æœç´¢å¼•æ“ä¼˜åŒ–å’ŒçŸ¥è¯†å›¾è°±æ„å»ºç­‰è®¸å¤šåº”ç”¨ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚</p><blockquote><p>Part-of-speech tagging</p></blockquote><p><mark>Part-of-speech tagging involves assigning grammatical attributes (such as noun, verb, adjective, etc.) to each word</mark>. For example, in the sentence &quot;çŒ«æ‰äº†è€é¼ &quot;,&quot;çŒ«&quot; is tagged as a noun and &quot;æ‰&quot; as a verb. <mark>These three tasks collectively constitute fundamental modules of<br> language processing, enabling computers to better understand and process human language</mark>.</p><p>è¯æ€§æ ‡æ³¨æ˜¯æŒ‡ä¸ºæ¯ä¸ªè¯åˆ†é…è¯­æ³•å±æ€§ï¼ˆä¾‹å¦‚åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ç­‰ï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨â€œçŒ«æ‰äº†è€é¼ â€è¿™å¥è¯ä¸­ï¼Œâ€œçŒ«â€è¢«æ ‡æ³¨ä¸ºåè¯ï¼Œâ€œæ‰â€è¢«æ ‡æ³¨ä¸ºåŠ¨è¯ã€‚<strong>è¿™ä¸‰ä¸ªä»»åŠ¡å…±åŒæ„æˆäº†è¯­è¨€å¤„ç†çš„åŸºæœ¬æ¨¡å—ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå¤„ç†äººç±»è¯­è¨€ã€‚</strong></p><p>Part-of-speech tagging: This fundamental NLP task involves assigning a part-of-speech label (e.g., noun, verb, adjective) to each word in a sentence. It provides essential grammatical information for subsequent syntactic analysis and semantic understanding.</p><p>è¯æ€§æ ‡æ³¨ï¼šè¿™é¡¹åŸºç¡€çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡æ¶‰åŠä¸ºå¥å­ä¸­çš„æ¯ä¸ªå•è¯åˆ†é…è¯æ€§æ ‡ç­¾ï¼ˆä¾‹å¦‚ï¼Œåè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ï¼‰ã€‚å®ƒä¸ºåç»­çš„å¥æ³•åˆ†æå’Œè¯­ä¹‰ç†è§£æä¾›å¿…è¦çš„è¯­æ³•ä¿¡æ¯ã€‚</p><h2 id="syntactic-parsing" tabindex="-1"><a class="header-anchor" href="#syntactic-parsing"><span>Syntactic Parsing</span></a></h2><blockquote><p>æ³¨é‡æ¦‚å¿µè®°å¿†ä¸ç†è§£ï¼Œä¸è¦æ±‚ç®—æ³•è®¡ç®—ã€‚é‡ç‚¹è€ƒæŸ¥å¥æ³•ç»“æ„åˆ†æåº”ç”¨çš„æ¦‚å¿µã€ä»»åŠ¡å’Œä¸»è¦æ¦‚æ‹¬å†…å®¹ã€‚</p></blockquote><figure><img src="'+v+'" alt="image-20250623011126090" tabindex="0" loading="lazy"><figcaption>image-20250623011126090</figcaption></figure><h3 id="q1-5" tabindex="-1"><a class="header-anchor" href="#q1-5"><span>Q1</span></a></h3><blockquote><p>Basic parsing methods based on PCFG</p></blockquote><p><strong>Probabilistic Context-Free Grammar æ¦‚ç‡ä¸Šä¸‹æ–‡æ— å…³æ–‡æ³•</strong></p><figure><img src="'+w+'" alt="image-20250622164217188" tabindex="0" loading="lazy"><figcaption>image-20250622164217188</figcaption></figure><h3 id="q2-4" tabindex="-1"><a class="header-anchor" href="#q2-4"><span>Q2</span></a></h3><blockquote><p>Lexicalized phrase structure parser</p></blockquote><p>è¿™ä¿©æ˜¯æ–¹æ³•</p><h3 id="q3-2" tabindex="-1"><a class="header-anchor" href="#q3-2"><span>Q3</span></a></h3><blockquote><p>Unlexicalized syntactic parsers</p></blockquote><h2 id="semantic-analysis" tabindex="-1"><a class="header-anchor" href="#semantic-analysis"><span>Semantic Analysis</span></a></h2><blockquote><p><strong>Supervised word sense disambiguation methods</strong></p></blockquote><figure><img src="'+z+'" alt="image-20250622172555655" tabindex="0" loading="lazy"><figcaption>image-20250622172555655</figcaption></figure><blockquote><p><strong>Dictionary-based semantic disambiguation methods</strong></p></blockquote><ul><li>åŸºäºè¯­ä¹‰å®šä¹‰çš„æ¶ˆæ­§</li><li>åŸºäºä¹‰ç±»è¾å…¸ (thesaurus) çš„æ¶ˆæ­§</li><li>åŸºäºåŒè¯­è¯å…¸çš„æ¶ˆæ­§</li><li>Yarowsky æ¶ˆæ­§ç®—æ³•</li></ul><blockquote><p><strong>Unsupervised word sense disambiguation methods</strong></p></blockquote><figure><img src="'+T+'" alt="image-20250622172723224" tabindex="0" loading="lazy"><figcaption>image-20250622172723224</figcaption></figure>',126)]))}const Q=t(C,[["render",P]]),R=JSON.parse('{"path":"/zh/NLP/Review_question.html","title":"å¤ä¹ é—®é¢˜","lang":"zh-CN","frontmatter":{"title":"å¤ä¹ é—®é¢˜","icon":"alias","date":"2025-06-22T12:22:43.000Z","author":"XiaoXianYue","isOriginal":true,"category":["å¤§ä¸‰ä¸‹","NLP"],"tag":["å¤§ä¸‰ä¸‹","NLP"],"sticky":false,"star":false,"article":true,"timeline":true,"image":false,"navbar":true,"sidebarIcon":true,"headerDepth":5,"lastUpdated":true,"editLink":false,"backToTop":true,"toc":true,"description":"Introduction Q1 The developmental history of natural language processing technologies ï¼ˆFocus on mastering the three major tasks of natural language processing, as well as the ea...","head":[["meta",{"property":"og:url","content":"https://bougiemoonintaurus/zh/NLP/Review_question.html"}],["meta",{"property":"og:site_name","content":"å¥¶é…ªå¥¶é…ª"}],["meta",{"property":"og:title","content":"å¤ä¹ é—®é¢˜"}],["meta",{"property":"og:description","content":"Introduction Q1 The developmental history of natural language processing technologies ï¼ˆFocus on mastering the three major tasks of natural language processing, as well as the ea..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-07-21T01:25:32.000Z"}],["meta",{"property":"article:author","content":"XiaoXianYue"}],["meta",{"property":"article:tag","content":"å¤§ä¸‰ä¸‹"}],["meta",{"property":"article:tag","content":"NLP"}],["meta",{"property":"article:published_time","content":"2025-06-22T12:22:43.000Z"}],["meta",{"property":"article:modified_time","content":"2025-07-21T01:25:32.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"å¤ä¹ é—®é¢˜\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-06-22T12:22:43.000Z\\",\\"dateModified\\":\\"2025-07-21T01:25:32.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"XiaoXianYue\\"}]}"]]},"git":{"createdTime":1753061132000,"updatedTime":1753061132000,"contributors":[{"name":"Xiaoxianyue","username":"Xiaoxianyue","email":"2310219843@qq.com","commits":1,"url":"https://github.com/Xiaoxianyue"}]},"readingTime":{"minutes":8.09,"words":2428},"filePathRelative":"zh/NLP/Review_question.md","localizedDate":"2025å¹´6æœˆ22æ—¥","excerpt":"<h2>Introduction</h2>\\n<h3>Q1</h3>\\n<blockquote>\\n<p>The developmental history of natural language processing technologies ï¼ˆFocus on mastering the three major tasks of natural language processing, as well as the earliest natural language processing task.<strong>ï¼‰</strong></p>\\n</blockquote>\\n<ul>\\n<li>Rule-based (symbolic) approach</li>\\n<li>Statistical approach (traditional machine learning)</li>\\n<li>Connectionist approach</li>\\n<li>Pre-Training</li>\\n</ul>","autoDesc":true}');export{Q as comp,R as data};
