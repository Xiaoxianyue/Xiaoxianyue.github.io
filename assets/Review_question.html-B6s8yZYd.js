import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as e,b as i,o as n}from"./app-84lBMjzT.js";const o="/assets/image-20250622191618177-CIr8XmE8.png",s="/assets/image-20250622191642744-D8YSbenN.png",r="/assets/image-20250622191656878-agZWGsqC.png",l="/assets/image-20250622123746772-BaYrJThN.png",g="/assets/image-20250622123815494-cAQCfuh1.png",p="/assets/image-20250622123823003-CioQ7dgr.png",c="/assets/image-20250622123829053-DKxv7BIB.png",d="/assets/image-20250622123834145-B0LWXIFr.png",m="/assets/image-20250622123916986-DNvZKn_E.png",u="/assets/image-20250622123936839-CjNCvp0C.png",h="/assets/image-20250622123945600-BoR0pmXx.png",f="/assets/image-20250622124228480-BOeE2brB.png",b="/assets/image-20250622125908783-DggOWl-J.png",y="/assets/image-20250622130301542-DWPDGvsO.png",q="/assets/image-20250622130325680-5aolrZeB.png",k="/assets/image-20250622130641174-DdgfHoci.png",x="/assets/image-20250622133119741-CUwwcnAS.png",_="/assets/image-20250624121312718-BwRNJgvf.png",v="/assets/image-20250623011126090-iEoF_D8l.png",w="/assets/image-20250622164217188-DFHan5dv.png",z="/assets/image-20250622172555655-Df8wiPqx.png",T="/assets/image-20250622172723224-DkuBCf-j.png",C={};function P(N,a){return n(),e("div",null,a[0]||(a[0]=[i('<h2 id="introduction" tabindex="-1"><a class="header-anchor" href="#introduction"><span>Introduction</span></a></h2><h3 id="q1" tabindex="-1"><a class="header-anchor" href="#q1"><span>Q1</span></a></h3><blockquote><p>The developmental history of natural language processing technologies （Focus on mastering the three major tasks of natural language processing, as well as the earliest natural language processing task.<strong>）</strong></p></blockquote><ul><li>Rule-based (symbolic) approach</li><li>Statistical approach (traditional machine learning)</li><li>Connectionist approach</li><li>Pre-Training</li></ul><p>major tasks:</p><ul><li>Computational Linguistic Tasks</li><li>Information Extraction Tasks</li><li>Natural Language Generation</li></ul><p>the earliest natural language processing task：</p><ul><li>Machine Translation</li></ul><h3 id="q2" tabindex="-1"><a class="header-anchor" href="#q2"><span>Q2</span></a></h3><blockquote><p>Various levels of linguistic analysis</p></blockquote><table><thead><tr><th>层级</th><th>内容</th></tr></thead><tbody><tr><td><strong>音系学 (Phonology)</strong></td><td>声音系统的结构分析</td></tr><tr><td><strong>形态学 (Morphology)</strong></td><td>词的内部结构，如词缀、词根等</td></tr><tr><td><strong>句法学 (Syntax)</strong></td><td>分析词的结构与句法规则，包含：</td></tr></tbody></table><p>分词（Tokenization）</p><p>词性标注（POS Tagging）</p><p>成分句法分析（Constituent Parsing）</p><p>依存句法分析（Dependency Parsing）</p><ul><li><p><strong>语义学 (Semantics)</strong> 理解词与句子的意义，如：</p><ul><li><p>语义角色标注</p></li><li><p>指代消解</p></li><li><p>文本蕴含（Textual Entailment）</p></li></ul></li><li><p><strong>语用学 (Pragmatics)</strong> 上下文和意图理解<br><strong>篇章分析 (Discourse)</strong> 多句之间的结构关系，如：</p><ul><li><p>篇章分段</p></li><li><p>篇章连贯性分析（Rhetorical Structure Theory） |</p></li></ul></li></ul><h3 id="q3" tabindex="-1"><a class="header-anchor" href="#q3"><span>Q3</span></a></h3><blockquote><p>Current research status and related applications of natural language processing</p></blockquote><p>自然语言处理的研究现状及相关应用</p><figure><img src="'+o+'" alt="image-20250622191618177" tabindex="0" loading="lazy"><figcaption>image-20250622191618177</figcaption></figure><figure><img src="'+s+'" alt="image-20250622191642744" tabindex="0" loading="lazy"><figcaption>image-20250622191642744</figcaption></figure><figure><img src="'+r+'" alt="image-20250622191656878" tabindex="0" loading="lazy"><figcaption>image-20250622191656878</figcaption></figure><h2 id="_2-formal-languages-and-automata" tabindex="-1"><a class="header-anchor" href="#_2-formal-languages-and-automata"><span>2. Formal Languages and Automata</span></a></h2><p>Key examination content, focusing on formal grammars and automata. Key focus on relevant calculations and derivations.</p><p>考试内容重点：形式语法和自动机。重点关注相关计算和推导。</p><h3 id="题目" tabindex="-1"><a class="header-anchor" href="#题目"><span>题目</span></a></h3><h4 id="q1-1" tabindex="-1"><a class="header-anchor" href="#q1-1"><span>Q1</span></a></h4><blockquote><p>Definition and types of formal grammars (key examination content)</p></blockquote><figure><img src="'+l+'" alt="image-20250622123746772" tabindex="0" loading="lazy"><figcaption>image-20250622123746772</figcaption></figure><figure><img src="'+g+'" alt="image-20250622123815494" tabindex="0" loading="lazy"><figcaption>image-20250622123815494</figcaption></figure><figure><img src="'+p+'" alt="image-20250622123823003" tabindex="0" loading="lazy"><figcaption>image-20250622123823003</figcaption></figure><figure><img src="'+c+'" alt="image-20250622123829053" tabindex="0" loading="lazy"><figcaption>image-20250622123829053</figcaption></figure><figure><img src="'+d+'" alt="image-20250622123834145" tabindex="0" loading="lazy"><figcaption>image-20250622123834145</figcaption></figure><h4 id="q2-1" tabindex="-1"><a class="header-anchor" href="#q2-1"><span>Q2</span></a></h4><blockquote><p>Relationship between regular grammars and automata (key examination content)</p></blockquote><figure><img src="'+m+'" alt="image-20250622123916986" tabindex="0" loading="lazy"><figcaption>image-20250622123916986</figcaption></figure><figure><img src="'+u+'" alt="image-20250622123936839" tabindex="0" loading="lazy"><figcaption>image-20250622123936839</figcaption></figure><figure><img src="'+h+'" alt="image-20250622123945600" tabindex="0" loading="lazy"><figcaption>image-20250622123945600</figcaption></figure><figure><img src="'+f+'" alt="image-20250622124228480" tabindex="0" loading="lazy"><figcaption>image-20250622124228480</figcaption></figure><h3 id="重点" tabindex="-1"><a class="header-anchor" href="#重点"><span>重点</span></a></h3><ul><li><p>最左推导最右推导</p></li><li><p>CFG的定义（上面有）</p></li><li><p>CSG的定义</p></li><li><p>Ambiguity of Context-Free Grammars A grammar G is said to be ambiguous if there exists at least one sentence that corresponds to more than one parse tree. Draw the derivation tree of phrases and sentences.</p><ul><li>上下文无关文法的歧义性：如果文法G中至少有一个句子对应多棵解析树，则称该文法为歧义文法。请画出短语和句子的派生树。</li></ul></li><li><p>正则文法构建自动机</p></li></ul><h2 id="language-model" tabindex="-1"><a class="header-anchor" href="#language-model"><span>Language model</span></a></h2><h3 id="q1-2" tabindex="-1"><a class="header-anchor" href="#q1-2"><span>Q1</span></a></h3><blockquote><p>N-gram models (key examination content，Key focus on probability calculations utilizing 2-gram and 3-gram models）</p></blockquote><p>N-gram 模型（重点考试内容，重点关注利用 2-gram 和 3-gram 模型进行的概率计算）</p><figure><img src="'+b+'" alt="image-20250622125908783" tabindex="0" loading="lazy"><figcaption>image-20250622125908783</figcaption></figure><h3 id="q2-2" tabindex="-1"><a class="header-anchor" href="#q2-2"><span>Q2</span></a></h3><blockquote><p>Performance evaluation of language models (key examination content, conceptual memorization</p></blockquote><p>语言模型绩效评估（重点考试内容、概念记忆）</p><figure><img src="'+y+'" alt="image-20250622130301542" tabindex="0" loading="lazy"><figcaption>image-20250622130301542</figcaption></figure><figure><img src="'+q+'" alt="image-20250622130325680" tabindex="0" loading="lazy"><figcaption>image-20250622130325680</figcaption></figure><h3 id="q3-1" tabindex="-1"><a class="header-anchor" href="#q3-1"><span>Q3</span></a></h3><blockquote><p>Data smoothing methods (focusing on major data smoothing calculations)</p></blockquote><figure><img src="'+k+'" alt="image-20250622130641174" tabindex="0" loading="lazy"><figcaption>image-20250622130641174</figcaption></figure><h3 id="q4" tabindex="-1"><a class="header-anchor" href="#q4"><span>Q4</span></a></h3><blockquote><p>Language model adaptation methods (for classroom explanation only)</p></blockquote><p>语言模型自适应方法（仅供课堂讲解）</p><h2 id="隐马尔科夫和条件随机场" tabindex="-1"><a class="header-anchor" href="#隐马尔科夫和条件随机场"><span>隐马尔科夫和条件随机场</span></a></h2><h3 id="题目-1" tabindex="-1"><a class="header-anchor" href="#题目-1"><span>题目</span></a></h3><h4 id="q1-3" tabindex="-1"><a class="header-anchor" href="#q1-3"><span>Q1</span></a></h4><blockquote><p>Basic methods of probabilistic graphical models (including Hidden Markov Models)</p></blockquote><p>概率图模型（包括隐马尔可夫模型）的基本方法</p><figure><img src="'+x+'" alt="image-20250622133119741" tabindex="0" loading="lazy"><figcaption>image-20250622133119741</figcaption></figure><h4 id="q2-3" tabindex="-1"><a class="header-anchor" href="#q2-3"><span>Q2</span></a></h4><blockquote><p>Conditional random field models (for theoretical comprehension only</p></blockquote><figure><img src="'+_+'" alt="image-20250624121312718" tabindex="0" loading="lazy"><figcaption>image-20250624121312718</figcaption></figure><table><thead><tr><th>项目</th><th>HMM</th><th>CRF</th></tr></thead><tbody><tr><td>模型关注</td><td>模拟球的生成过程（状态转移 + 生成输出）</td><td>给定球的颜色序列，判断袋子的选择</td></tr><tr><td>结构</td><td>( p(x, y) = p(y)p(x</td><td>y) )</td></tr><tr><td>特征建模</td><td>只允许状态之间和状态-观测之间的简单概率关系</td><td>允许定义任意特征函数，灵活组合历史和上下文信息</td></tr><tr><td>推理方式</td><td>生成概率最大路径</td><td>条件概率最大路径</td></tr></tbody></table><h3 id="重点-1" tabindex="-1"><a class="header-anchor" href="#重点-1"><span>重点</span></a></h3><p>🔹 Forward Algorithm（前向算法）</p><p><strong>定义：</strong><br> 前向算法是<strong>隐马尔可夫模型（Hidden Markov Model, HMM）中的核心算法之一，主要用于计算一个观测序列出现的概率</strong>。</p><p><strong>算法步骤：</strong></p><ol><li><strong>初始化（Initialization）：</strong><br> 计算初始时刻每个隐藏状态的前向概率。</li><li><strong>递推（Recursion）：</strong><br> 对于后续的每个时刻，递归地计算每个隐藏状态的前向概率。</li><li><strong>终止（Termination）：</strong><br> 所有前向概率的加和即为整个观测序列的总概率。</li></ol><hr><p>🔹 Backward Algorithm（后向算法）</p><p>**注意：**该术语在两个不同领域中有不同含义：</p><hr><p>✅ 1. 在<strong>神经网络</strong>中：</p><p><strong>Backpropagation Algorithm（反向传播算法）：</strong></p><ul><li>是一种计算损失函数关于神经网络参数的<strong>梯度</strong>的算法。</li><li>它基于链式法则，从输出层开始<strong>逐层向后计算梯度</strong>，从而优化模型参数。</li><li>这是深度学习中最核心的技术之一，广泛用于各种神经网络的训练中。</li></ul><hr><p>✅ 2. 在<strong>概率图模型 / HMM</strong> 中：</p><p><strong>Backward Algorithm（后向算法）：</strong></p><ul><li>是隐马尔可夫模型中的<strong>动态规划算法</strong>，用于<strong>计算给定观测序列的条件下，某一时刻处于特定隐藏状态的概率</strong>。</li><li>通常与前向算法结合使用，能有效求解 HMM 中的多个概率计算任务，包括： <ul><li>序列概率评估</li><li>最优状态路径解码</li><li>模型参数学习</li></ul></li></ul><h2 id="automatic-word-segmentation-named-entity-recognition-and-part-of-speech-tagging" tabindex="-1"><a class="header-anchor" href="#automatic-word-segmentation-named-entity-recognition-and-part-of-speech-tagging"><span>Automatic Word Segmentation, Named Entity Recognition, and Part-of-Speech Tagging</span></a></h2><p>概念记忆。重点探讨中文分词和命名实体识别等概念、它们的任务以及主要的研究挑战。</p><h3 id="q1-4" tabindex="-1"><a class="header-anchor" href="#q1-4"><span>Q1</span></a></h3><p><mark>Automatic word segmentation, named entity recognition, and part-of-speech tagging</mark>. These tasks form the foundation of text processing and language understanding, with wide applications in machine translation, information retrieval, question-answering systems, and other scenarios.</p><p>自然语言处理 (NLP) 领域的三大关键任务：自动分词、命名实体识别和词性标注。这些任务构成了文本处理和语言理解的基础，广泛应用于机器翻译、信息检索、问答系统等场景。</p><blockquote><p>Fundamental issues in Chinese automatic word segmentation</p></blockquote><ol><li>Word standardization Problem: Key issue: What’s a “word”? <ul><li>Morphemes or phrases</li></ul></li><li>Recognition of Unknown words</li><li>Ambiguity in Segmentation: <ul><li>Overlapping ambiguity</li><li>Combination ambiguity</li></ul></li></ol><p>Automatic word segmentation refers to the process of <mark>dividing continuous character sequences into meaningful word units</mark>, which is particularly <mark>crucial for languages like Chinese that lack clear word boundaries</mark>. For example, segmenting &quot;我喜欢学习&quot; into &quot;我/喜欢/学习“</p><p>自动分词是指将连续的字符序列划分成有意义的词单元的过程，这对于像中文这样缺乏明确词汇界限的语言尤为重要。例如，将“我喜欢学习”分割成“我/喜欢/学习”</p><p>Automatic word segmentation: This is a crucial step in natural language processing for dividing unsegmented text into independent word units. It is particularly important for languages like Chinese where words aren&#39;t separated by spaces.</p><p>自动分词：这是自然语言处理中的关键步骤，用于将未分段的文本划分为独立的词单元。对于像中文这样词与词之间不带空格分隔的语言来说，这一点尤为重要。</p><blockquote><p>Methods for Chinese word segmentation</p></blockquote><ol><li>最大匹配法（Maximum Matching， MM）</li><li>最少分词法（最短路径法）（Short Path Method）</li><li>基于语言模型的分词方法（Language Model-based Word Segmentation）</li><li>基于隐马尔科夫模型的分词方法（Hidden Markov model-based word segmentation）</li><li>由字构词（基于字标注）的分词方法（Character-based Tagging Method)</li><li>生成式方法和区分式方法的结合（Combination of Generative and Discriminative Model）</li></ol><blockquote><p>Named entity recognition</p></blockquote><ol><li>关于中国姓名： <ul><li>姓名库匹配</li><li>计算潜在姓名的概率估值</li></ul></li><li>地名： <ul><li>建立地名资源知识库</li><li>建立识别规则库</li><li><strong>统计模型</strong></li><li><strong>通过训练语料选取阈值</strong></li><li><strong>地名初筛选</strong></li><li><strong>寻找可以利用的上下文信息</strong></li><li><strong>利用规则进一步确定地名</strong></li></ul></li></ol><p><mark>Named entity recognition aims to identify specific types of entity names from text, such as person names, location names, and organization names</mark>. For instance, identifying &quot; 李 华 &quot; as a person name and &quot;北京&quot; as a location name from the sentence &quot;李华在北京 工作&quot;.</p><p>命名实体识别旨在从文本中识别特定类型的实体名称，例如人名、地名、机构名称等。例如，从“李华在北京工作”这句话中识别出“李华”为人名，识别出“北京”为地名。</p><p>Named entity recognition: Abbreviated as NER (Named Entity Recognition), it&#39;s a subtask of information extraction that identifies specific meaningful entities in text, such as person names, locations, organizations, dates, etc. It plays a key role in many applications like search engine optimization and knowledge graph construction.</p><p>命名实体识别：简称NER（Named Entity Recognition），它是信息提取的一个子任务，用于识别文本中特定的、有意义的实体，例如人名、地点、组织、日期等。它在搜索引擎优化和知识图谱构建等许多应用中发挥着关键作用。</p><blockquote><p>Part-of-speech tagging</p></blockquote><p><mark>Part-of-speech tagging involves assigning grammatical attributes (such as noun, verb, adjective, etc.) to each word</mark>. For example, in the sentence &quot;猫捉了老鼠&quot;,&quot;猫&quot; is tagged as a noun and &quot;捉&quot; as a verb. <mark>These three tasks collectively constitute fundamental modules of<br> language processing, enabling computers to better understand and process human language</mark>.</p><p>词性标注是指为每个词分配语法属性（例如名词、动词、形容词等）。例如，在“猫捉了老鼠”这句话中，“猫”被标注为名词，“捉”被标注为动词。<strong>这三个任务共同构成了语言处理的基本模块，使计算机能够更好地理解和处理人类语言。</strong></p><p>Part-of-speech tagging: This fundamental NLP task involves assigning a part-of-speech label (e.g., noun, verb, adjective) to each word in a sentence. It provides essential grammatical information for subsequent syntactic analysis and semantic understanding.</p><p>词性标注：这项基础的自然语言处理任务涉及为句子中的每个单词分配词性标签（例如，名词、动词、形容词）。它为后续的句法分析和语义理解提供必要的语法信息。</p><h2 id="syntactic-parsing" tabindex="-1"><a class="header-anchor" href="#syntactic-parsing"><span>Syntactic Parsing</span></a></h2><blockquote><p>注重概念记忆与理解，不要求算法计算。重点考查句法结构分析应用的概念、任务和主要概括内容。</p></blockquote><figure><img src="'+v+'" alt="image-20250623011126090" tabindex="0" loading="lazy"><figcaption>image-20250623011126090</figcaption></figure><h3 id="q1-5" tabindex="-1"><a class="header-anchor" href="#q1-5"><span>Q1</span></a></h3><blockquote><p>Basic parsing methods based on PCFG</p></blockquote><p><strong>Probabilistic Context-Free Grammar 概率上下文无关文法</strong></p><figure><img src="'+w+'" alt="image-20250622164217188" tabindex="0" loading="lazy"><figcaption>image-20250622164217188</figcaption></figure><h3 id="q2-4" tabindex="-1"><a class="header-anchor" href="#q2-4"><span>Q2</span></a></h3><blockquote><p>Lexicalized phrase structure parser</p></blockquote><p>这俩是方法</p><h3 id="q3-2" tabindex="-1"><a class="header-anchor" href="#q3-2"><span>Q3</span></a></h3><blockquote><p>Unlexicalized syntactic parsers</p></blockquote><h2 id="semantic-analysis" tabindex="-1"><a class="header-anchor" href="#semantic-analysis"><span>Semantic Analysis</span></a></h2><blockquote><p><strong>Supervised word sense disambiguation methods</strong></p></blockquote><figure><img src="'+z+'" alt="image-20250622172555655" tabindex="0" loading="lazy"><figcaption>image-20250622172555655</figcaption></figure><blockquote><p><strong>Dictionary-based semantic disambiguation methods</strong></p></blockquote><ul><li>基于语义定义的消歧</li><li>基于义类辞典 (thesaurus) 的消歧</li><li>基于双语词典的消歧</li><li>Yarowsky 消歧算法</li></ul><blockquote><p><strong>Unsupervised word sense disambiguation methods</strong></p></blockquote><figure><img src="'+T+'" alt="image-20250622172723224" tabindex="0" loading="lazy"><figcaption>image-20250622172723224</figcaption></figure>',126)]))}const Q=t(C,[["render",P]]),R=JSON.parse('{"path":"/zh/NLP/Review_question.html","title":"复习问题","lang":"zh-CN","frontmatter":{"title":"复习问题","icon":"alias","date":"2025-06-22T12:22:43.000Z","author":"XiaoXianYue","isOriginal":true,"category":["大三下","NLP"],"tag":["大三下","NLP"],"sticky":false,"star":false,"article":true,"timeline":true,"image":false,"navbar":true,"sidebarIcon":true,"headerDepth":5,"lastUpdated":true,"editLink":false,"backToTop":true,"toc":true,"description":"Introduction Q1 The developmental history of natural language processing technologies （Focus on mastering the three major tasks of natural language processing, as well as the ea...","head":[["meta",{"property":"og:url","content":"https://bougiemoonintaurus/zh/NLP/Review_question.html"}],["meta",{"property":"og:site_name","content":"奶酪奶酪"}],["meta",{"property":"og:title","content":"复习问题"}],["meta",{"property":"og:description","content":"Introduction Q1 The developmental history of natural language processing technologies （Focus on mastering the three major tasks of natural language processing, as well as the ea..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-07-21T01:25:32.000Z"}],["meta",{"property":"article:author","content":"XiaoXianYue"}],["meta",{"property":"article:tag","content":"大三下"}],["meta",{"property":"article:tag","content":"NLP"}],["meta",{"property":"article:published_time","content":"2025-06-22T12:22:43.000Z"}],["meta",{"property":"article:modified_time","content":"2025-07-21T01:25:32.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"复习问题\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-06-22T12:22:43.000Z\\",\\"dateModified\\":\\"2025-07-21T01:25:32.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"XiaoXianYue\\"}]}"]]},"git":{"createdTime":1753061132000,"updatedTime":1753061132000,"contributors":[{"name":"Xiaoxianyue","username":"Xiaoxianyue","email":"2310219843@qq.com","commits":1,"url":"https://github.com/Xiaoxianyue"}]},"readingTime":{"minutes":8.09,"words":2428},"filePathRelative":"zh/NLP/Review_question.md","localizedDate":"2025年6月22日","excerpt":"<h2>Introduction</h2>\\n<h3>Q1</h3>\\n<blockquote>\\n<p>The developmental history of natural language processing technologies （Focus on mastering the three major tasks of natural language processing, as well as the earliest natural language processing task.<strong>）</strong></p>\\n</blockquote>\\n<ul>\\n<li>Rule-based (symbolic) approach</li>\\n<li>Statistical approach (traditional machine learning)</li>\\n<li>Connectionist approach</li>\\n<li>Pre-Training</li>\\n</ul>","autoDesc":true}');export{Q as comp,R as data};
