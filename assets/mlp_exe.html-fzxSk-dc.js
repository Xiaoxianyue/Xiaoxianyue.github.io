import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as a,b as l,o as p}from"./app-Cc7WVUoo.js";const o="/assets/image-20250305105800654-BvubQ9CH.png",e="/assets/image-20250305110637969-DfaLuYVL.png",t="/assets/image-20250305114333054-VrbGpsiY.png",F="/assets/image-20250305192419350-Bt7tBkfC.png",c="/assets/image-20250305192454066-CV1_g0ZS.png",r="/assets/image-20250305193222677-BGUDDtye.png",i="/assets/image-20250305201001514-AtlviG4E.png",y="/assets/image-20250305200248238-Dr0vTGKo.png",d="/assets/image-20250305202109776-B3mCxRK0.png",v={};function m(u,s){return p(),a("div",null,s[0]||(s[0]=[l(`<h3 id="代码" tabindex="-1"><a class="header-anchor" href="#代码"><span>代码</span></a></h3><div class="language-python line-numbers-mode has-collapsed-lines collapsed" data-highlighter="shiki" data-ext="python" style="--vp-collapsed-lines:15;background-color:#272822;color:#F8F8F2;"><pre class="shiki monokai vp-code"><code><span class="line"><span style="color:#F92672;">import</span><span style="color:#F8F8F2;"> torch</span></span>
<span class="line"><span style="color:#F92672;">from</span><span style="color:#F8F8F2;"> torch </span><span style="color:#F92672;">import</span><span style="color:#F8F8F2;"> nn</span></span>
<span class="line"><span style="color:#F92672;">import</span><span style="color:#F8F8F2;"> torchvision</span></span>
<span class="line"><span style="color:#F92672;">import</span><span style="color:#F8F8F2;"> torchvision.transforms </span><span style="color:#F92672;">as</span><span style="color:#F8F8F2;"> transforms</span></span>
<span class="line"><span style="color:#F92672;">import</span><span style="color:#F8F8F2;"> matplotlib.pyplot </span><span style="color:#F92672;">as</span><span style="color:#F8F8F2;"> plt</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F8F8F2;">batch_size </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 256</span></span>
<span class="line"><span style="color:#F8F8F2;">num_inputs, num_outputs </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 784</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">10</span><span style="color:#F8F8F2;">  </span></span>
<span class="line"><span style="color:#F8F8F2;">num_hiddens </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 128</span><span style="color:#F8F8F2;">  </span></span>
<span class="line"><span style="color:#F8F8F2;">num_layers </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 2</span><span style="color:#F8F8F2;"> </span></span>
<span class="line"><span style="color:#F8F8F2;">learning_rate </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 0.1</span><span style="color:#F8F8F2;">  </span></span>
<span class="line"><span style="color:#F8F8F2;">num_epochs </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 10</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="color:#F8F8F2;">transform </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> transforms.Compose([transforms.ToTensor()])</span></span>
<span class="line"><span style="color:#F8F8F2;">train_dataset </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> torchvision.datasets.FashionMNIST(</span><span style="color:#FD971F;font-style:italic;">root</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;./data&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">train</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">True</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">transform</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">transform, </span><span style="color:#FD971F;font-style:italic;">download</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">False</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">test_dataset </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> torchvision.datasets.FashionMNIST(</span><span style="color:#FD971F;font-style:italic;">root</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;./data&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">train</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">False</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">transform</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">transform, </span><span style="color:#FD971F;font-style:italic;">download</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">False</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">train_loader </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> torch.utils.data.DataLoader(train_dataset, </span><span style="color:#FD971F;font-style:italic;">batch_size</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">batch_size, </span><span style="color:#FD971F;font-style:italic;">shuffle</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">True</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">test_loader </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> torch.utils.data.DataLoader(test_dataset, </span><span style="color:#FD971F;font-style:italic;">batch_size</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">batch_size, </span><span style="color:#FD971F;font-style:italic;">shuffle</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">False</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="color:#66D9EF;font-style:italic;">class</span><span> </span><span style="color:#A6E22E;text-decoration:underline;">MLP</span><span style="color:#F8F8F2;">(</span><span style="color:#A6E22E;font-style:italic;text-decoration:underline;">nn</span><span style="color:#F8F8F2;">.</span><span style="color:#A6E22E;font-style:italic;text-decoration:underline;">Module</span><span style="color:#F8F8F2;">):</span></span>
<span class="line"><span style="color:#66D9EF;font-style:italic;">    def</span><span style="color:#66D9EF;"> __init__</span><span style="color:#F8F8F2;">(</span><span style="color:#FD971F;font-style:italic;">self</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">num_inputs</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">num_hiddens</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">num_outputs</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">num_layers</span><span style="color:#F8F8F2;">):</span></span>
<span class="line"><span style="color:#66D9EF;font-style:italic;">        super</span><span style="color:#F8F8F2;">(</span><span style="color:#AE81FF;">MLP</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;">self</span><span style="color:#F8F8F2;">).</span><span style="color:#66D9EF;">__init__</span><span style="color:#F8F8F2;">()</span></span>
<span class="line"><span style="color:#F8F8F2;">        layers </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> []</span></span>
<span class="line"><span style="color:#F8F8F2;">        layers.append(nn.Linear(num_inputs, num_hiddens))</span></span>
<span class="line"><span style="color:#F8F8F2;">        layers.append(nn.ReLU())</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F92672;">        for</span><span style="color:#F8F8F2;"> _ </span><span style="color:#F92672;">in</span><span style="color:#66D9EF;"> range</span><span style="color:#F8F8F2;">(num_layers </span><span style="color:#F92672;">-</span><span style="color:#AE81FF;"> 1</span><span style="color:#F8F8F2;">):</span></span>
<span class="line"><span style="color:#F8F8F2;">            layers.append(nn.Linear(num_hiddens, num_hiddens))</span></span>
<span class="line"><span style="color:#F8F8F2;">            layers.append(nn.ReLU())</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F8F8F2;">        layers.append(nn.Linear(num_hiddens, num_outputs))</span></span>
<span class="line"><span style="color:#FD971F;">        self</span><span style="color:#F8F8F2;">.net </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> nn.Sequential(</span><span style="color:#F92672;">*</span><span style="color:#F8F8F2;">layers)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#66D9EF;font-style:italic;">    def</span><span style="color:#A6E22E;"> forward</span><span style="color:#F8F8F2;">(</span><span style="color:#FD971F;font-style:italic;">self</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">X</span><span style="color:#F8F8F2;">):</span></span>
<span class="line"><span style="color:#F92672;">        return</span><span style="color:#FD971F;"> self</span><span style="color:#F8F8F2;">.net(X.view(</span><span style="color:#F92672;">-</span><span style="color:#AE81FF;">1</span><span style="color:#F8F8F2;">, num_inputs))  </span><span style="color:#88846F;"># 展平输入</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="color:#F8F8F2;">net </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> MLP(num_inputs, num_hiddens, num_outputs, num_layers)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F8F8F2;">loss </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> nn.CrossEntropyLoss()</span></span>
<span class="line"><span style="color:#F8F8F2;">optimizer </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> torch.optim.SGD(net.parameters(), </span><span style="color:#FD971F;font-style:italic;">lr</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">learning_rate)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="color:#66D9EF;font-style:italic;">def</span><span style="color:#A6E22E;"> train</span><span style="color:#F8F8F2;">(</span><span style="color:#FD971F;font-style:italic;">net</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">train_loader</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">test_loader</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">loss</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">num_epochs</span><span style="color:#F8F8F2;">):</span></span>
<span class="line"><span style="color:#F8F8F2;">    train_loss, val_loss, val_acc </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> [], [], []</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F92672;">    for</span><span style="color:#F8F8F2;"> epoch </span><span style="color:#F92672;">in</span><span style="color:#66D9EF;"> range</span><span style="color:#F8F8F2;">(num_epochs):</span></span>
<span class="line"><span style="color:#F8F8F2;">        net.train()</span></span>
<span class="line"><span style="color:#F8F8F2;">        total_loss, total_samples </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span></span>
<span class="line"><span style="color:#F92672;">        for</span><span style="color:#F8F8F2;"> X, y </span><span style="color:#F92672;">in</span><span style="color:#F8F8F2;"> train_loader:</span></span>
<span class="line"><span style="color:#F8F8F2;">            y_hat </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> net(X)</span></span>
<span class="line"><span style="color:#F8F8F2;">            l </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> loss(y_hat, y)</span></span>
<span class="line"><span style="color:#F8F8F2;">            optimizer.zero_grad()</span></span>
<span class="line"><span style="color:#F8F8F2;">            l.backward()</span></span>
<span class="line"><span style="color:#F8F8F2;">            optimizer.step()</span></span>
<span class="line"><span style="color:#F8F8F2;">            total_loss </span><span style="color:#F92672;">+=</span><span style="color:#F8F8F2;"> l.item() </span><span style="color:#F92672;">*</span><span style="color:#F8F8F2;"> y.size(</span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">            total_samples </span><span style="color:#F92672;">+=</span><span style="color:#F8F8F2;"> y.size(</span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">        train_loss.append(total_loss </span><span style="color:#F92672;">/</span><span style="color:#F8F8F2;"> total_samples)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F8F8F2;">        net.eval()</span></span>
<span class="line"><span style="color:#F8F8F2;">        correct, total, test_loss </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span></span>
<span class="line"><span style="color:#F92672;">        with</span><span style="color:#F8F8F2;"> torch.no_grad():</span></span>
<span class="line"><span style="color:#F92672;">            for</span><span style="color:#F8F8F2;"> X, y </span><span style="color:#F92672;">in</span><span style="color:#F8F8F2;"> test_loader:</span></span>
<span class="line"><span style="color:#F8F8F2;">                y_hat </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> net(X)</span></span>
<span class="line"><span style="color:#F8F8F2;">                test_loss </span><span style="color:#F92672;">+=</span><span style="color:#F8F8F2;"> loss(y_hat, y).item() </span><span style="color:#F92672;">*</span><span style="color:#F8F8F2;"> y.size(</span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">                correct </span><span style="color:#F92672;">+=</span><span style="color:#F8F8F2;"> (y_hat.argmax(</span><span style="color:#FD971F;font-style:italic;">dim</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">1</span><span style="color:#F8F8F2;">) </span><span style="color:#F92672;">==</span><span style="color:#F8F8F2;"> y).sum().item()</span></span>
<span class="line"><span style="color:#F8F8F2;">                total </span><span style="color:#F92672;">+=</span><span style="color:#F8F8F2;"> y.size(</span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">        val_loss.append(test_loss </span><span style="color:#F92672;">/</span><span style="color:#F8F8F2;"> total)</span></span>
<span class="line"><span style="color:#F8F8F2;">        val_acc.append(correct </span><span style="color:#F92672;">/</span><span style="color:#F8F8F2;"> total)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#66D9EF;">        print</span><span style="color:#F8F8F2;">(</span></span>
<span class="line"><span style="color:#66D9EF;font-style:italic;">            f</span><span style="color:#E6DB74;">&quot;Epoch </span><span style="color:#AE81FF;">{</span><span style="color:#F8F8F2;">epoch </span><span style="color:#F92672;">+</span><span style="color:#AE81FF;"> 1}</span><span style="color:#E6DB74;">: train_loss=</span><span style="color:#AE81FF;">{</span><span style="color:#F8F8F2;">train_loss[</span><span style="color:#F92672;">-</span><span style="color:#AE81FF;">1</span><span style="color:#F8F8F2;">]</span><span style="color:#66D9EF;font-style:italic;">:.4f</span><span style="color:#AE81FF;">}</span><span style="color:#E6DB74;">, val_loss=</span><span style="color:#AE81FF;">{</span><span style="color:#F8F8F2;">val_loss[</span><span style="color:#F92672;">-</span><span style="color:#AE81FF;">1</span><span style="color:#F8F8F2;">]</span><span style="color:#66D9EF;font-style:italic;">:.4f</span><span style="color:#AE81FF;">}</span><span style="color:#E6DB74;">, val_acc=</span><span style="color:#AE81FF;">{</span><span style="color:#F8F8F2;">val_acc[</span><span style="color:#F92672;">-</span><span style="color:#AE81FF;">1</span><span style="color:#F8F8F2;">]</span><span style="color:#66D9EF;font-style:italic;">:.4f</span><span style="color:#AE81FF;">}</span><span style="color:#E6DB74;">&quot;</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="color:#F8F8F2;">    plt.plot(</span><span style="color:#66D9EF;">range</span><span style="color:#F8F8F2;">(</span><span style="color:#AE81FF;">1</span><span style="color:#F8F8F2;">, num_epochs </span><span style="color:#F92672;">+</span><span style="color:#AE81FF;"> 1</span><span style="color:#F8F8F2;">), train_loss, </span><span style="color:#FD971F;font-style:italic;">label</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;train_loss&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">color</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;blue&quot;</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">    plt.plot(</span><span style="color:#66D9EF;">range</span><span style="color:#F8F8F2;">(</span><span style="color:#AE81FF;">1</span><span style="color:#F8F8F2;">, num_epochs </span><span style="color:#F92672;">+</span><span style="color:#AE81FF;"> 1</span><span style="color:#F8F8F2;">), val_loss, </span><span style="color:#FD971F;font-style:italic;">label</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;val_loss&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">color</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;orange&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">linestyle</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;dashed&quot;</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">    plt.plot(</span><span style="color:#66D9EF;">range</span><span style="color:#F8F8F2;">(</span><span style="color:#AE81FF;">1</span><span style="color:#F8F8F2;">, num_epochs </span><span style="color:#F92672;">+</span><span style="color:#AE81FF;"> 1</span><span style="color:#F8F8F2;">), val_acc, </span><span style="color:#FD971F;font-style:italic;">label</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;val_acc&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">color</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;green&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">linestyle</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;dashdot&quot;</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">    plt.xlabel(</span><span style="color:#E6DB74;">&quot;Epoch&quot;</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">    plt.legend()</span></span>
<span class="line"><span style="color:#F8F8F2;">    plt.show()</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="color:#88846F;"># 训练模型</span></span>
<span class="line"><span style="color:#F8F8F2;">train(net, train_loader, test_loader, loss, num_epochs)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div><div class="collapsed-lines"></div></div><h3 id="测试隐藏层神经元个数变化" tabindex="-1"><a class="header-anchor" href="#测试隐藏层神经元个数变化"><span>测试隐藏层神经元个数变化</span></a></h3><blockquote><p>超参数【除了隐藏层神经元个数】：</p></blockquote><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="background-color:#272822;color:#F8F8F2;"><pre class="shiki monokai vp-code"><code><span class="line"><span style="color:#F8F8F2;">num_inputs, num_outputs </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 784</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">10</span><span style="color:#F8F8F2;">  </span></span>
<span class="line"><span style="color:#F8F8F2;">num_layers </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 2</span><span style="color:#F8F8F2;"> </span></span>
<span class="line"><span style="color:#F8F8F2;">learning_rate </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 0.1</span><span style="color:#F8F8F2;">  </span></span>
<span class="line"><span style="color:#F8F8F2;">num_epochs </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 10</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_128" tabindex="-1"><a class="header-anchor" href="#_128"><span>128</span></a></h4><p>隐藏层神经元个数：<mark>128</mark></p><img src="`+o+'" alt="image-20250305105800654" style="zoom:50%;"><ul><li><p>蓝线是训练损失，但随着训练轮次的增加，表明模型在训练集上表现越来越好。</p></li><li><p>黄色虚线是验证损失，刚开始在验证集上的误差越来越小，但在一个点上开始增加，可能是因为过拟合发生了。</p></li><li><p>绿色虚线是在验证集准确率。随着训练轮次的增加，验证准确率通常会逐渐提高，但是模型过拟合时，准确率开始下降。</p></li></ul><h4 id="_64" tabindex="-1"><a class="header-anchor" href="#_64"><span>64</span></a></h4><p>隐藏层神经元个数：<mark>64</mark></p><img src="'+e+'" alt="image-20250305110637969" style="zoom:50%;"><ul><li>可以看到，验证集正确率没有显著的提升。变化最大的是验证损失，过拟合现象不如神经元数为 128 时明显。</li></ul><h4 id="_192" tabindex="-1"><a class="header-anchor" href="#_192"><span>192</span></a></h4><p>隐藏层神经元个数：<mark>192</mark></p><img src="'+t+`" alt="image-20250305114333054" style="zoom:50%;"><ul><li><p><strong>192 神经元的 train_loss 更低</strong>，表明模型拟合训练数据的能力更强。</p></li><li><p><strong>192 神经元的验证损失（橙色虚线）略高于 128 神经元</strong>，并且在训练后期有上升趋势，可能表明过拟合问题。</p></li><li><p><strong>192 神经元的验证准确率提升不明显</strong>，甚至可能略低于 128 神经元的情况。</p><p><strong>可能原因</strong>：过多的神经元容易导致过拟合，即模型在训练集上表现很好，但对新数据（验证集）泛化能力不佳。</p></li></ul><h4 id="_128-vs-64-vs-192" tabindex="-1"><a class="header-anchor" href="#_128-vs-64-vs-192"><span>128 vs 64 vs 192</span></a></h4><p>最推荐 64. 在最优情况差不多的前提下，64 下降得更加稳定，减少了过拟合的风险。可能 128 和 192 最优情况出现比较早。</p><h3 id="测试学习率变化" tabindex="-1"><a class="header-anchor" href="#测试学习率变化"><span>测试学习率变化</span></a></h3><p>设置超参数【除了学习率】：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="background-color:#272822;color:#F8F8F2;"><pre class="shiki monokai vp-code"><code><span class="line"><span style="color:#F8F8F2;">batch_size </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 256</span></span>
<span class="line"><span style="color:#F8F8F2;">num_inputs, num_outputs </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 784</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">10</span><span style="color:#F8F8F2;">  </span></span>
<span class="line"><span style="color:#F8F8F2;">num_hiddens </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 128</span><span style="color:#F8F8F2;">  </span></span>
<span class="line"><span style="color:#F8F8F2;">num_layers </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 2</span><span style="color:#F8F8F2;">  </span></span>
<span class="line"><span style="color:#F8F8F2;">num_epochs </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 10</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_0-1" tabindex="-1"><a class="header-anchor" href="#_0-1"><span>0.1</span></a></h4><img src="`+F+'" alt="image-20250305192419350" style="zoom:50%;"><h4 id="_0-2" tabindex="-1"><a class="header-anchor" href="#_0-2"><span>0.2</span></a></h4><img src="'+c+'" alt="image-20250305192454066" style="zoom:50%;"><h4 id="_0-1-vs-0-2" tabindex="-1"><a class="header-anchor" href="#_0-1-vs-0-2"><span>0.1 vs 0.2</span></a></h4><p>对比两个不同学习率的训练结果：</p><ul><li>train_loss：0.1 和 0.2 都在稳定下降，0.2 收敛较为快。</li><li>val_loss：0.1 和 0.2 都呈现下降的趋势，但是 0.2 后期波动较大，表明高学习率可能导致模型在验证集上不够稳定，甚至可能影响泛化能力。</li><li>val_acc： <ul><li>学习率 0.1（上图） 的验证准确率略微上升，最终趋于稳定。</li><li>学习率 0.2（下图） 虽然初期上升较快，但后期波动更大，甚至可能出现下降的趋势。</li></ul></li><li>以上说明，0.2 波动较大，0.1 下降较慢，我们折中试试：</li></ul><h4 id="_0-15" tabindex="-1"><a class="header-anchor" href="#_0-15"><span>0.15</span></a></h4><img src="'+r+`" alt="image-20250305193222677" style="zoom:50%;"><h4 id="_0-1-vs-0-15-vs-0-2" tabindex="-1"><a class="header-anchor" href="#_0-1-vs-0-15-vs-0-2"><span>0.1 vs 0.15 vs 0.2</span></a></h4><p>对比两个不同学习率的训练结果：</p><ul><li>train_loss：0.1 和 0.15 和 0.2 都在稳定下降，0.2 收敛最快。</li><li>val_loss：0.1 和 0.15 和 0.2 都呈现下降的趋势，但是 0.15 在最优点震动比较大，0.2 震动最大，0.1 最平稳，表明高学习率可能导致模型在验证集上不够稳定，甚至可能影响泛化能力。</li><li>val_acc：0.1 和 0.15 都在上升并且比较平稳，0.2 相对不平稳。</li></ul><p>总结下来最推荐 0.1，因为最优情况差不多，0.1 较为稳定。</p><h3 id="测试隐藏层层数" tabindex="-1"><a class="header-anchor" href="#测试隐藏层层数"><span>测试隐藏层层数</span></a></h3><p>设置超参数：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="background-color:#272822;color:#F8F8F2;"><pre class="shiki monokai vp-code"><code><span class="line"><span style="color:#F8F8F2;">batch_size </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 256</span></span>
<span class="line"><span style="color:#F8F8F2;">num_inputs, num_outputs </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 784</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">10</span><span style="color:#F8F8F2;">  </span></span>
<span class="line"><span style="color:#F8F8F2;">num_hiddens </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 128</span><span style="color:#F8F8F2;">  </span></span>
<span class="line"><span style="color:#F8F8F2;">learning_rate </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 0.1</span><span style="color:#F8F8F2;">  </span></span>
<span class="line"><span style="color:#F8F8F2;">num_epochs </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 10</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_2-层隐藏层" tabindex="-1"><a class="header-anchor" href="#_2-层隐藏层"><span>2 层隐藏层</span></a></h4><img src="`+i+'" alt="image-20250305201001514" style="zoom:50%;"><h4 id="_3-层隐藏层" tabindex="-1"><a class="header-anchor" href="#_3-层隐藏层"><span>3 层隐藏层</span></a></h4><img src="'+y+'" alt="image-20250305200248238" style="zoom:50%;"><h4 id="_2-vs-3" tabindex="-1"><a class="header-anchor" href="#_2-vs-3"><span>2 vs 3</span></a></h4><ul><li><p>训练损失（train_loss）：2,3 层都在下降，3 层训练损失下降更快，说明更深的网络可以更好地拟合数据。</p></li><li><p>验证损失（val_loss）：3 层隐藏层 训练后期，验证损失比 2 层高，出现了一定程度的上升，可能有<strong>过拟合</strong>现象。</p></li><li><p>验证准确率（val_acc）：3 层隐藏层早期增长较快，但后期波动较大，表明增加隐藏层可能会导致模型不稳定。</p></li></ul><h4 id="_1-层-隐藏层" tabindex="-1"><a class="header-anchor" href="#_1-层-隐藏层"><span>1 层 隐藏层</span></a></h4><img src="'+d+'" alt="image-20250305202109776" style="zoom:50%;"><h4 id="_1-vs-2-vs-3" tabindex="-1"><a class="header-anchor" href="#_1-vs-2-vs-3"><span>1 vs 2 vs 3</span></a></h4><table><thead><tr><th><strong>隐藏层数</strong></th><th><strong>训练损失 (train_loss)</strong></th><th><strong>验证损失 (val_loss)</strong></th><th><strong>验证准确率 (val_acc)</strong></th><th><strong>表现分析</strong></th></tr></thead><tbody><tr><td><strong>1 层</strong></td><td><strong>下降较慢</strong>，但持续下降</td><td><strong>较稳定，但下降有限</strong></td><td><strong>最高</strong>，约 85% 左右</td><td><strong>泛化能力较好，学习能力不足</strong></td></tr><tr><td><strong>2 层</strong></td><td>下降较快</td><td><strong>比 1 层下降更快</strong></td><td><strong>较高，80% 左右</strong></td><td><strong>学习能力更强，仍有较好泛化能力</strong></td></tr><tr><td><strong>3 层</strong></td><td>下降最快</td><td><strong>后期验证损失有上升趋势</strong></td><td><strong>波动较大，接近 78%-80%</strong></td><td><strong>可能过拟合，泛化能力下降</strong></td></tr></tbody></table><p>因此，推荐 1 层。</p>',49)]))}const b=n(v,[["render",m]]),E=JSON.parse('{"path":"/zh/DeepThinking/mlp_exe.html","title":"实现多层感知机","lang":"zh-CN","frontmatter":{"title":"实现多层感知机","icon":"alias","date":"2025-03-05T10:58:18.000Z","author":"XiaoXianYue","isOriginal":true,"category":["大三下","神经网络与深度学习"],"tag":["大三下","神经网络与深度学习"],"sticky":false,"star":false,"article":true,"timeline":true,"image":false,"navbar":true,"sidebarIcon":true,"headerDepth":5,"lastUpdated":true,"editLink":false,"backToTop":true,"toc":true,"description":"代码 测试隐藏层神经元个数变化 超参数【除了隐藏层神经元个数】： 128 隐藏层神经元个数：128 image-20250305105800654 蓝线是训练损失，但随着训练轮次的增加，表明模型在训练集上表现越来越好。 黄色虚线是验证损失，刚开始在验证集上的误差越来越小，但在一个点上开始增加，可能是因为过拟合发生了。 绿色虚线是在验证集准确率。随着训练...","head":[["meta",{"property":"og:url","content":"https://bougiemoonintaurus/zh/DeepThinking/mlp_exe.html"}],["meta",{"property":"og:site_name","content":"奶酪奶酪"}],["meta",{"property":"og:title","content":"实现多层感知机"}],["meta",{"property":"og:description","content":"代码 测试隐藏层神经元个数变化 超参数【除了隐藏层神经元个数】： 128 隐藏层神经元个数：128 image-20250305105800654 蓝线是训练损失，但随着训练轮次的增加，表明模型在训练集上表现越来越好。 黄色虚线是验证损失，刚开始在验证集上的误差越来越小，但在一个点上开始增加，可能是因为过拟合发生了。 绿色虚线是在验证集准确率。随着训练..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-03-19T17:01:16.000Z"}],["meta",{"property":"article:author","content":"XiaoXianYue"}],["meta",{"property":"article:tag","content":"大三下"}],["meta",{"property":"article:tag","content":"神经网络与深度学习"}],["meta",{"property":"article:published_time","content":"2025-03-05T10:58:18.000Z"}],["meta",{"property":"article:modified_time","content":"2025-03-19T17:01:16.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"实现多层感知机\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-03-05T10:58:18.000Z\\",\\"dateModified\\":\\"2025-03-19T17:01:16.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"XiaoXianYue\\"}]}"]]},"git":{"createdTime":1742403676000,"updatedTime":1742403676000,"contributors":[{"name":"Xiaoxianyue","username":"Xiaoxianyue","email":"2310219843@qq.com","commits":1,"url":"https://github.com/Xiaoxianyue"}]},"readingTime":{"minutes":5.07,"words":1520},"filePathRelative":"zh/DeepThinking/mlp_exe.md","localizedDate":"2025年3月5日","excerpt":"<h3>代码</h3>\\n<div class=\\"language-python line-numbers-mode has-collapsed-lines collapsed\\" data-highlighter=\\"shiki\\" data-ext=\\"python\\" style=\\"--vp-collapsed-lines:15;background-color:#272822;color:#F8F8F2\\"><pre class=\\"shiki monokai vp-code\\"><code><span class=\\"line\\"><span style=\\"color:#F92672\\">import</span><span style=\\"color:#F8F8F2\\"> torch</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F92672\\">from</span><span style=\\"color:#F8F8F2\\"> torch </span><span style=\\"color:#F92672\\">import</span><span style=\\"color:#F8F8F2\\"> nn</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F92672\\">import</span><span style=\\"color:#F8F8F2\\"> torchvision</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F92672\\">import</span><span style=\\"color:#F8F8F2\\"> torchvision.transforms </span><span style=\\"color:#F92672\\">as</span><span style=\\"color:#F8F8F2\\"> transforms</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F92672\\">import</span><span style=\\"color:#F8F8F2\\"> matplotlib.pyplot </span><span style=\\"color:#F92672\\">as</span><span style=\\"color:#F8F8F2\\"> plt</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">batch_size </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\"> 256</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">num_inputs, num_outputs </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\"> 784</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#AE81FF\\">10</span><span style=\\"color:#F8F8F2\\">  </span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">num_hiddens </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\"> 128</span><span style=\\"color:#F8F8F2\\">  </span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">num_layers </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\"> 2</span><span style=\\"color:#F8F8F2\\"> </span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">learning_rate </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\"> 0.1</span><span style=\\"color:#F8F8F2\\">  </span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">num_epochs </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\"> 10</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">transform </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> transforms.Compose([transforms.ToTensor()])</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">train_dataset </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> torchvision.datasets.FashionMNIST(</span><span style=\\"color:#FD971F;font-style:italic\\">root</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#E6DB74\\">\\"./data\\"</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">train</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\">True</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">transform</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\">transform, </span><span style=\\"color:#FD971F;font-style:italic\\">download</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\">False</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">test_dataset </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> torchvision.datasets.FashionMNIST(</span><span style=\\"color:#FD971F;font-style:italic\\">root</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#E6DB74\\">\\"./data\\"</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">train</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\">False</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">transform</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\">transform, </span><span style=\\"color:#FD971F;font-style:italic\\">download</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\">False</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">train_loader </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> torch.utils.data.DataLoader(train_dataset, </span><span style=\\"color:#FD971F;font-style:italic\\">batch_size</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\">batch_size, </span><span style=\\"color:#FD971F;font-style:italic\\">shuffle</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\">True</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">test_loader </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> torch.utils.data.DataLoader(test_dataset, </span><span style=\\"color:#FD971F;font-style:italic\\">batch_size</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\">batch_size, </span><span style=\\"color:#FD971F;font-style:italic\\">shuffle</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\">False</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#66D9EF;font-style:italic\\">class</span><span> </span><span style=\\"color:#A6E22E;text-decoration:underline\\">MLP</span><span style=\\"color:#F8F8F2\\">(</span><span style=\\"color:#A6E22E;font-style:italic;text-decoration:underline\\">nn</span><span style=\\"color:#F8F8F2\\">.</span><span style=\\"color:#A6E22E;font-style:italic;text-decoration:underline\\">Module</span><span style=\\"color:#F8F8F2\\">):</span></span>\\n<span class=\\"line\\"><span style=\\"color:#66D9EF;font-style:italic\\">    def</span><span style=\\"color:#66D9EF\\"> __init__</span><span style=\\"color:#F8F8F2\\">(</span><span style=\\"color:#FD971F;font-style:italic\\">self</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">num_inputs</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">num_hiddens</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">num_outputs</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">num_layers</span><span style=\\"color:#F8F8F2\\">):</span></span>\\n<span class=\\"line\\"><span style=\\"color:#66D9EF;font-style:italic\\">        super</span><span style=\\"color:#F8F8F2\\">(</span><span style=\\"color:#AE81FF\\">MLP</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F\\">self</span><span style=\\"color:#F8F8F2\\">).</span><span style=\\"color:#66D9EF\\">__init__</span><span style=\\"color:#F8F8F2\\">()</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">        layers </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> []</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">        layers.append(nn.Linear(num_inputs, num_hiddens))</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">        layers.append(nn.ReLU())</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#F92672\\">        for</span><span style=\\"color:#F8F8F2\\"> _ </span><span style=\\"color:#F92672\\">in</span><span style=\\"color:#66D9EF\\"> range</span><span style=\\"color:#F8F8F2\\">(num_layers </span><span style=\\"color:#F92672\\">-</span><span style=\\"color:#AE81FF\\"> 1</span><span style=\\"color:#F8F8F2\\">):</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">            layers.append(nn.Linear(num_hiddens, num_hiddens))</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">            layers.append(nn.ReLU())</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">        layers.append(nn.Linear(num_hiddens, num_outputs))</span></span>\\n<span class=\\"line\\"><span style=\\"color:#FD971F\\">        self</span><span style=\\"color:#F8F8F2\\">.net </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> nn.Sequential(</span><span style=\\"color:#F92672\\">*</span><span style=\\"color:#F8F8F2\\">layers)</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#66D9EF;font-style:italic\\">    def</span><span style=\\"color:#A6E22E\\"> forward</span><span style=\\"color:#F8F8F2\\">(</span><span style=\\"color:#FD971F;font-style:italic\\">self</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">X</span><span style=\\"color:#F8F8F2\\">):</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F92672\\">        return</span><span style=\\"color:#FD971F\\"> self</span><span style=\\"color:#F8F8F2\\">.net(X.view(</span><span style=\\"color:#F92672\\">-</span><span style=\\"color:#AE81FF\\">1</span><span style=\\"color:#F8F8F2\\">, num_inputs))  </span><span style=\\"color:#88846F\\"># 展平输入</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">net </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> MLP(num_inputs, num_hiddens, num_outputs, num_layers)</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">loss </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> nn.CrossEntropyLoss()</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">optimizer </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> torch.optim.SGD(net.parameters(), </span><span style=\\"color:#FD971F;font-style:italic\\">lr</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\">learning_rate)</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#66D9EF;font-style:italic\\">def</span><span style=\\"color:#A6E22E\\"> train</span><span style=\\"color:#F8F8F2\\">(</span><span style=\\"color:#FD971F;font-style:italic\\">net</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">train_loader</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">test_loader</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">loss</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">num_epochs</span><span style=\\"color:#F8F8F2\\">):</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">    train_loss, val_loss, val_acc </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> [], [], []</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#F92672\\">    for</span><span style=\\"color:#F8F8F2\\"> epoch </span><span style=\\"color:#F92672\\">in</span><span style=\\"color:#66D9EF\\"> range</span><span style=\\"color:#F8F8F2\\">(num_epochs):</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">        net.train()</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">        total_loss, total_samples </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\"> 0</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#AE81FF\\">0</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F92672\\">        for</span><span style=\\"color:#F8F8F2\\"> X, y </span><span style=\\"color:#F92672\\">in</span><span style=\\"color:#F8F8F2\\"> train_loader:</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">            y_hat </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> net(X)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">            l </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> loss(y_hat, y)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">            optimizer.zero_grad()</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">            l.backward()</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">            optimizer.step()</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">            total_loss </span><span style=\\"color:#F92672\\">+=</span><span style=\\"color:#F8F8F2\\"> l.item() </span><span style=\\"color:#F92672\\">*</span><span style=\\"color:#F8F8F2\\"> y.size(</span><span style=\\"color:#AE81FF\\">0</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">            total_samples </span><span style=\\"color:#F92672\\">+=</span><span style=\\"color:#F8F8F2\\"> y.size(</span><span style=\\"color:#AE81FF\\">0</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">        train_loss.append(total_loss </span><span style=\\"color:#F92672\\">/</span><span style=\\"color:#F8F8F2\\"> total_samples)</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">        net.eval()</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">        correct, total, test_loss </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\"> 0</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#AE81FF\\">0</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#AE81FF\\">0</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F92672\\">        with</span><span style=\\"color:#F8F8F2\\"> torch.no_grad():</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F92672\\">            for</span><span style=\\"color:#F8F8F2\\"> X, y </span><span style=\\"color:#F92672\\">in</span><span style=\\"color:#F8F8F2\\"> test_loader:</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">                y_hat </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> net(X)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">                test_loss </span><span style=\\"color:#F92672\\">+=</span><span style=\\"color:#F8F8F2\\"> loss(y_hat, y).item() </span><span style=\\"color:#F92672\\">*</span><span style=\\"color:#F8F8F2\\"> y.size(</span><span style=\\"color:#AE81FF\\">0</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">                correct </span><span style=\\"color:#F92672\\">+=</span><span style=\\"color:#F8F8F2\\"> (y_hat.argmax(</span><span style=\\"color:#FD971F;font-style:italic\\">dim</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\">1</span><span style=\\"color:#F8F8F2\\">) </span><span style=\\"color:#F92672\\">==</span><span style=\\"color:#F8F8F2\\"> y).sum().item()</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">                total </span><span style=\\"color:#F92672\\">+=</span><span style=\\"color:#F8F8F2\\"> y.size(</span><span style=\\"color:#AE81FF\\">0</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">        val_loss.append(test_loss </span><span style=\\"color:#F92672\\">/</span><span style=\\"color:#F8F8F2\\"> total)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">        val_acc.append(correct </span><span style=\\"color:#F92672\\">/</span><span style=\\"color:#F8F8F2\\"> total)</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#66D9EF\\">        print</span><span style=\\"color:#F8F8F2\\">(</span></span>\\n<span class=\\"line\\"><span style=\\"color:#66D9EF;font-style:italic\\">            f</span><span style=\\"color:#E6DB74\\">\\"Epoch </span><span style=\\"color:#AE81FF\\">{</span><span style=\\"color:#F8F8F2\\">epoch </span><span style=\\"color:#F92672\\">+</span><span style=\\"color:#AE81FF\\"> 1}</span><span style=\\"color:#E6DB74\\">: train_loss=</span><span style=\\"color:#AE81FF\\">{</span><span style=\\"color:#F8F8F2\\">train_loss[</span><span style=\\"color:#F92672\\">-</span><span style=\\"color:#AE81FF\\">1</span><span style=\\"color:#F8F8F2\\">]</span><span style=\\"color:#66D9EF;font-style:italic\\">:.4f</span><span style=\\"color:#AE81FF\\">}</span><span style=\\"color:#E6DB74\\">, val_loss=</span><span style=\\"color:#AE81FF\\">{</span><span style=\\"color:#F8F8F2\\">val_loss[</span><span style=\\"color:#F92672\\">-</span><span style=\\"color:#AE81FF\\">1</span><span style=\\"color:#F8F8F2\\">]</span><span style=\\"color:#66D9EF;font-style:italic\\">:.4f</span><span style=\\"color:#AE81FF\\">}</span><span style=\\"color:#E6DB74\\">, val_acc=</span><span style=\\"color:#AE81FF\\">{</span><span style=\\"color:#F8F8F2\\">val_acc[</span><span style=\\"color:#F92672\\">-</span><span style=\\"color:#AE81FF\\">1</span><span style=\\"color:#F8F8F2\\">]</span><span style=\\"color:#66D9EF;font-style:italic\\">:.4f</span><span style=\\"color:#AE81FF\\">}</span><span style=\\"color:#E6DB74\\">\\"</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">    plt.plot(</span><span style=\\"color:#66D9EF\\">range</span><span style=\\"color:#F8F8F2\\">(</span><span style=\\"color:#AE81FF\\">1</span><span style=\\"color:#F8F8F2\\">, num_epochs </span><span style=\\"color:#F92672\\">+</span><span style=\\"color:#AE81FF\\"> 1</span><span style=\\"color:#F8F8F2\\">), train_loss, </span><span style=\\"color:#FD971F;font-style:italic\\">label</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#E6DB74\\">\\"train_loss\\"</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">color</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#E6DB74\\">\\"blue\\"</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">    plt.plot(</span><span style=\\"color:#66D9EF\\">range</span><span style=\\"color:#F8F8F2\\">(</span><span style=\\"color:#AE81FF\\">1</span><span style=\\"color:#F8F8F2\\">, num_epochs </span><span style=\\"color:#F92672\\">+</span><span style=\\"color:#AE81FF\\"> 1</span><span style=\\"color:#F8F8F2\\">), val_loss, </span><span style=\\"color:#FD971F;font-style:italic\\">label</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#E6DB74\\">\\"val_loss\\"</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">color</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#E6DB74\\">\\"orange\\"</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">linestyle</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#E6DB74\\">\\"dashed\\"</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">    plt.plot(</span><span style=\\"color:#66D9EF\\">range</span><span style=\\"color:#F8F8F2\\">(</span><span style=\\"color:#AE81FF\\">1</span><span style=\\"color:#F8F8F2\\">, num_epochs </span><span style=\\"color:#F92672\\">+</span><span style=\\"color:#AE81FF\\"> 1</span><span style=\\"color:#F8F8F2\\">), val_acc, </span><span style=\\"color:#FD971F;font-style:italic\\">label</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#E6DB74\\">\\"val_acc\\"</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">color</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#E6DB74\\">\\"green\\"</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">linestyle</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#E6DB74\\">\\"dashdot\\"</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">    plt.xlabel(</span><span style=\\"color:#E6DB74\\">\\"Epoch\\"</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">    plt.legend()</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">    plt.show()</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#88846F\\"># 训练模型</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">train(net, train_loader, test_loader, loss, num_epochs)</span></span></code></pre>\\n<div class=\\"line-numbers\\" aria-hidden=\\"true\\" style=\\"counter-reset:line-number 0\\"><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div></div><div class=\\"collapsed-lines\\"></div></div>","autoDesc":true}');export{b as comp,E as data};
