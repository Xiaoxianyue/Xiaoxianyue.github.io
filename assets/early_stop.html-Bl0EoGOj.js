import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as a,b as l,o as p}from"./app-Cr94KhCm.js";const o="/assets/image-20250311115711863-Dr7yTnBJ.png",e="/assets/image-20250311213036142-DqYGgWaZ.png",t="/assets/image-20250311215618677-C-xy6oid.png",F={};function c(r,s){return p(),a("div",null,s[0]||(s[0]=[l(`<h2 id="task-01" tabindex="-1"><a class="header-anchor" href="#task-01"><span>Task 01</span></a></h2><blockquote><p>According to the referenced code related to weight decay, plot the functions of the training loss and the test loss with respect to λ</p></blockquote><h3 id="代码" tabindex="-1"><a class="header-anchor" href="#代码"><span>代码</span></a></h3><div class="language-python line-numbers-mode has-collapsed-lines collapsed" data-highlighter="shiki" data-ext="python" style="--vp-collapsed-lines:15;background-color:#272822;color:#F8F8F2;"><pre class="shiki monokai vp-code"><code><span class="line"><span style="color:#F92672;">import</span><span style="color:#F8F8F2;"> torch</span></span>
<span class="line"><span style="color:#F92672;">from</span><span style="color:#F8F8F2;"> torch </span><span style="color:#F92672;">import</span><span style="color:#F8F8F2;"> nn</span></span>
<span class="line"><span style="color:#F92672;">import</span><span style="color:#F8F8F2;"> torchvision</span></span>
<span class="line"><span style="color:#F92672;">import</span><span style="color:#F8F8F2;"> torchvision.transforms </span><span style="color:#F92672;">as</span><span style="color:#F8F8F2;"> transforms</span></span>
<span class="line"><span style="color:#F92672;">import</span><span style="color:#F8F8F2;"> matplotlib.pyplot </span><span style="color:#F92672;">as</span><span style="color:#F8F8F2;"> plt</span></span>
<span class="line"></span>
<span class="line"><span style="color:#88846F;"># 设定超参数</span></span>
<span class="line"><span style="color:#F8F8F2;">batch_size </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 256</span></span>
<span class="line"><span style="color:#F8F8F2;">num_inputs, num_outputs </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 784</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">10</span></span>
<span class="line"><span style="color:#F8F8F2;">num_hiddens </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 128</span></span>
<span class="line"><span style="color:#F8F8F2;">num_layers </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 2</span></span>
<span class="line"><span style="color:#F8F8F2;">learning_rate </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 0.1</span></span>
<span class="line"><span style="color:#F8F8F2;">num_epochs </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 10</span></span>
<span class="line"><span style="color:#F8F8F2;">lambda_values </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> [</span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0.0001</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0.001</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0.01</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0.1</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">1</span><span style="color:#F8F8F2;">]  </span><span style="color:#88846F;"># 不同的权重衰减系数 λ</span></span>
<span class="line"></span>
<span class="line"><span style="color:#88846F;"># 预处理 FashionMNIST 数据集</span></span>
<span class="line"><span style="color:#F8F8F2;">transform </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> transforms.Compose([transforms.ToTensor()])</span></span>
<span class="line"><span style="color:#F8F8F2;">train_dataset </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> torchvision.datasets.FashionMNIST(</span><span style="color:#FD971F;font-style:italic;">root</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;./data&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">train</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">True</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">transform</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">transform, </span><span style="color:#FD971F;font-style:italic;">download</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">True</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">test_dataset </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> torchvision.datasets.FashionMNIST(</span><span style="color:#FD971F;font-style:italic;">root</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;./data&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">train</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">False</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">transform</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">transform, </span><span style="color:#FD971F;font-style:italic;">download</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">True</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">train_loader </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> torch.utils.data.DataLoader(train_dataset, </span><span style="color:#FD971F;font-style:italic;">batch_size</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">batch_size, </span><span style="color:#FD971F;font-style:italic;">shuffle</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">True</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">test_loader </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> torch.utils.data.DataLoader(test_dataset, </span><span style="color:#FD971F;font-style:italic;">batch_size</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">batch_size, </span><span style="color:#FD971F;font-style:italic;">shuffle</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">False</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#88846F;"># 定义 MLP 网络</span></span>
<span class="line"><span style="color:#66D9EF;font-style:italic;">class</span><span> </span><span style="color:#A6E22E;text-decoration:underline;">MLP</span><span style="color:#F8F8F2;">(</span><span style="color:#A6E22E;font-style:italic;text-decoration:underline;">nn</span><span style="color:#F8F8F2;">.</span><span style="color:#A6E22E;font-style:italic;text-decoration:underline;">Module</span><span style="color:#F8F8F2;">):</span></span>
<span class="line"><span style="color:#66D9EF;font-style:italic;">    def</span><span style="color:#66D9EF;"> __init__</span><span style="color:#F8F8F2;">(</span><span style="color:#FD971F;font-style:italic;">self</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">num_inputs</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">num_hiddens</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">num_outputs</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">num_layers</span><span style="color:#F8F8F2;">):</span></span>
<span class="line"><span style="color:#66D9EF;font-style:italic;">        super</span><span style="color:#F8F8F2;">(</span><span style="color:#AE81FF;">MLP</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;">self</span><span style="color:#F8F8F2;">).</span><span style="color:#66D9EF;">__init__</span><span style="color:#F8F8F2;">()</span></span>
<span class="line"><span style="color:#F8F8F2;">        layers </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> []</span></span>
<span class="line"><span style="color:#F8F8F2;">        layers.append(nn.Linear(num_inputs, num_hiddens))</span></span>
<span class="line"><span style="color:#F8F8F2;">        layers.append(nn.ReLU())</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F92672;">        for</span><span style="color:#F8F8F2;"> _ </span><span style="color:#F92672;">in</span><span style="color:#66D9EF;"> range</span><span style="color:#F8F8F2;">(num_layers </span><span style="color:#F92672;">-</span><span style="color:#AE81FF;"> 1</span><span style="color:#F8F8F2;">):</span></span>
<span class="line"><span style="color:#F8F8F2;">            layers.append(nn.Linear(num_hiddens, num_hiddens))</span></span>
<span class="line"><span style="color:#F8F8F2;">            layers.append(nn.ReLU())</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F8F8F2;">        layers.append(nn.Linear(num_hiddens, num_outputs))</span></span>
<span class="line"><span style="color:#FD971F;">        self</span><span style="color:#F8F8F2;">.net </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> nn.Sequential(</span><span style="color:#F92672;">*</span><span style="color:#F8F8F2;">layers)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#66D9EF;font-style:italic;">    def</span><span style="color:#A6E22E;"> forward</span><span style="color:#F8F8F2;">(</span><span style="color:#FD971F;font-style:italic;">self</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">X</span><span style="color:#F8F8F2;">):</span></span>
<span class="line"><span style="color:#F92672;">        return</span><span style="color:#FD971F;"> self</span><span style="color:#F8F8F2;">.net(X.view(</span><span style="color:#F92672;">-</span><span style="color:#AE81FF;">1</span><span style="color:#F8F8F2;">, num_inputs))  </span><span style="color:#88846F;"># 展平输入</span></span>
<span class="line"></span>
<span class="line"><span style="color:#88846F;"># 训练函数，接收不同的 weight_decay 参数</span></span>
<span class="line"><span style="color:#66D9EF;font-style:italic;">def</span><span style="color:#A6E22E;"> train_with_weight_decay</span><span style="color:#F8F8F2;">(</span><span style="color:#FD971F;font-style:italic;">lambda_values</span><span style="color:#F8F8F2;">):</span></span>
<span class="line"><span style="color:#F8F8F2;">    train_losses, val_losses </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> [], []</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F92672;">    for</span><span style="color:#F8F8F2;"> weight_decay </span><span style="color:#F92672;">in</span><span style="color:#F8F8F2;"> lambda_values:</span></span>
<span class="line"><span style="color:#F8F8F2;">        net </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> MLP(num_inputs, num_hiddens, num_outputs, num_layers)</span></span>
<span class="line"><span style="color:#F8F8F2;">        loss </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> nn.CrossEntropyLoss()</span></span>
<span class="line"><span style="color:#F8F8F2;">        optimizer </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> torch.optim.SGD(net.parameters(), </span><span style="color:#FD971F;font-style:italic;">lr</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">learning_rate, </span><span style="color:#FD971F;font-style:italic;">weight_decay</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">weight_decay)  </span><span style="color:#88846F;"># 添加权重衰减</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F8F8F2;">        train_loss, val_loss </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> [], []</span></span>
<span class="line"><span style="color:#F92672;">        for</span><span style="color:#F8F8F2;"> epoch </span><span style="color:#F92672;">in</span><span style="color:#66D9EF;"> range</span><span style="color:#F8F8F2;">(num_epochs):</span></span>
<span class="line"><span style="color:#F8F8F2;">            net.train()</span></span>
<span class="line"><span style="color:#F8F8F2;">            total_loss, total_samples </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span></span>
<span class="line"><span style="color:#F92672;">            for</span><span style="color:#F8F8F2;"> X, y </span><span style="color:#F92672;">in</span><span style="color:#F8F8F2;"> train_loader:</span></span>
<span class="line"><span style="color:#F8F8F2;">                y_hat </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> net(X)</span></span>
<span class="line"><span style="color:#F8F8F2;">                l </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> loss(y_hat, y)</span></span>
<span class="line"><span style="color:#F8F8F2;">                optimizer.zero_grad()</span></span>
<span class="line"><span style="color:#F8F8F2;">                l.backward()</span></span>
<span class="line"><span style="color:#F8F8F2;">                optimizer.step()</span></span>
<span class="line"><span style="color:#F8F8F2;">                total_loss </span><span style="color:#F92672;">+=</span><span style="color:#F8F8F2;"> l.item() </span><span style="color:#F92672;">*</span><span style="color:#F8F8F2;"> y.size(</span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">                total_samples </span><span style="color:#F92672;">+=</span><span style="color:#F8F8F2;"> y.size(</span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">            train_loss.append(total_loss </span><span style="color:#F92672;">/</span><span style="color:#F8F8F2;"> total_samples)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F8F8F2;">            net.eval()</span></span>
<span class="line"><span style="color:#F8F8F2;">            total, test_loss </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span></span>
<span class="line"><span style="color:#F92672;">            with</span><span style="color:#F8F8F2;"> torch.no_grad():</span></span>
<span class="line"><span style="color:#F92672;">                for</span><span style="color:#F8F8F2;"> X, y </span><span style="color:#F92672;">in</span><span style="color:#F8F8F2;"> test_loader:</span></span>
<span class="line"><span style="color:#F8F8F2;">                    y_hat </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> net(X)</span></span>
<span class="line"><span style="color:#F8F8F2;">                    test_loss </span><span style="color:#F92672;">+=</span><span style="color:#F8F8F2;"> loss(y_hat, y).item() </span><span style="color:#F92672;">*</span><span style="color:#F8F8F2;"> y.size(</span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">                    total </span><span style="color:#F92672;">+=</span><span style="color:#F8F8F2;"> y.size(</span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">            val_loss.append(test_loss </span><span style="color:#F92672;">/</span><span style="color:#F8F8F2;"> total)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F8F8F2;">        train_losses.append(train_loss[</span><span style="color:#F92672;">-</span><span style="color:#AE81FF;">1</span><span style="color:#F8F8F2;">])</span></span>
<span class="line"><span style="color:#F8F8F2;">        val_losses.append(val_loss[</span><span style="color:#F92672;">-</span><span style="color:#AE81FF;">1</span><span style="color:#F8F8F2;">])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#66D9EF;">        print</span><span style="color:#F8F8F2;">(</span><span style="color:#66D9EF;font-style:italic;">f</span><span style="color:#E6DB74;">&quot;λ=</span><span style="color:#AE81FF;">{</span><span style="color:#F8F8F2;">weight_decay</span><span style="color:#66D9EF;font-style:italic;">:.5f</span><span style="color:#AE81FF;">}</span><span style="color:#E6DB74;">, Final Train Loss=</span><span style="color:#AE81FF;">{</span><span style="color:#F8F8F2;">train_loss[</span><span style="color:#F92672;">-</span><span style="color:#AE81FF;">1</span><span style="color:#F8F8F2;">]</span><span style="color:#66D9EF;font-style:italic;">:.4f</span><span style="color:#AE81FF;">}</span><span style="color:#E6DB74;">, Final Val Loss=</span><span style="color:#AE81FF;">{</span><span style="color:#F8F8F2;">val_loss[</span><span style="color:#F92672;">-</span><span style="color:#AE81FF;">1</span><span style="color:#F8F8F2;">]</span><span style="color:#66D9EF;font-style:italic;">:.4f</span><span style="color:#AE81FF;">}</span><span style="color:#E6DB74;">&quot;</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F92672;">    return</span><span style="color:#F8F8F2;"> train_losses, val_losses</span></span>
<span class="line"></span>
<span class="line"><span style="color:#88846F;"># 训练并记录不同 λ 对损失的影响</span></span>
<span class="line"><span style="color:#F8F8F2;">train_losses, val_losses </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> train_with_weight_decay(lambda_values)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#88846F;"># 绘制 λ vs 训练损失 / 测试损失曲线</span></span>
<span class="line"><span style="color:#F8F8F2;">plt.figure(</span><span style="color:#FD971F;font-style:italic;">figsize</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">(</span><span style="color:#AE81FF;">8</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">5</span><span style="color:#F8F8F2;">))</span></span>
<span class="line"><span style="color:#F8F8F2;">plt.plot(lambda_values, train_losses, </span><span style="color:#FD971F;font-style:italic;">marker</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;o&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">label</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;Train Loss&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">color</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;blue&quot;</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">plt.plot(lambda_values, val_losses, </span><span style="color:#FD971F;font-style:italic;">marker</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;s&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">label</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;Test Loss&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">color</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;orange&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">linestyle</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;dashed&quot;</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">plt.xlabel(</span><span style="color:#E6DB74;">&quot;Weight Decay (λ)&quot;</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">plt.ylabel(</span><span style="color:#E6DB74;">&quot;Loss&quot;</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">plt.xscale(</span><span style="color:#E6DB74;">&quot;log&quot;</span><span style="color:#F8F8F2;">)  </span><span style="color:#88846F;"># 采用对数刻度更清晰</span></span>
<span class="line"><span style="color:#F8F8F2;">plt.legend()</span></span>
<span class="line"><span style="color:#F8F8F2;">plt.title(</span><span style="color:#E6DB74;">&quot;Effect of Weight Decay (λ) on Loss&quot;</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">plt.show()</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div><div class="collapsed-lines"></div></div><h3 id="运行结果" tabindex="-1"><a class="header-anchor" href="#运行结果"><span>运行结果</span></a></h3><figure><img src="`+o+`" alt="image-20250311115711863" tabindex="0" loading="lazy"><figcaption>image-20250311115711863</figcaption></figure><h3 id="分析" tabindex="-1"><a class="header-anchor" href="#分析"><span>分析</span></a></h3><ol><li><strong>训练损失（Train Loss）</strong>： <ul><li>随着 λ 的增加，训练损失逐渐上升。这是因为权重衰减通过惩罚较大的权重来限制模型的复杂度，从而可能导致模型在训练数据上的拟合能力下降。</li><li>当 λ 较小时，训练损失较低，表明模型在训练数据上拟合得较好。</li><li>当 λ 较大时，训练损失显著增加，表明模型可能欠拟合。</li></ul></li><li><strong>测试损失（Test Loss）</strong>： <ul><li>测试损失的变化趋势与训练损失类似，但随着 λ 的增加，测试损失的上升速度可能较慢。</li><li>在 λ 较小的范围内，测试损失可能较低，表明模型在未见过的数据上表现较好。</li><li>当 λ 继续增加时，测试损失也会增加，表明模型可能过于简单，无法捕捉数据中的复杂模式。</li></ul></li></ol><h2 id="task-02" tabindex="-1"><a class="header-anchor" href="#task-02"><span>Task 02</span></a></h2><blockquote><p>According to the code related to dropout, what will happen if we change the dropout probabilities of the first and second layers?</p></blockquote><h3 id="代码-1" tabindex="-1"><a class="header-anchor" href="#代码-1"><span>代码</span></a></h3><div class="language-python line-numbers-mode has-collapsed-lines collapsed" data-highlighter="shiki" data-ext="python" style="--vp-collapsed-lines:15;background-color:#272822;color:#F8F8F2;"><pre class="shiki monokai vp-code"><code><span class="line"><span style="color:#F92672;">import</span><span style="color:#F8F8F2;"> torch</span></span>
<span class="line"><span style="color:#F92672;">from</span><span style="color:#F8F8F2;"> torch </span><span style="color:#F92672;">import</span><span style="color:#F8F8F2;"> nn</span></span>
<span class="line"><span style="color:#F92672;">import</span><span style="color:#F8F8F2;"> torchvision</span></span>
<span class="line"><span style="color:#F92672;">import</span><span style="color:#F8F8F2;"> torchvision.transforms </span><span style="color:#F92672;">as</span><span style="color:#F8F8F2;"> transforms</span></span>
<span class="line"><span style="color:#F92672;">import</span><span style="color:#F8F8F2;"> matplotlib.pyplot </span><span style="color:#F92672;">as</span><span style="color:#F8F8F2;"> plt</span></span>
<span class="line"></span>
<span class="line"><span style="color:#88846F;"># 设定超参数</span></span>
<span class="line"><span style="color:#F8F8F2;">batch_size </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 256</span></span>
<span class="line"><span style="color:#F8F8F2;">num_inputs, num_outputs </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 784</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">10</span></span>
<span class="line"><span style="color:#F8F8F2;">num_hiddens </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 128</span></span>
<span class="line"><span style="color:#F8F8F2;">num_layers </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 2</span></span>
<span class="line"><span style="color:#F8F8F2;">learning_rate </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 0.1</span></span>
<span class="line"><span style="color:#F8F8F2;">num_epochs </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 10</span></span>
<span class="line"><span style="color:#F8F8F2;">dropout_probs </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> [</span><span style="color:#AE81FF;">0.0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0.2</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0.5</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0.8</span><span style="color:#F8F8F2;">]  </span><span style="color:#88846F;"># 不同的 dropout 概率</span></span>
<span class="line"></span>
<span class="line"><span style="color:#88846F;"># 预处理 FashionMNIST 数据集</span></span>
<span class="line"><span style="color:#F8F8F2;">transform </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> transforms.Compose([transforms.ToTensor()])</span></span>
<span class="line"><span style="color:#F8F8F2;">train_dataset </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> torchvision.datasets.FashionMNIST(</span><span style="color:#FD971F;font-style:italic;">root</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;./data&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">train</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">True</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">transform</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">transform, </span><span style="color:#FD971F;font-style:italic;">download</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">True</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">test_dataset </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> torchvision.datasets.FashionMNIST(</span><span style="color:#FD971F;font-style:italic;">root</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;./data&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">train</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">False</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">transform</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">transform, </span><span style="color:#FD971F;font-style:italic;">download</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">True</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">train_loader </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> torch.utils.data.DataLoader(train_dataset, </span><span style="color:#FD971F;font-style:italic;">batch_size</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">batch_size, </span><span style="color:#FD971F;font-style:italic;">shuffle</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">True</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">test_loader </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> torch.utils.data.DataLoader(test_dataset, </span><span style="color:#FD971F;font-style:italic;">batch_size</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">batch_size, </span><span style="color:#FD971F;font-style:italic;">shuffle</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">False</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#88846F;"># 定义 MLP 网络，包含 dropout</span></span>
<span class="line"><span style="color:#66D9EF;font-style:italic;">class</span><span> </span><span style="color:#A6E22E;text-decoration:underline;">MLP</span><span style="color:#F8F8F2;">(</span><span style="color:#A6E22E;font-style:italic;text-decoration:underline;">nn</span><span style="color:#F8F8F2;">.</span><span style="color:#A6E22E;font-style:italic;text-decoration:underline;">Module</span><span style="color:#F8F8F2;">):</span></span>
<span class="line"><span style="color:#66D9EF;font-style:italic;">    def</span><span style="color:#66D9EF;"> __init__</span><span style="color:#F8F8F2;">(</span><span style="color:#FD971F;font-style:italic;">self</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">num_inputs</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">num_hiddens</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">num_outputs</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">num_layers</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">dropout_prob</span><span style="color:#F8F8F2;">):</span></span>
<span class="line"><span style="color:#66D9EF;font-style:italic;">        super</span><span style="color:#F8F8F2;">(</span><span style="color:#AE81FF;">MLP</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;">self</span><span style="color:#F8F8F2;">).</span><span style="color:#66D9EF;">__init__</span><span style="color:#F8F8F2;">()</span></span>
<span class="line"><span style="color:#F8F8F2;">        layers </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> []</span></span>
<span class="line"><span style="color:#F8F8F2;">        layers.append(nn.Linear(num_inputs, num_hiddens))</span></span>
<span class="line"><span style="color:#F8F8F2;">        layers.append(nn.ReLU())</span></span>
<span class="line"><span style="color:#F8F8F2;">        layers.append(nn.Dropout(dropout_prob))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F92672;">        for</span><span style="color:#F8F8F2;"> _ </span><span style="color:#F92672;">in</span><span style="color:#66D9EF;"> range</span><span style="color:#F8F8F2;">(num_layers </span><span style="color:#F92672;">-</span><span style="color:#AE81FF;"> 1</span><span style="color:#F8F8F2;">):</span></span>
<span class="line"><span style="color:#F8F8F2;">            layers.append(nn.Linear(num_hiddens, num_hiddens))</span></span>
<span class="line"><span style="color:#F8F8F2;">            layers.append(nn.ReLU())</span></span>
<span class="line"><span style="color:#F8F8F2;">            layers.append(nn.Dropout(dropout_prob))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F8F8F2;">        layers.append(nn.Linear(num_hiddens, num_outputs))</span></span>
<span class="line"><span style="color:#FD971F;">        self</span><span style="color:#F8F8F2;">.net </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> nn.Sequential(</span><span style="color:#F92672;">*</span><span style="color:#F8F8F2;">layers)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#66D9EF;font-style:italic;">    def</span><span style="color:#A6E22E;"> forward</span><span style="color:#F8F8F2;">(</span><span style="color:#FD971F;font-style:italic;">self</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">X</span><span style="color:#F8F8F2;">):</span></span>
<span class="line"><span style="color:#F92672;">        return</span><span style="color:#FD971F;"> self</span><span style="color:#F8F8F2;">.net(X.view(</span><span style="color:#F92672;">-</span><span style="color:#AE81FF;">1</span><span style="color:#F8F8F2;">, num_inputs))  </span><span style="color:#88846F;"># 展平输入</span></span>
<span class="line"></span>
<span class="line"><span style="color:#88846F;"># 训练函数，接收不同的 dropout 概率</span></span>
<span class="line"><span style="color:#66D9EF;font-style:italic;">def</span><span style="color:#A6E22E;"> train_with_dropout</span><span style="color:#F8F8F2;">(</span><span style="color:#FD971F;font-style:italic;">dropout_probs</span><span style="color:#F8F8F2;">):</span></span>
<span class="line"><span style="color:#F8F8F2;">    train_losses, val_losses </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> [], []</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F92672;">    for</span><span style="color:#F8F8F2;"> dropout_prob </span><span style="color:#F92672;">in</span><span style="color:#F8F8F2;"> dropout_probs:</span></span>
<span class="line"><span style="color:#F8F8F2;">        net </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> MLP(num_inputs, num_hiddens, num_outputs, num_layers, dropout_prob)</span></span>
<span class="line"><span style="color:#F8F8F2;">        loss </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> nn.CrossEntropyLoss()</span></span>
<span class="line"><span style="color:#F8F8F2;">        optimizer </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> torch.optim.SGD(net.parameters(), </span><span style="color:#FD971F;font-style:italic;">lr</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">learning_rate)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F8F8F2;">        train_loss, val_loss </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> [], []</span></span>
<span class="line"><span style="color:#F92672;">        for</span><span style="color:#F8F8F2;"> epoch </span><span style="color:#F92672;">in</span><span style="color:#66D9EF;"> range</span><span style="color:#F8F8F2;">(num_epochs):</span></span>
<span class="line"><span style="color:#F8F8F2;">            net.train()</span></span>
<span class="line"><span style="color:#F8F8F2;">            total_loss, total_samples </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span></span>
<span class="line"><span style="color:#F92672;">            for</span><span style="color:#F8F8F2;"> X, y </span><span style="color:#F92672;">in</span><span style="color:#F8F8F2;"> train_loader:</span></span>
<span class="line"><span style="color:#F8F8F2;">                y_hat </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> net(X)</span></span>
<span class="line"><span style="color:#F8F8F2;">                l </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> loss(y_hat, y)</span></span>
<span class="line"><span style="color:#F8F8F2;">                optimizer.zero_grad()</span></span>
<span class="line"><span style="color:#F8F8F2;">                l.backward()</span></span>
<span class="line"><span style="color:#F8F8F2;">                optimizer.step()</span></span>
<span class="line"><span style="color:#F8F8F2;">                total_loss </span><span style="color:#F92672;">+=</span><span style="color:#F8F8F2;"> l.item() </span><span style="color:#F92672;">*</span><span style="color:#F8F8F2;"> y.size(</span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">                total_samples </span><span style="color:#F92672;">+=</span><span style="color:#F8F8F2;"> y.size(</span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">            train_loss.append(total_loss </span><span style="color:#F92672;">/</span><span style="color:#F8F8F2;"> total_samples)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F8F8F2;">            net.eval()</span></span>
<span class="line"><span style="color:#F8F8F2;">            total, test_loss </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span></span>
<span class="line"><span style="color:#F92672;">            with</span><span style="color:#F8F8F2;"> torch.no_grad():</span></span>
<span class="line"><span style="color:#F92672;">                for</span><span style="color:#F8F8F2;"> X, y </span><span style="color:#F92672;">in</span><span style="color:#F8F8F2;"> test_loader:</span></span>
<span class="line"><span style="color:#F8F8F2;">                    y_hat </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> net(X)</span></span>
<span class="line"><span style="color:#F8F8F2;">                    test_loss </span><span style="color:#F92672;">+=</span><span style="color:#F8F8F2;"> loss(y_hat, y).item() </span><span style="color:#F92672;">*</span><span style="color:#F8F8F2;"> y.size(</span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">                    total </span><span style="color:#F92672;">+=</span><span style="color:#F8F8F2;"> y.size(</span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">            val_loss.append(test_loss </span><span style="color:#F92672;">/</span><span style="color:#F8F8F2;"> total)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F8F8F2;">        train_losses.append(train_loss[</span><span style="color:#F92672;">-</span><span style="color:#AE81FF;">1</span><span style="color:#F8F8F2;">])</span></span>
<span class="line"><span style="color:#F8F8F2;">        val_losses.append(val_loss[</span><span style="color:#F92672;">-</span><span style="color:#AE81FF;">1</span><span style="color:#F8F8F2;">])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#66D9EF;">        print</span><span style="color:#F8F8F2;">(</span><span style="color:#66D9EF;font-style:italic;">f</span><span style="color:#E6DB74;">&quot;Dropout Prob=</span><span style="color:#AE81FF;">{</span><span style="color:#F8F8F2;">dropout_prob</span><span style="color:#66D9EF;font-style:italic;">:.2f</span><span style="color:#AE81FF;">}</span><span style="color:#E6DB74;">, Final Train Loss=</span><span style="color:#AE81FF;">{</span><span style="color:#F8F8F2;">train_loss[</span><span style="color:#F92672;">-</span><span style="color:#AE81FF;">1</span><span style="color:#F8F8F2;">]</span><span style="color:#66D9EF;font-style:italic;">:.4f</span><span style="color:#AE81FF;">}</span><span style="color:#E6DB74;">, Final Val Loss=</span><span style="color:#AE81FF;">{</span><span style="color:#F8F8F2;">val_loss[</span><span style="color:#F92672;">-</span><span style="color:#AE81FF;">1</span><span style="color:#F8F8F2;">]</span><span style="color:#66D9EF;font-style:italic;">:.4f</span><span style="color:#AE81FF;">}</span><span style="color:#E6DB74;">&quot;</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F92672;">    return</span><span style="color:#F8F8F2;"> train_losses, val_losses</span></span>
<span class="line"></span>
<span class="line"><span style="color:#88846F;"># 训练并记录不同 dropout 概率对损失的影响</span></span>
<span class="line"><span style="color:#F8F8F2;">train_losses, val_losses </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> train_with_dropout(dropout_probs)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#88846F;"># 绘制 dropout 概率 vs 训练损失 / 测试损失曲线</span></span>
<span class="line"><span style="color:#F8F8F2;">plt.figure(</span><span style="color:#FD971F;font-style:italic;">figsize</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">(</span><span style="color:#AE81FF;">8</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">5</span><span style="color:#F8F8F2;">))</span></span>
<span class="line"><span style="color:#F8F8F2;">plt.plot(dropout_probs, train_losses, </span><span style="color:#FD971F;font-style:italic;">marker</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;o&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">label</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;Train Loss&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">color</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;blue&quot;</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">plt.plot(dropout_probs, val_losses, </span><span style="color:#FD971F;font-style:italic;">marker</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;s&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">label</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;Test Loss&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">color</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;orange&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">linestyle</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;dashed&quot;</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">plt.xlabel(</span><span style="color:#E6DB74;">&quot;Dropout Probability&quot;</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">plt.ylabel(</span><span style="color:#E6DB74;">&quot;Loss&quot;</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">plt.legend()</span></span>
<span class="line"><span style="color:#F8F8F2;">plt.title(</span><span style="color:#E6DB74;">&quot;Effect of Dropout Probability on Loss&quot;</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">plt.show()</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div><div class="collapsed-lines"></div></div><h3 id="运行结果-1" tabindex="-1"><a class="header-anchor" href="#运行结果-1"><span>运行结果</span></a></h3><img src="`+e+`" alt="image-20250311213036142" style="zoom:50%;"><p>从图中可以看出，随着 dropout 概率的增加，训练损失（Train Loss）和测试损失（Test Loss）呈现出不同的变化趋势。当 dropout 概率较低时（如 0.0 到 0.2），训练损失较低，但测试损失相对较高，这表明模型可能存在过拟合现象。随着 dropout 概率的增加（如 0.3 到 0.5），训练损失逐渐上升，而测试损失则有所下降，说明适度的 dropout 有效地减少了过拟合，提高了模型的泛化能力。然而，当 dropout 概率进一步增加（如 0.6 到 0.8），训练损失和测试损失都显著上升，表明过高的 dropout 概率可能导致模型欠拟合，无法充分学习数据中的特征。dropout 概率在 0.5 处为最佳。</p><h2 id="task-03" tabindex="-1"><a class="header-anchor" href="#task-03"><span>Task 03</span></a></h2><blockquote><p>Through experiments, test: What will happen if dropout and weight decay are used simultaneously? Will the results be cumulative? Will it be worse? Or will they cancel each other out?</p></blockquote><h3 id="代码-2" tabindex="-1"><a class="header-anchor" href="#代码-2"><span>代码</span></a></h3><div class="language-python line-numbers-mode has-collapsed-lines collapsed" data-highlighter="shiki" data-ext="python" style="--vp-collapsed-lines:15;background-color:#272822;color:#F8F8F2;"><pre class="shiki monokai vp-code"><code><span class="line"><span style="color:#F92672;">import</span><span style="color:#F8F8F2;"> torch</span></span>
<span class="line"><span style="color:#F92672;">from</span><span style="color:#F8F8F2;"> torch </span><span style="color:#F92672;">import</span><span style="color:#F8F8F2;"> nn</span></span>
<span class="line"><span style="color:#F92672;">import</span><span style="color:#F8F8F2;"> torchvision</span></span>
<span class="line"><span style="color:#F92672;">import</span><span style="color:#F8F8F2;"> torchvision.transforms </span><span style="color:#F92672;">as</span><span style="color:#F8F8F2;"> transforms</span></span>
<span class="line"><span style="color:#F92672;">import</span><span style="color:#F8F8F2;"> matplotlib.pyplot </span><span style="color:#F92672;">as</span><span style="color:#F8F8F2;"> plt</span></span>
<span class="line"></span>
<span class="line"><span style="color:#88846F;"># 设定超参数</span></span>
<span class="line"><span style="color:#F8F8F2;">batch_size </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 256</span></span>
<span class="line"><span style="color:#F8F8F2;">num_inputs, num_outputs </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 784</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">10</span></span>
<span class="line"><span style="color:#F8F8F2;">num_hiddens </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 128</span></span>
<span class="line"><span style="color:#F8F8F2;">num_layers </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 2</span></span>
<span class="line"><span style="color:#F8F8F2;">learning_rate </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 0.1</span></span>
<span class="line"><span style="color:#F8F8F2;">num_epochs </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 10</span></span>
<span class="line"><span style="color:#F8F8F2;">dropout_probs </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> [</span><span style="color:#AE81FF;">0.0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0.2</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0.5</span><span style="color:#F8F8F2;">]  </span><span style="color:#88846F;"># 不同的 dropout 概率</span></span>
<span class="line"><span style="color:#F8F8F2;">lambda_values </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> [</span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0.0001</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0.001</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0.01</span><span style="color:#F8F8F2;">]  </span><span style="color:#88846F;"># 不同的权重衰减系数 λ</span></span>
<span class="line"></span>
<span class="line"><span style="color:#88846F;"># 预处理 FashionMNIST 数据集</span></span>
<span class="line"><span style="color:#F8F8F2;">transform </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> transforms.Compose([transforms.ToTensor()])</span></span>
<span class="line"><span style="color:#F8F8F2;">train_dataset </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> torchvision.datasets.FashionMNIST(</span><span style="color:#FD971F;font-style:italic;">root</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;./data&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">train</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">True</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">transform</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">transform, </span><span style="color:#FD971F;font-style:italic;">download</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">True</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">test_dataset </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> torchvision.datasets.FashionMNIST(</span><span style="color:#FD971F;font-style:italic;">root</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;./data&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">train</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">False</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">transform</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">transform, </span><span style="color:#FD971F;font-style:italic;">download</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">True</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">train_loader </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> torch.utils.data.DataLoader(train_dataset, </span><span style="color:#FD971F;font-style:italic;">batch_size</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">batch_size, </span><span style="color:#FD971F;font-style:italic;">shuffle</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">True</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">test_loader </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> torch.utils.data.DataLoader(test_dataset, </span><span style="color:#FD971F;font-style:italic;">batch_size</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">batch_size, </span><span style="color:#FD971F;font-style:italic;">shuffle</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">False</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#88846F;"># 定义 MLP 网络，包含 dropout</span></span>
<span class="line"><span style="color:#66D9EF;font-style:italic;">class</span><span> </span><span style="color:#A6E22E;text-decoration:underline;">MLP</span><span style="color:#F8F8F2;">(</span><span style="color:#A6E22E;font-style:italic;text-decoration:underline;">nn</span><span style="color:#F8F8F2;">.</span><span style="color:#A6E22E;font-style:italic;text-decoration:underline;">Module</span><span style="color:#F8F8F2;">):</span></span>
<span class="line"><span style="color:#66D9EF;font-style:italic;">    def</span><span style="color:#66D9EF;"> __init__</span><span style="color:#F8F8F2;">(</span><span style="color:#FD971F;font-style:italic;">self</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">num_inputs</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">num_hiddens</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">num_outputs</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">num_layers</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">dropout_prob</span><span style="color:#F8F8F2;">):</span></span>
<span class="line"><span style="color:#66D9EF;font-style:italic;">        super</span><span style="color:#F8F8F2;">(</span><span style="color:#AE81FF;">MLP</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;">self</span><span style="color:#F8F8F2;">).</span><span style="color:#66D9EF;">__init__</span><span style="color:#F8F8F2;">()</span></span>
<span class="line"><span style="color:#F8F8F2;">        layers </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> []</span></span>
<span class="line"><span style="color:#F8F8F2;">        layers.append(nn.Linear(num_inputs, num_hiddens))</span></span>
<span class="line"><span style="color:#F8F8F2;">        layers.append(nn.ReLU())</span></span>
<span class="line"><span style="color:#F8F8F2;">        layers.append(nn.Dropout(dropout_prob))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F92672;">        for</span><span style="color:#F8F8F2;"> _ </span><span style="color:#F92672;">in</span><span style="color:#66D9EF;"> range</span><span style="color:#F8F8F2;">(num_layers </span><span style="color:#F92672;">-</span><span style="color:#AE81FF;"> 1</span><span style="color:#F8F8F2;">):</span></span>
<span class="line"><span style="color:#F8F8F2;">            layers.append(nn.Linear(num_hiddens, num_hiddens))</span></span>
<span class="line"><span style="color:#F8F8F2;">            layers.append(nn.ReLU())</span></span>
<span class="line"><span style="color:#F8F8F2;">            layers.append(nn.Dropout(dropout_prob))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F8F8F2;">        layers.append(nn.Linear(num_hiddens, num_outputs))</span></span>
<span class="line"><span style="color:#FD971F;">        self</span><span style="color:#F8F8F2;">.net </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> nn.Sequential(</span><span style="color:#F92672;">*</span><span style="color:#F8F8F2;">layers)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#66D9EF;font-style:italic;">    def</span><span style="color:#A6E22E;"> forward</span><span style="color:#F8F8F2;">(</span><span style="color:#FD971F;font-style:italic;">self</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">X</span><span style="color:#F8F8F2;">):</span></span>
<span class="line"><span style="color:#F92672;">        return</span><span style="color:#FD971F;"> self</span><span style="color:#F8F8F2;">.net(X.view(</span><span style="color:#F92672;">-</span><span style="color:#AE81FF;">1</span><span style="color:#F8F8F2;">, num_inputs))  </span><span style="color:#88846F;"># 展平输入</span></span>
<span class="line"></span>
<span class="line"><span style="color:#88846F;"># 训练函数，接收不同的 dropout 概率和 weight decay 参数</span></span>
<span class="line"><span style="color:#66D9EF;font-style:italic;">def</span><span style="color:#A6E22E;"> train_with_dropout_and_weight_decay</span><span style="color:#F8F8F2;">(</span><span style="color:#FD971F;font-style:italic;">dropout_probs</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">lambda_values</span><span style="color:#F8F8F2;">):</span></span>
<span class="line"><span style="color:#F8F8F2;">    results </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> []</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F92672;">    for</span><span style="color:#F8F8F2;"> dropout_prob </span><span style="color:#F92672;">in</span><span style="color:#F8F8F2;"> dropout_probs:</span></span>
<span class="line"><span style="color:#F92672;">        for</span><span style="color:#F8F8F2;"> weight_decay </span><span style="color:#F92672;">in</span><span style="color:#F8F8F2;"> lambda_values:</span></span>
<span class="line"><span style="color:#F8F8F2;">            net </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> MLP(num_inputs, num_hiddens, num_outputs, num_layers, dropout_prob)</span></span>
<span class="line"><span style="color:#F8F8F2;">            loss </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> nn.CrossEntropyLoss()</span></span>
<span class="line"><span style="color:#F8F8F2;">            optimizer </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> torch.optim.SGD(net.parameters(), </span><span style="color:#FD971F;font-style:italic;">lr</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">learning_rate, </span><span style="color:#FD971F;font-style:italic;">weight_decay</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">weight_decay)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F8F8F2;">            train_loss, val_loss </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> [], []</span></span>
<span class="line"><span style="color:#F92672;">            for</span><span style="color:#F8F8F2;"> epoch </span><span style="color:#F92672;">in</span><span style="color:#66D9EF;"> range</span><span style="color:#F8F8F2;">(num_epochs):</span></span>
<span class="line"><span style="color:#F8F8F2;">                net.train()</span></span>
<span class="line"><span style="color:#F8F8F2;">                total_loss, total_samples </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span></span>
<span class="line"><span style="color:#F92672;">                for</span><span style="color:#F8F8F2;"> X, y </span><span style="color:#F92672;">in</span><span style="color:#F8F8F2;"> train_loader:</span></span>
<span class="line"><span style="color:#F8F8F2;">                    y_hat </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> net(X)</span></span>
<span class="line"><span style="color:#F8F8F2;">                    l </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> loss(y_hat, y)</span></span>
<span class="line"><span style="color:#F8F8F2;">                    optimizer.zero_grad()</span></span>
<span class="line"><span style="color:#F8F8F2;">                    l.backward()</span></span>
<span class="line"><span style="color:#F8F8F2;">                    optimizer.step()</span></span>
<span class="line"><span style="color:#F8F8F2;">                    total_loss </span><span style="color:#F92672;">+=</span><span style="color:#F8F8F2;"> l.item() </span><span style="color:#F92672;">*</span><span style="color:#F8F8F2;"> y.size(</span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">                    total_samples </span><span style="color:#F92672;">+=</span><span style="color:#F8F8F2;"> y.size(</span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">                train_loss.append(total_loss </span><span style="color:#F92672;">/</span><span style="color:#F8F8F2;"> total_samples)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F8F8F2;">                net.eval()</span></span>
<span class="line"><span style="color:#F8F8F2;">                total, test_loss </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span></span>
<span class="line"><span style="color:#F92672;">                with</span><span style="color:#F8F8F2;"> torch.no_grad():</span></span>
<span class="line"><span style="color:#F92672;">                    for</span><span style="color:#F8F8F2;"> X, y </span><span style="color:#F92672;">in</span><span style="color:#F8F8F2;"> test_loader:</span></span>
<span class="line"><span style="color:#F8F8F2;">                        y_hat </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> net(X)</span></span>
<span class="line"><span style="color:#F8F8F2;">                        test_loss </span><span style="color:#F92672;">+=</span><span style="color:#F8F8F2;"> loss(y_hat, y).item() </span><span style="color:#F92672;">*</span><span style="color:#F8F8F2;"> y.size(</span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">                        total </span><span style="color:#F92672;">+=</span><span style="color:#F8F8F2;"> y.size(</span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">                val_loss.append(test_loss </span><span style="color:#F92672;">/</span><span style="color:#F8F8F2;"> total)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F8F8F2;">            results.append({</span></span>
<span class="line"><span style="color:#E6DB74;">                &#39;dropout_prob&#39;</span><span style="color:#F8F8F2;">: dropout_prob,</span></span>
<span class="line"><span style="color:#E6DB74;">                &#39;weight_decay&#39;</span><span style="color:#F8F8F2;">: weight_decay,</span></span>
<span class="line"><span style="color:#E6DB74;">                &#39;train_loss&#39;</span><span style="color:#F8F8F2;">: train_loss[</span><span style="color:#F92672;">-</span><span style="color:#AE81FF;">1</span><span style="color:#F8F8F2;">],</span></span>
<span class="line"><span style="color:#E6DB74;">                &#39;val_loss&#39;</span><span style="color:#F8F8F2;">: val_loss[</span><span style="color:#F92672;">-</span><span style="color:#AE81FF;">1</span><span style="color:#F8F8F2;">]</span></span>
<span class="line"><span style="color:#F8F8F2;">            })</span></span>
<span class="line"></span>
<span class="line"><span style="color:#66D9EF;">            print</span><span style="color:#F8F8F2;">(</span><span style="color:#66D9EF;font-style:italic;">f</span><span style="color:#E6DB74;">&quot;Dropout Prob=</span><span style="color:#AE81FF;">{</span><span style="color:#F8F8F2;">dropout_prob</span><span style="color:#66D9EF;font-style:italic;">:.2f</span><span style="color:#AE81FF;">}</span><span style="color:#E6DB74;">, λ=</span><span style="color:#AE81FF;">{</span><span style="color:#F8F8F2;">weight_decay</span><span style="color:#66D9EF;font-style:italic;">:.5f</span><span style="color:#AE81FF;">}</span><span style="color:#E6DB74;">, Final Train Loss=</span><span style="color:#AE81FF;">{</span><span style="color:#F8F8F2;">train_loss[</span><span style="color:#F92672;">-</span><span style="color:#AE81FF;">1</span><span style="color:#F8F8F2;">]</span><span style="color:#66D9EF;font-style:italic;">:.4f</span><span style="color:#AE81FF;">}</span><span style="color:#E6DB74;">, Final Val Loss=</span><span style="color:#AE81FF;">{</span><span style="color:#F8F8F2;">val_loss[</span><span style="color:#F92672;">-</span><span style="color:#AE81FF;">1</span><span style="color:#F8F8F2;">]</span><span style="color:#66D9EF;font-style:italic;">:.4f</span><span style="color:#AE81FF;">}</span><span style="color:#E6DB74;">&quot;</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F92672;">    return</span><span style="color:#F8F8F2;"> results</span></span>
<span class="line"></span>
<span class="line"><span style="color:#88846F;"># 训练并记录不同 dropout 概率和 weight decay 对损失的影响</span></span>
<span class="line"><span style="color:#F8F8F2;">results </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> train_with_dropout_and_weight_decay(dropout_probs, lambda_values)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#88846F;"># 绘制结果</span></span>
<span class="line"><span style="color:#F92672;">for</span><span style="color:#F8F8F2;"> dropout_prob </span><span style="color:#F92672;">in</span><span style="color:#F8F8F2;"> dropout_probs:</span></span>
<span class="line"><span style="color:#F8F8F2;">    train_losses </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> [r[</span><span style="color:#E6DB74;">&#39;train_loss&#39;</span><span style="color:#F8F8F2;">] </span><span style="color:#F92672;">for</span><span style="color:#F8F8F2;"> r </span><span style="color:#F92672;">in</span><span style="color:#F8F8F2;"> results </span><span style="color:#F92672;">if</span><span style="color:#F8F8F2;"> r[</span><span style="color:#E6DB74;">&#39;dropout_prob&#39;</span><span style="color:#F8F8F2;">] </span><span style="color:#F92672;">==</span><span style="color:#F8F8F2;"> dropout_prob]</span></span>
<span class="line"><span style="color:#F8F8F2;">    val_losses </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> [r[</span><span style="color:#E6DB74;">&#39;val_loss&#39;</span><span style="color:#F8F8F2;">] </span><span style="color:#F92672;">for</span><span style="color:#F8F8F2;"> r </span><span style="color:#F92672;">in</span><span style="color:#F8F8F2;"> results </span><span style="color:#F92672;">if</span><span style="color:#F8F8F2;"> r[</span><span style="color:#E6DB74;">&#39;dropout_prob&#39;</span><span style="color:#F8F8F2;">] </span><span style="color:#F92672;">==</span><span style="color:#F8F8F2;"> dropout_prob]</span></span>
<span class="line"><span style="color:#F8F8F2;">    plt.plot(lambda_values, train_losses, </span><span style="color:#FD971F;font-style:italic;">marker</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;o&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">label</span><span style="color:#F92672;">=</span><span style="color:#66D9EF;font-style:italic;">f</span><span style="color:#E6DB74;">&quot;Train Loss (Dropout=</span><span style="color:#AE81FF;">{</span><span style="color:#F8F8F2;">dropout_prob</span><span style="color:#AE81FF;">}</span><span style="color:#E6DB74;">)&quot;</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">    plt.plot(lambda_values, val_losses, </span><span style="color:#FD971F;font-style:italic;">marker</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;s&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">label</span><span style="color:#F92672;">=</span><span style="color:#66D9EF;font-style:italic;">f</span><span style="color:#E6DB74;">&quot;Test Loss (Dropout=</span><span style="color:#AE81FF;">{</span><span style="color:#F8F8F2;">dropout_prob</span><span style="color:#AE81FF;">}</span><span style="color:#E6DB74;">)&quot;</span><span style="color:#F8F8F2;">, </span><span style="color:#FD971F;font-style:italic;">linestyle</span><span style="color:#F92672;">=</span><span style="color:#E6DB74;">&quot;dashed&quot;</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F8F8F2;">plt.xlabel(</span><span style="color:#E6DB74;">&quot;Weight Decay (λ)&quot;</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">plt.ylabel(</span><span style="color:#E6DB74;">&quot;Loss&quot;</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">plt.xscale(</span><span style="color:#E6DB74;">&quot;log&quot;</span><span style="color:#F8F8F2;">)  </span><span style="color:#88846F;"># 采用对数刻度更清晰</span></span>
<span class="line"><span style="color:#F8F8F2;">plt.legend()</span></span>
<span class="line"><span style="color:#F8F8F2;">plt.title(</span><span style="color:#E6DB74;">&quot;Effect of Dropout and Weight Decay on Loss&quot;</span><span style="color:#F8F8F2;">)</span></span>
<span class="line"><span style="color:#F8F8F2;">plt.show()</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div><div class="collapsed-lines"></div></div><h3 id="运行结果-2" tabindex="-1"><a class="header-anchor" href="#运行结果-2"><span>运行结果</span></a></h3><img src="`+t+'" alt="image-20250311215618677" style="zoom:50%;"><ol><li><strong>Dropout=0.0 时</strong>： <ul><li>随着 weight decay 的增加，训练损失和测试损失都逐渐上升。这是因为没有 dropout 的情况下，weight decay 单独作用，可能会过度限制模型的复杂度，导致欠拟合。</li></ul></li><li><strong>Dropout=0.2 时</strong>： <ul><li>训练损失和测试损失的变化较为平稳。适度的 dropout 和 weight decay 组合能够有效减少过拟合，同时不会显著增加训练损失。这表明两者在一定程度上可以协同作用，提升模型的泛化能力。</li></ul></li><li><strong>Dropout=0.5 时</strong>： <ul><li>训练损失和测试损失都较高，尤其是在 weight decay 较大时。较高的 dropout 概率已经对模型进行了较强的正则化，再加上 weight decay 的约束，可能会导致模型欠拟合，表现变差。</li></ul></li></ol><p>所以：</p><ul><li><strong>累积效果</strong>：在适度的 dropout 和 weight decay 组合下（如 dropout=0.2，weight decay=0.0001），两者的正则化效果可以叠加，进一步提升模型的泛化能力。</li><li><strong>抵消效果</strong>：当 dropout 概率较高时（如 dropout=0.5），再增加 weight decay 可能会导致模型欠拟合，损失上升，效果不如单独使用其中一种正则化方法。</li><li><strong>最优组合</strong>：实验中，dropout=0.2 和 weight decay=0.0001 的组合表现较好，能够在减少过拟合的同时保持较低的损失。</li></ul>',24)]))}const d=n(F,[["render",c]]),v=JSON.parse('{"path":"/zh/DeepThinking/early_stop.html","title":"早停调参","lang":"zh-CN","frontmatter":{"title":"早停调参","icon":"alias","date":"2025-03-11T11:33:22.000Z","author":"XiaoXianYue","isOriginal":true,"category":["大三下","神经网络与深度学习"],"tag":["大三下","神经网络与深度学习"],"sticky":false,"star":false,"article":true,"timeline":true,"image":false,"navbar":true,"sidebarIcon":true,"headerDepth":5,"lastUpdated":true,"editLink":false,"backToTop":true,"toc":true,"description":"Task 01 According to the referenced code related to weight decay, plot the functions of the training loss and the test loss with respect to λ 代码 运行结果 image-20250311115711863imag...","head":[["meta",{"property":"og:url","content":"https://bougiemoonintaurus/zh/DeepThinking/early_stop.html"}],["meta",{"property":"og:site_name","content":"奶酪奶酪"}],["meta",{"property":"og:title","content":"早停调参"}],["meta",{"property":"og:description","content":"Task 01 According to the referenced code related to weight decay, plot the functions of the training loss and the test loss with respect to λ 代码 运行结果 image-20250311115711863imag..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-03-19T17:01:16.000Z"}],["meta",{"property":"article:author","content":"XiaoXianYue"}],["meta",{"property":"article:tag","content":"大三下"}],["meta",{"property":"article:tag","content":"神经网络与深度学习"}],["meta",{"property":"article:published_time","content":"2025-03-11T11:33:22.000Z"}],["meta",{"property":"article:modified_time","content":"2025-03-19T17:01:16.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"早停调参\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-03-11T11:33:22.000Z\\",\\"dateModified\\":\\"2025-03-19T17:01:16.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"XiaoXianYue\\"}]}"]]},"git":{"createdTime":1742403676000,"updatedTime":1742403676000,"contributors":[{"name":"Xiaoxianyue","username":"Xiaoxianyue","email":"2310219843@qq.com","commits":1,"url":"https://github.com/Xiaoxianyue"}]},"readingTime":{"minutes":7.04,"words":2112},"filePathRelative":"zh/DeepThinking/early_stop.md","localizedDate":"2025年3月11日","excerpt":"<h2>Task 01</h2>\\n<blockquote>\\n<p>According to the referenced code related to weight decay, plot the functions of the training loss and the test loss with respect to λ</p>\\n</blockquote>\\n<h3>代码</h3>\\n<div class=\\"language-python line-numbers-mode has-collapsed-lines collapsed\\" data-highlighter=\\"shiki\\" data-ext=\\"python\\" style=\\"--vp-collapsed-lines:15;background-color:#272822;color:#F8F8F2\\"><pre class=\\"shiki monokai vp-code\\"><code><span class=\\"line\\"><span style=\\"color:#F92672\\">import</span><span style=\\"color:#F8F8F2\\"> torch</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F92672\\">from</span><span style=\\"color:#F8F8F2\\"> torch </span><span style=\\"color:#F92672\\">import</span><span style=\\"color:#F8F8F2\\"> nn</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F92672\\">import</span><span style=\\"color:#F8F8F2\\"> torchvision</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F92672\\">import</span><span style=\\"color:#F8F8F2\\"> torchvision.transforms </span><span style=\\"color:#F92672\\">as</span><span style=\\"color:#F8F8F2\\"> transforms</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F92672\\">import</span><span style=\\"color:#F8F8F2\\"> matplotlib.pyplot </span><span style=\\"color:#F92672\\">as</span><span style=\\"color:#F8F8F2\\"> plt</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#88846F\\"># 设定超参数</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">batch_size </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\"> 256</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">num_inputs, num_outputs </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\"> 784</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#AE81FF\\">10</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">num_hiddens </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\"> 128</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">num_layers </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\"> 2</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">learning_rate </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\"> 0.1</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">num_epochs </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\"> 10</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">lambda_values </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> [</span><span style=\\"color:#AE81FF\\">0</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#AE81FF\\">0.0001</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#AE81FF\\">0.001</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#AE81FF\\">0.01</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#AE81FF\\">0.1</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#AE81FF\\">1</span><span style=\\"color:#F8F8F2\\">]  </span><span style=\\"color:#88846F\\"># 不同的权重衰减系数 λ</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#88846F\\"># 预处理 FashionMNIST 数据集</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">transform </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> transforms.Compose([transforms.ToTensor()])</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">train_dataset </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> torchvision.datasets.FashionMNIST(</span><span style=\\"color:#FD971F;font-style:italic\\">root</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#E6DB74\\">\\"./data\\"</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">train</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\">True</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">transform</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\">transform, </span><span style=\\"color:#FD971F;font-style:italic\\">download</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\">True</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">test_dataset </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> torchvision.datasets.FashionMNIST(</span><span style=\\"color:#FD971F;font-style:italic\\">root</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#E6DB74\\">\\"./data\\"</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">train</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\">False</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">transform</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\">transform, </span><span style=\\"color:#FD971F;font-style:italic\\">download</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\">True</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">train_loader </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> torch.utils.data.DataLoader(train_dataset, </span><span style=\\"color:#FD971F;font-style:italic\\">batch_size</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\">batch_size, </span><span style=\\"color:#FD971F;font-style:italic\\">shuffle</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\">True</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">test_loader </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> torch.utils.data.DataLoader(test_dataset, </span><span style=\\"color:#FD971F;font-style:italic\\">batch_size</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\">batch_size, </span><span style=\\"color:#FD971F;font-style:italic\\">shuffle</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\">False</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#88846F\\"># 定义 MLP 网络</span></span>\\n<span class=\\"line\\"><span style=\\"color:#66D9EF;font-style:italic\\">class</span><span> </span><span style=\\"color:#A6E22E;text-decoration:underline\\">MLP</span><span style=\\"color:#F8F8F2\\">(</span><span style=\\"color:#A6E22E;font-style:italic;text-decoration:underline\\">nn</span><span style=\\"color:#F8F8F2\\">.</span><span style=\\"color:#A6E22E;font-style:italic;text-decoration:underline\\">Module</span><span style=\\"color:#F8F8F2\\">):</span></span>\\n<span class=\\"line\\"><span style=\\"color:#66D9EF;font-style:italic\\">    def</span><span style=\\"color:#66D9EF\\"> __init__</span><span style=\\"color:#F8F8F2\\">(</span><span style=\\"color:#FD971F;font-style:italic\\">self</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">num_inputs</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">num_hiddens</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">num_outputs</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">num_layers</span><span style=\\"color:#F8F8F2\\">):</span></span>\\n<span class=\\"line\\"><span style=\\"color:#66D9EF;font-style:italic\\">        super</span><span style=\\"color:#F8F8F2\\">(</span><span style=\\"color:#AE81FF\\">MLP</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F\\">self</span><span style=\\"color:#F8F8F2\\">).</span><span style=\\"color:#66D9EF\\">__init__</span><span style=\\"color:#F8F8F2\\">()</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">        layers </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> []</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">        layers.append(nn.Linear(num_inputs, num_hiddens))</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">        layers.append(nn.ReLU())</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#F92672\\">        for</span><span style=\\"color:#F8F8F2\\"> _ </span><span style=\\"color:#F92672\\">in</span><span style=\\"color:#66D9EF\\"> range</span><span style=\\"color:#F8F8F2\\">(num_layers </span><span style=\\"color:#F92672\\">-</span><span style=\\"color:#AE81FF\\"> 1</span><span style=\\"color:#F8F8F2\\">):</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">            layers.append(nn.Linear(num_hiddens, num_hiddens))</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">            layers.append(nn.ReLU())</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">        layers.append(nn.Linear(num_hiddens, num_outputs))</span></span>\\n<span class=\\"line\\"><span style=\\"color:#FD971F\\">        self</span><span style=\\"color:#F8F8F2\\">.net </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> nn.Sequential(</span><span style=\\"color:#F92672\\">*</span><span style=\\"color:#F8F8F2\\">layers)</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#66D9EF;font-style:italic\\">    def</span><span style=\\"color:#A6E22E\\"> forward</span><span style=\\"color:#F8F8F2\\">(</span><span style=\\"color:#FD971F;font-style:italic\\">self</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">X</span><span style=\\"color:#F8F8F2\\">):</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F92672\\">        return</span><span style=\\"color:#FD971F\\"> self</span><span style=\\"color:#F8F8F2\\">.net(X.view(</span><span style=\\"color:#F92672\\">-</span><span style=\\"color:#AE81FF\\">1</span><span style=\\"color:#F8F8F2\\">, num_inputs))  </span><span style=\\"color:#88846F\\"># 展平输入</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#88846F\\"># 训练函数，接收不同的 weight_decay 参数</span></span>\\n<span class=\\"line\\"><span style=\\"color:#66D9EF;font-style:italic\\">def</span><span style=\\"color:#A6E22E\\"> train_with_weight_decay</span><span style=\\"color:#F8F8F2\\">(</span><span style=\\"color:#FD971F;font-style:italic\\">lambda_values</span><span style=\\"color:#F8F8F2\\">):</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">    train_losses, val_losses </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> [], []</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#F92672\\">    for</span><span style=\\"color:#F8F8F2\\"> weight_decay </span><span style=\\"color:#F92672\\">in</span><span style=\\"color:#F8F8F2\\"> lambda_values:</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">        net </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> MLP(num_inputs, num_hiddens, num_outputs, num_layers)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">        loss </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> nn.CrossEntropyLoss()</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">        optimizer </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> torch.optim.SGD(net.parameters(), </span><span style=\\"color:#FD971F;font-style:italic\\">lr</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\">learning_rate, </span><span style=\\"color:#FD971F;font-style:italic\\">weight_decay</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\">weight_decay)  </span><span style=\\"color:#88846F\\"># 添加权重衰减</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">        train_loss, val_loss </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> [], []</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F92672\\">        for</span><span style=\\"color:#F8F8F2\\"> epoch </span><span style=\\"color:#F92672\\">in</span><span style=\\"color:#66D9EF\\"> range</span><span style=\\"color:#F8F8F2\\">(num_epochs):</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">            net.train()</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">            total_loss, total_samples </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\"> 0</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#AE81FF\\">0</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F92672\\">            for</span><span style=\\"color:#F8F8F2\\"> X, y </span><span style=\\"color:#F92672\\">in</span><span style=\\"color:#F8F8F2\\"> train_loader:</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">                y_hat </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> net(X)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">                l </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> loss(y_hat, y)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">                optimizer.zero_grad()</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">                l.backward()</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">                optimizer.step()</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">                total_loss </span><span style=\\"color:#F92672\\">+=</span><span style=\\"color:#F8F8F2\\"> l.item() </span><span style=\\"color:#F92672\\">*</span><span style=\\"color:#F8F8F2\\"> y.size(</span><span style=\\"color:#AE81FF\\">0</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">                total_samples </span><span style=\\"color:#F92672\\">+=</span><span style=\\"color:#F8F8F2\\"> y.size(</span><span style=\\"color:#AE81FF\\">0</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">            train_loss.append(total_loss </span><span style=\\"color:#F92672\\">/</span><span style=\\"color:#F8F8F2\\"> total_samples)</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">            net.eval()</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">            total, test_loss </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#AE81FF\\"> 0</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#AE81FF\\">0</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F92672\\">            with</span><span style=\\"color:#F8F8F2\\"> torch.no_grad():</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F92672\\">                for</span><span style=\\"color:#F8F8F2\\"> X, y </span><span style=\\"color:#F92672\\">in</span><span style=\\"color:#F8F8F2\\"> test_loader:</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">                    y_hat </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> net(X)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">                    test_loss </span><span style=\\"color:#F92672\\">+=</span><span style=\\"color:#F8F8F2\\"> loss(y_hat, y).item() </span><span style=\\"color:#F92672\\">*</span><span style=\\"color:#F8F8F2\\"> y.size(</span><span style=\\"color:#AE81FF\\">0</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">                    total </span><span style=\\"color:#F92672\\">+=</span><span style=\\"color:#F8F8F2\\"> y.size(</span><span style=\\"color:#AE81FF\\">0</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">            val_loss.append(test_loss </span><span style=\\"color:#F92672\\">/</span><span style=\\"color:#F8F8F2\\"> total)</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">        train_losses.append(train_loss[</span><span style=\\"color:#F92672\\">-</span><span style=\\"color:#AE81FF\\">1</span><span style=\\"color:#F8F8F2\\">])</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">        val_losses.append(val_loss[</span><span style=\\"color:#F92672\\">-</span><span style=\\"color:#AE81FF\\">1</span><span style=\\"color:#F8F8F2\\">])</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#66D9EF\\">        print</span><span style=\\"color:#F8F8F2\\">(</span><span style=\\"color:#66D9EF;font-style:italic\\">f</span><span style=\\"color:#E6DB74\\">\\"λ=</span><span style=\\"color:#AE81FF\\">{</span><span style=\\"color:#F8F8F2\\">weight_decay</span><span style=\\"color:#66D9EF;font-style:italic\\">:.5f</span><span style=\\"color:#AE81FF\\">}</span><span style=\\"color:#E6DB74\\">, Final Train Loss=</span><span style=\\"color:#AE81FF\\">{</span><span style=\\"color:#F8F8F2\\">train_loss[</span><span style=\\"color:#F92672\\">-</span><span style=\\"color:#AE81FF\\">1</span><span style=\\"color:#F8F8F2\\">]</span><span style=\\"color:#66D9EF;font-style:italic\\">:.4f</span><span style=\\"color:#AE81FF\\">}</span><span style=\\"color:#E6DB74\\">, Final Val Loss=</span><span style=\\"color:#AE81FF\\">{</span><span style=\\"color:#F8F8F2\\">val_loss[</span><span style=\\"color:#F92672\\">-</span><span style=\\"color:#AE81FF\\">1</span><span style=\\"color:#F8F8F2\\">]</span><span style=\\"color:#66D9EF;font-style:italic\\">:.4f</span><span style=\\"color:#AE81FF\\">}</span><span style=\\"color:#E6DB74\\">\\"</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#F92672\\">    return</span><span style=\\"color:#F8F8F2\\"> train_losses, val_losses</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#88846F\\"># 训练并记录不同 λ 对损失的影响</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">train_losses, val_losses </span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\"> train_with_weight_decay(lambda_values)</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"color:#88846F\\"># 绘制 λ vs 训练损失 / 测试损失曲线</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">plt.figure(</span><span style=\\"color:#FD971F;font-style:italic\\">figsize</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#F8F8F2\\">(</span><span style=\\"color:#AE81FF\\">8</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#AE81FF\\">5</span><span style=\\"color:#F8F8F2\\">))</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">plt.plot(lambda_values, train_losses, </span><span style=\\"color:#FD971F;font-style:italic\\">marker</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#E6DB74\\">\\"o\\"</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">label</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#E6DB74\\">\\"Train Loss\\"</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">color</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#E6DB74\\">\\"blue\\"</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">plt.plot(lambda_values, val_losses, </span><span style=\\"color:#FD971F;font-style:italic\\">marker</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#E6DB74\\">\\"s\\"</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">label</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#E6DB74\\">\\"Test Loss\\"</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">color</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#E6DB74\\">\\"orange\\"</span><span style=\\"color:#F8F8F2\\">, </span><span style=\\"color:#FD971F;font-style:italic\\">linestyle</span><span style=\\"color:#F92672\\">=</span><span style=\\"color:#E6DB74\\">\\"dashed\\"</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">plt.xlabel(</span><span style=\\"color:#E6DB74\\">\\"Weight Decay (λ)\\"</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">plt.ylabel(</span><span style=\\"color:#E6DB74\\">\\"Loss\\"</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">plt.xscale(</span><span style=\\"color:#E6DB74\\">\\"log\\"</span><span style=\\"color:#F8F8F2\\">)  </span><span style=\\"color:#88846F\\"># 采用对数刻度更清晰</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">plt.legend()</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">plt.title(</span><span style=\\"color:#E6DB74\\">\\"Effect of Weight Decay (λ) on Loss\\"</span><span style=\\"color:#F8F8F2\\">)</span></span>\\n<span class=\\"line\\"><span style=\\"color:#F8F8F2\\">plt.show()</span></span></code></pre>\\n<div class=\\"line-numbers\\" aria-hidden=\\"true\\" style=\\"counter-reset:line-number 0\\"><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div></div><div class=\\"collapsed-lines\\"></div></div>","autoDesc":true}');export{d as comp,v as data};
