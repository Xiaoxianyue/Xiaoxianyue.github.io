import{_ as u}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as d,b as o,d as r,e as t,a as i,r as c,o as h,f as e}from"./app-B675GKIW.js";const b="/assets/image-20241027201718846-D2z__cD3.png",f="/assets/image-20241027201744702-CBC0weog.png",_="/assets/image-20241027202909032-D7xQqezW.png",v="/assets/image-20241027204655653-CMYHlqTk.png",y="/assets/image-20241027204805392-XfZBj4EH.png",x="/assets/image-20241027204853946-D-LOIey8.png",w="/assets/image-20241027205543522-D3RZiah3.png",k="/assets/image-20241027205602049-D5bI6bH3.png",P="/assets/image-20241027221737039-D-R4olfx.png",z="/assets/image-20241027222233423-VA68czbh.png",D="/assets/image-20241027222244376-BhFAU8Og.png",B="/assets/image-20241027225533474-D9hAapVf.png",C="/assets/image-20241027232323261-VIBf_5DQ.png",M="/assets/image-20241027234909233-D8T4YmTU.png",A="/assets/image-20241028000705459-DkBIKUdU.png",I="/assets/image-20241028110307514-Bm27DKia.png",T="/assets/image-20241028000824104-D9G_BWgK.png",j="/assets/image-20241028000906053-Ddwcmlwt.png",L="/assets/image-20241028111502992-CV0W71Z4.png",E="/assets/image-20241028112138008-DbrceSto.png",R="/assets/image-20241028113358468-CMDVUHD_.png",q="/assets/image-20241028113429673-_PkFkKu2.png",N="/assets/image-20241028113955457-hpha7szv.png",F="/assets/image-20241028115816229-z1DSUZt0.png",V="/assets/image-20241028120312355-BqhUcjsb.png",G="/assets/image-20241028120701500-C3C8cnOa.png",U="/assets/image-20241028120731059-Pfqg6ZNM.png",W="/assets/image-20241028120747482-gnZSrA3l.png",H="/assets/image-20241028121314041-C8humTM5.png",O="/assets/image-20241028123535402-Dz4vUWCl.png",Q="/assets/image-20241028145020012-BaL6v1dT.png",Z="/assets/image-20241028153349691-C_ME5EEw.png",S="/assets/image-20241028163353111-FsIc7IjQ.png",K="/assets/image-20241028164632595-BPII71Sb.png",X="/assets/image-20241028165938691-CDOOdxiO.png",Y="/assets/image-20241028170307868-CVaxfghM.png",g="/assets/image-20241028172139566-D9MjdW9O.png",p="/assets/image-20241028172155352-ByLB5e6G.png",m="/assets/image-20241028172212701-D5VvFXDy.png",J="/assets/image-20241030151834269-49DGIHO7.png",$="/assets/image-20241028201747950-CqC78eLT.png",ii="/assets/image-20241028202519998-BNmQRwxS.png",ai="/assets/image-20241028202639040-Dzh-iAeB.png",ei="/assets/image-20241028194942381-BJMzKz8q.png",ti="/assets/image-20241028211328059-CP-Ip2Fw.png",ni="/assets/image-20241028211356956-cnHCrEcv.png",li="/assets/image-20241028211906427-BnAOAeD4.png",si="/assets/image-20241028212025558-DAkvEAsR.png",ri="/assets/image-20241028213137862-dvZayq2K.png",oi="/assets/image-20241028213430962-CAfTf1MU.png",gi="/assets/image-20241028213441225-DCv5DZ9-.png",pi="/assets/image-20241028214510342-BHWkaVq9.png",mi="/assets/image-20241029120131551-cn4DY4Lb.png",ui="/assets/ttt8-Cyq8O-FV.png",di="/assets/image-20241029121022435-CNRHZaVY.png",ci="/assets/image-20241029121122770-D7_4AsBQ.png",hi="/assets/image-20241029123211634-DBo_rrmQ.png",bi="/assets/image-20241029124224566-BMHmfnVs.png",fi={};function _i(vi,a){const s=c("Tabs");return h(),d("div",null,[a[21]||(a[21]=o('<h2 id="lecture-8" tabindex="-1"><a class="header-anchor" href="#lecture-8"><span>Lecture 8</span></a></h2><h3 id="_1-1-random-variable" tabindex="-1"><a class="header-anchor" href="#_1-1-random-variable"><span>1.1 Random Variable</span></a></h3><p>Discrete :</p><img src="'+b+'" alt="image-20241027201718846" style="zoom:33%;"><p>Continuous :</p><img src="'+f+'" alt="image-20241027201744702" style="zoom:33%;"><h3 id="_1-2-joint-probability" tabindex="-1"><a class="header-anchor" href="#_1-2-joint-probability"><span>1.2 Joint Probability</span></a></h3><p>• Written as Pr(𝑥, 𝑦)</p><p>• Can read Pr(𝑥, 𝑦) as “probability of x and y ”</p><h3 id="_1-3-marginalization" tabindex="-1"><a class="header-anchor" href="#_1-3-marginalization"><span>1.3 Marginalization</span></a></h3><details class="hint-container details"><summary>详情</summary><p>这张幻灯片介绍了概率中的边缘化（Marginalization）概念。边缘化是从联合概率分布中获得单一变量的概率分布的方法。</p><p><strong>具体解释：</strong></p><ol><li><p><strong>联合分布</strong>：</p><ul><li>在图像中心，我们可以看到一个二维的联合概率分布 ( Pr(x, y) )，用来表示变量 ( x ) 和 ( y ) 的联合分布情况。亮度越高的区域表示联合分布的概率值越大。</li></ul></li><li><p><strong>边缘概率</strong>：</p><ul><li>边缘化的过程是通过将联合分布 ( Pr(x, y) ) 中其他变量积分（或求和）以得到感兴趣变量的概率分布。</li><li>例如，我们可以通过对 ( Pr(x, y) ) 在 ( y ) 上积分来获得 ( x ) 的边缘分布 ( Pr(x) )： [ Pr(x) = \\int Pr(x, y) , dy ]</li><li>同样，通过对 ( Pr(x, y) ) 在 ( x ) 上积分来获得 ( y ) 的边缘分布 ( Pr(y) )： [ Pr(y) = \\int Pr(x, y) , dx ]</li></ul></li><li><p><strong>图示</strong>：</p><ul><li>下面右侧的图示展示了边缘化的效果。我们从二维联合分布 ( Pr(x, y) ) 中分别得到 ( Pr(x) ) 和 ( Pr(y) ) 的单独分布。</li><li>在顶部和左侧的曲线分别表示 ( Pr(x) ) 和 ( Pr(y) ) 的分布，即分别将 ( y ) 和 ( x ) 变量“边缘化”之后的概率密度函数。</li></ul></li><li><p><strong>直观理解</strong>：</p><ul><li>可以把边缘化理解为在二维分布上“投影”到单个轴上。例如，计算 ( Pr(x) ) 时，相当于将 ( y ) 方向上的分布“压扁”或“合并”，从而得到一个仅与 ( x ) 相关的概率分布。</li></ul></li></ol><p><strong>应用：</strong></p><p>边缘化广泛应用于概率和统计学中，用来计算单一变量的分布，即使在实际中我们关注的是联合分布。</p><img src="'+_+'" alt="image-20241027202909032" style="zoom:33%;"></details><p>在更高维也可以用边缘分布——leaves joint distribution between whatever variables are left</p><h3 id="_1-4-conditional-probability" tabindex="-1"><a class="header-anchor" href="#_1-4-conditional-probability"><span>1.4 Conditional Probability</span></a></h3><p>• Conditional probability can be extracted from joint probability</p><p>• Extract appropriate slice and normalize</p><img src="'+v+'" alt="image-20241027204655653" style="zoom:50%;"><ul><li><p>求解公式：</p><figure><img src="'+y+'" alt="image-20241027204805392" tabindex="0" loading="lazy"><figcaption>image-20241027204805392</figcaption></figure></li><li><p>延伸至高阶</p><figure><img src="'+x+'" alt="image-20241027204853946" tabindex="0" loading="lazy"><figcaption>image-20241027204853946</figcaption></figure></li></ul><h3 id="_1-5-bayes-rule" tabindex="-1"><a class="header-anchor" href="#_1-5-bayes-rule"><span>1.5 Bayes&#39; Rule</span></a></h3><img src="'+w+'" alt="image-20241027205543522" style="zoom:50%;"><img src="'+k+'" alt="image-20241027205602049" style="zoom:50%;"><h3 id="_1-6-independence" tabindex="-1"><a class="header-anchor" href="#_1-6-independence"><span>1.6 Independence</span></a></h3><ul><li><p>When variable are independent:</p><p>Pr(x, y) = Pr(x|y)Pr(y) = Pr(x)Pr(y)</p></li></ul><h3 id="_1-7-expectation" tabindex="-1"><a class="header-anchor" href="#_1-7-expectation"><span>1.7 Expectation</span></a></h3><p><strong>期望是什么：</strong></p><ul><li><p>离散时：</p><figure><img src="'+P+'" alt="image-20241027221737039" tabindex="0" loading="lazy"><figcaption>image-20241027221737039</figcaption></figure></li><li><p>连续时:</p><figure><img src="'+z+'" alt="image-20241027222233423" tabindex="0" loading="lazy"><figcaption>image-20241027222233423</figcaption></figure></li></ul><figure><img src="'+D+'" alt="image-20241027222244376" tabindex="0" loading="lazy"><figcaption>image-20241027222244376</figcaption></figure><p><strong>期望的计算规则</strong>：</p><ul><li><p><strong>规则 1</strong>：常数的期望等于常数本身。 E[k] = k</p></li><li><p><strong>规则 2</strong>：常数乘以函数的期望等于常数乘以函数的期望值。 E[k f[x]]=k E[f[x]]</p></li><li><p><strong>规则 3</strong>：两个函数之和的期望等于它们各自期望的和。 E[f[x]+g[x]]=E[f[x]]+E[g[x]]</p></li><li><p>**规则 4：**两个函数乘积的和的期望等于他们各自期望的积（如果他们是独立的）。E[f[x]g[x]]=E[f[x]]E[g[x]]</p></li></ul><h3 id="_1-8-bernoulli-distribution" tabindex="-1"><a class="header-anchor" href="#_1-8-bernoulli-distribution"><span>1.8 Bernoulli Distribution</span></a></h3><p>只有 0 和 1 的情况</p><img src="'+B+'" alt="image-20241027225533474" style="zoom:50%;"><h3 id="_1-9-beta-distribution" tabindex="-1"><a class="header-anchor" href="#_1-9-beta-distribution"><span>1.9 Beta Distribution</span></a></h3><figure><img src="'+C+'" alt="image-20241027232323261" tabindex="0" loading="lazy"><figcaption>image-20241027232323261</figcaption></figure><details class="hint-container details"><summary>Beta分布和 Bernoulli Distribution 的关系</summary><p><strong>Beta分布</strong>并不是伯努利分布的积分，但它确实和伯努利分布密切相关。具体来说，Beta分布通常被用作伯努利分布中参数 ( p ) 的<strong>先验分布</strong>，特别是在<strong>贝叶斯统计</strong>中。</p><ol><li><mark><strong>Beta分布与伯努利分布的关系</strong></mark></li></ol><ul><li><p><strong>伯努利分布</strong>描述了二元随机变量的分布，即事件成功的概率为 ( p )（例如投掷硬币正面朝上的概率），其概率质量函数为：</p><figure><img src="'+M+'" alt="image-20241027234909233" tabindex="0" loading="lazy"><figcaption>image-20241027234909233</figcaption></figure></li><li><p><strong>Beta分布</strong>可以看作是对这个成功概率 ( p ) 的一个概率模型。Beta分布用于表示 ( p ) 的不确定性。也就是说，我们并不直接知道 ( p ) 的值，而是通过 Beta 分布来描述我们对 ( p ) 的信念。</p></li><li><p><strong>贝叶斯更新</strong>：在贝叶斯框架中，如果我们对伯努利分布的参数 ( p ) 进行建模，通常会选择 Beta 分布作为 ( p ) 的先验分布。这样，当我们获得新的数据（成功或失败的观测）时，可以通过更新 Beta 分布的参数来得到 ( p ) 的后验分布。</p></li></ul><ol start="2"><li><mark><strong>为什么使用Beta分布作为先验</strong></mark></li></ol><p>Beta 分布有两个参数 ( \\alpha ) 和 ( \\beta )，它们表示我们在事前对 ( p ) 的成功和失败次数的“虚拟观测”。例如：</p><ul><li>如果我们选择 ( \\alpha = 1 ), ( \\beta = 1 )，那么 Beta 分布是一个均匀分布，表示我们对 ( p ) 没有偏好。</li><li>如果 ( \\alpha ) 很大，而 ( \\beta ) 很小，表示我们认为事件大概率会成功（( p ) 接近 1）。</li><li>反之，( \\alpha ) 很小，( \\beta ) 很大时，表示我们认为事件大概率会失败（( p ) 接近 0）。</li></ul><ol start="3"><li><mark><strong>Beta分布并不代表“伯努利参数的准确性”</strong></mark></li></ol><p>Beta 分布是用来表示我们对 ( p ) 的不确定性的，而不是直接表示 ( p ) 的准确性。但随着数据的增多（比如更多的伯努利实验结果），我们可以更准确地估计 ( p )。具体来说：</p><ul><li>每当我们观测到一次“成功”事件，就增加 ( \\alpha ) 的值。</li><li>每当观测到一次“失败”事件，就增加 ( \\beta ) 的值。</li></ul><p>随着 ( \\alpha ) 和 ( \\beta ) 的增加，Beta 分布会逐渐收缩到一个较窄的范围，从而更精确地描述 ( p ) 的可能值。这可以理解为：<strong>Beta 分布会随着数据增多而对 ( p ) 的估计变得更准确</strong>。</p><ol start="4"><li><mark><strong>总结</strong></mark></li></ol><p>Beta分布和伯努利分布的关系在于，Beta分布可以作为伯努利分布参数 ( p ) 的先验分布。通过贝叶斯更新，我们可以在观测到更多成功或失败事件后，利用 Beta 分布来更新对 ( p ) 的信念，使得对 ( p ) 的估计更加精确。因此，Beta分布反映了我们对 ( p ) 的不确定性，而不直接表示 ( p ) 的准确性。</p></details><h3 id="_1-10-categorical-distribution" tabindex="-1"><a class="header-anchor" href="#_1-10-categorical-distribution"><span>1.10 Categorical Distribution</span></a></h3><p>分类分布（Categorical Distribution）的公式确实涉及概率的表示，但并不是将所有取值的概率相乘，而是通过一个简单的表达式来表明某个特定类别发生的概率。</p><figure><img src="'+A+'" alt="image-20241028000705459" tabindex="0" loading="lazy"><figcaption>image-20241028000705459</figcaption></figure><div class="hint-container info"><p class="hint-container-title">相关信息</p><figure><img src="'+I+'" alt="image-20241028110307514" tabindex="0" loading="lazy"><figcaption>image-20241028110307514</figcaption></figure></div><details class="hint-container details"><summary>例子</summary><p>假设我们有一个骰子，它有6个面（1到6），每个面朝上的概率不同。我们用一个分类分布来描述这个骰子每一面的概率。假设各面的概率如下：</p><p>lambda = [0.1, 0.2, 0.3, 0.15, 0.15, 0.1]</p><p>这个概率向量表示：</p><ul><li>掷出1的概率是0.1</li><li>掷出2的概率是0.2</li><li>掷出3的概率是0.3</li><li>掷出4的概率是0.15</li><li>掷出5的概率是0.15</li><li>掷出6的概率是0.1</li></ul><p><strong>问题：计算掷出“3”的概率</strong></p><p>为了表示我们想要的类别“3”，我们可以用一个独热向量表示，其中只有第三个位置是1，其余位置是0：</p><p>e 3 = [0, 0, 1, 0, 0, 0]</p><p>分类分布公式为：</p><figure><img src="'+T+'" alt="image-20241028000824104" tabindex="0" loading="lazy"><figcaption>image-20241028000824104</figcaption></figure><p>这里的 ( x j ) 是向量 ( e 3 ) 中的元素。因为独热向量的第三个位置为1，其他位置为0，上式会变成：</p><figure><img src="'+j+'" alt="image-20241028000906053" tabindex="0" loading="lazy"><figcaption>image-20241028000906053</figcaption></figure><p>因此，掷出“3”的概率就是0.3。</p><p><strong>总结</strong></p><p>在这个例子中，我们使用分类分布来表示一个不均匀骰子每一面朝上的概率。通过使用独热向量 ( e 3 )，我们可以筛选出掷出3的概率，即 0.3。</p></details><h3 id="_1-11-dirichlet-distribution" tabindex="-1"><a class="header-anchor" href="#_1-11-dirichlet-distribution"><span>1.11 Dirichlet Distribution</span></a></h3>',40)),r(s,{id:"404",data:[{id:"概率密度函数"},{id:"图形解释"},{id:"概率密度函数"},{id:"图示"}]},{title0:t(({value:n,isActive:l})=>a[0]||(a[0]=[e("概率密度函数")])),title1:t(({value:n,isActive:l})=>a[1]||(a[1]=[e("图形解释")])),title2:t(({value:n,isActive:l})=>a[2]||(a[2]=[e("概率密度函数")])),title3:t(({value:n,isActive:l})=>a[3]||(a[3]=[e("图示")])),tab0:t(({value:n,isActive:l})=>a[4]||(a[4]=[i("figure",null,[i("img",{src:L,alt:"image-20241028111502992",tabindex:"0",loading:"lazy"}),i("figcaption",null,"image-20241028111502992")],-1)])),tab1:t(({value:n,isActive:l})=>[a[5]||(a[5]=i("figure",null,[i("img",{src:E,alt:"image-20241028112138008",tabindex:"0",loading:"lazy"}),i("figcaption",null,"image-20241028112138008")],-1)),a[6]||(a[6]=i("ul",null,[i("li",null,[i("p",null,"图中左侧三角形图（a）展示了一个二维情况下的三维空间，表示 λ1,λ2,λ3的所有可能组合。")]),i("li",null,[i("p",null,"子图 (b) 到 (i) 展示了不同参数组合下的狄利克雷分布形状。每个子图标注的参数（如 (0.90, 0.90, 0.90)）表示不同的αk值，显示了在这种条件下概率密度的分布形态。随着参数的增大，分布会更加集中")])],-1)),a[7]||(a[7]=i("h3",{id:"_1-12-univariate-normal-distribution",tabindex:"-1"},[i("a",{class:"header-anchor",href:"#_1-12-univariate-normal-distribution"},[i("span",null,"1.12 Univariate Normal Distribution")])],-1)),a[8]||(a[8]=i("p",null,"又叫正态分布。",-1)),r(s,{id:"432",data:[]})]),tab2:t(({value:n,isActive:l})=>a[9]||(a[9]=[i("figure",null,[i("img",{src:R,alt:"image-20241028113358468",tabindex:"0",loading:"lazy"}),i("figcaption",null,"image-20241028113358468")],-1),i("p",null,"或者又可以写作：",-1),i("figure",null,[i("img",{src:q,alt:"image-20241028113429673",tabindex:"0",loading:"lazy"}),i("figcaption",null,"image-20241028113429673")],-1)])),tab3:t(({value:n,isActive:l})=>a[10]||(a[10]=[i("img",{src:N,alt:"image-20241028113955457",style:{zoom:"50%"}},null,-1),i("ul",null,[i("li",null,[i("p",null,"绿色曲线：均值 μ=−3.4方差 σ2=0.25。")]),i("li",null,[i("p",null,"红色曲线：均值 μ=0方差 σ2=1。")]),i("li",null,[i("p",null,"蓝色曲线：均值 μ=1.5方差 σ2=4.41。")])],-1),i("p",null,"这些曲线显示了不同均值和方差组合下正态分布的形状变化，均值越高曲线向右移动，方差越大曲线越平缓。",-1)])),_:1}),a[22]||(a[22]=i("h3",{id:"_1-13-normal-inverse-gamma-distribution",tabindex:"-1"},[i("a",{class:"header-anchor",href:"#_1-13-normal-inverse-gamma-distribution"},[i("span",null,"1.13 Normal Inverse Gamma Distribution")])],-1)),r(s,{id:"473",data:[{id:"概率密度函数"},{id:"图示"}]},{title0:t(({value:n,isActive:l})=>a[11]||(a[11]=[e("概率密度函数")])),title1:t(({value:n,isActive:l})=>a[12]||(a[12]=[e("图示")])),tab0:t(({value:n,isActive:l})=>a[13]||(a[13]=[i("figure",null,[i("img",{src:F,alt:"image-20241028115816229",tabindex:"0",loading:"lazy"}),i("figcaption",null,"image-20241028115816229")],-1)])),tab1:t(({value:n,isActive:l})=>a[14]||(a[14]=[i("img",{src:V,alt:"image-20241028120312355",style:{zoom:"67%"}},null,-1),i("p",null,"下方的热力图展示了在不同参数组合下正态逆伽马分布的概率密度。每张图上方的括号内容表示具体的参数值组合，例如：",-1),i("ul",null,[i("li",null,"a) 图的参数组合为 (1.0, 1.0, 1.0, 0.0)，表示 α=1，β=1，γ=1，δ=0。"),i("li",null,"b) 图的参数组合为 (0.5, 1.0, 1.0, 0.0)，等等。")],-1),i("p",null,"每张热力图显示了在不同均值 μ 和方差 σ22的情况下概率密度的分布情况。颜色越亮，表示在对应的 μ和 σ2 位置上概率密度越高。",-1)])),_:1}),a[23]||(a[23]=o('<h3 id="_1-14-multivariate-normal-distribution" tabindex="-1"><a class="header-anchor" href="#_1-14-multivariate-normal-distribution"><span>1.14 Multivariate Normal Distribution</span></a></h3><figure><img src="'+G+'" alt="image-20241028120701500" tabindex="0" loading="lazy"><figcaption>image-20241028120701500</figcaption></figure><h3 id="_1-15-normal-inverse-wishart" tabindex="-1"><a class="header-anchor" href="#_1-15-normal-inverse-wishart"><span>1.15 Normal Inverse Wishart</span></a></h3><figure><img src="'+U+'" alt="image-20241028120731059" tabindex="0" loading="lazy"><figcaption>image-20241028120731059</figcaption></figure><figure><img src="'+W+'" alt="image-20241028120747482" tabindex="0" loading="lazy"><figcaption>image-20241028120747482</figcaption></figure><h3 id="_1-16-conjugate-distributions" tabindex="-1"><a class="header-anchor" href="#_1-16-conjugate-distributions"><span>1.16 Conjugate Distributions</span></a></h3><div class="hint-container info"><p class="hint-container-title">常见的共轭分布</p><img src="'+H+'" alt="image-20241028121314041" style="zoom:33%;"></div><ul><li>When we take product of distribution and it’s conjugate, the result has the same form as the conjugate.</li></ul><details class="hint-container details"><summary>详情</summary><p>这句话的意思是，当我们将一个概率分布与它的共轭分布相乘时，所得的结果（即后验分布）仍然具有与共轭分布相同的数学形式。</p><p>在贝叶斯推断中，我们常常会用<strong>先验分布</strong>去描述某个未知参数的初始信念，然后结合<strong>似然函数</strong>（即观测数据的概率分布）来更新我们的信念，得到<strong>后验分布</strong>。如果我们选择的先验分布是似然函数的共轭分布，那么当我们将这个先验分布与似然函数相乘后，得到的后验分布将和先验分布保持相同的形式，只是分布的参数会发生变化。这就是<strong>共轭分布的性质</strong>。</p></details><ul><li><p>Example:</p><p>一个先验分布：我们暂时设做 beta 分布。</p><p>一个似然函数：从伯努利分布中提取，因为 beta 分布是他的共轭分布。这时我们从伯努利分布中得到一些观测函数（他们满足伯努利分布）</p><p>把后验分布：把先验分布与似然函数相乘，得到的后验分布仍然是 Beta 分布的形式。</p><img src="'+O+'" alt="image-20241028123535402" style="zoom:50%;"></li></ul><h3 id="_1-17-importance-of-conjugate-in-bayes-rule" tabindex="-1"><a class="header-anchor" href="#_1-17-importance-of-conjugate-in-bayes-rule"><span>1.17 Importance of Conjugate in Bayes’ Rule</span></a></h3><p>回顾一下贝叶斯定理是什么？</p><ul><li>在贝叶斯公式中，后验概率 Pr(y∣x) 是通过结合似然 Pr(x∣y) 和先验分布 Pr(y)计算得到的。共轭分布的一个重要特性是，当我们将先验分布和似然相乘时，得到的后验分布形式与先验分布相同，这就是“共轭”的含义。</li></ul><img src="'+Q+'" alt="image-20241028145020012" style="zoom:50%;"><details class="hint-container details"><summary>详情</summary><figure><img src="'+Z+'" alt="image-20241028153349691" tabindex="0" loading="lazy"><figcaption>image-20241028153349691</figcaption></figure></details><h3 id="_1-18-maximum-likelihood" tabindex="-1"><a class="header-anchor" href="#_1-18-maximum-likelihood"><span>1.18 Maximum Likelihood</span></a></h3>',16)),r(s,{id:"576",data:[{id:"GPT"}]},{title0:t(({value:n,isActive:l})=>a[15]||(a[15]=[e("GPT")])),tab0:t(({value:n,isActive:l})=>a[16]||(a[16]=[i("p",null,[i("strong",null,"什么是似然（likelihood）？")],-1),i("ul",null,[i("li",null,"似然就是给定参数时，数据出现的可能性。我们的目标是让这个可能性最大化，也就是让模型的参数尽量让数据更“合理”地出现。我们希望找到这样一组参数，让我们观察到的数据在该参数下出现的可能性最高。")],-1),i("p",null,[i("strong",null,"步骤"),e("：")],-1),i("ul",null,[i("li",null,"我们要找到一组参数，使得所有数据同时出现的可能性最大。在数学上，这可以理解为找出每个数据点的出现概率，并把它们的概率值相乘。为了简化这个过程，我们假设数据是独立的（即一个数据点的出现不影响其他数据点的出现），这样就可以直接把每个数据的概率乘起来。")],-1),i("p",null,[i("strong",null,"预测新数据"),e("：")],-1),i("ul",null,[i("li",null,"一旦找到让现有数据可能性最大的参数（最优参数），我们可以使用这个参数来预测新的数据。这就相当于：我们调整好了猜测器的设置，现在它可以更好地预测新情况。")],-1)])),_:1}),a[24]||(a[24]=o('<h3 id="_1-19-maximum-a-posteriori-map" tabindex="-1"><a class="header-anchor" href="#_1-19-maximum-a-posteriori-map"><span>1.19 Maximum a posteriori (MAP)</span></a></h3><p>也假设数据都是独立的。</p><img src="'+S+'" alt="image-20241028163353111" style="zoom:67%;"><h3 id="_1-20-bayesian-approach" tabindex="-1"><a class="header-anchor" href="#_1-20-bayesian-approach"><span>1.20 Bayesian Approach</span></a></h3><h4 id="_1-fitting" tabindex="-1"><a class="header-anchor" href="#_1-fitting"><span>1. Fitting</span></a></h4><p>Compute the posterior distribution over possible parameter values using Bayes’ rule:</p><figure><img src="'+K+'" alt="image-20241028164632595" tabindex="0" loading="lazy"><figcaption>image-20241028164632595</figcaption></figure><p>贝叶斯方法的一个核心思想是通过已知的数据来更新关于参数 θ 的知识，得到其后验分布。</p><p>这里：</p><ul><li>P(θ∣x1...I)表示在给定数据 x1...I 后，对参数 θ的后验分布。</li><li>P(xi∣θ)是似然函数，表示在参数 θ 下生成数据点 xi的概率。</li><li>P(θ)是先验分布，表示我们在观察到数据之前对参数 θ 的信念。</li><li>P(x1...I)是数据的边缘似然，用于归一化使得后验分布的总概率为1。</li></ul><p><strong>原则</strong>：贝叶斯方法不选择单一的最优参数，而是保留所有可能的参数值，并考虑每种情况的概率。这种方法能够捕捉到所有可能的解释，而不是仅仅选择一个。</p><div class="hint-container note"><p class="hint-container-title">注</p><p><strong>为何P(θ)是先验分布？它与后验分布函数形式一样吗？</strong></p><p>在贝叶斯统计中，先验分布 ( P(\\theta) ) 和后验分布 ( P(θ | x_{1...I}) ) 是两个不同的概念，尽管它们的形式可以在某些情况下相似。我们来详细探讨一下：</p><ol><li>先验分布 ( P(θ) )</li></ol><p>先验分布 ( P(θ) ) 表示我们在观察到数据之前，对参数 ( θ ) 的信念。它反映了在没有任何数据的情况下，关于 ( θ) 的不确定性。选择先验分布通常依赖于先验知识、经验或假设。在贝叶斯分析中，先验分布的选择对结果有很大影响，特别是在数据量少的情况下。</p><ol start="2"><li>后验分布 ( P(θ | x_{1...I}) )</li></ol><p>后验分布 ( P(θ | x_{1...I}) ) 表示在观察了数据 ( x_{1...I} ) 后，我们对参数 ( θ ) 的更新后的信念。根据贝叶斯定理，后验分布的计算公式为：</p><img src="'+X+'" alt="image-20241028165938691" style="zoom:50%;"><p>这里：</p><ul><li>( P(x_{1...I} |θ) ) 是似然函数，表示在给定参数 ( θ ) 的条件下观测到数据 ( x_{1...I} ) 的概率。</li><li>( P(x_{1...I}) ) 是边缘似然，用于归一化，使得后验分布的总概率为 1。</li></ul><p>因此，后验分布是基于先验分布和数据的似然函数共同决定的。</p><ol start="3"><li>先验分布与后验分布的形式关系</li></ol><p>在某些情况下，先验分布和后验分布的形式可以相似。例如，当我们选择一个共轭先验（conjugate prior）时，后验分布会与先验分布具有相同的分布族形式。共轭先验是一种特殊的先验分布形式，使得后验分布的形式保持与先验分布相同，这在计算上非常方便。例如：</p><ul><li>对于二项分布的参数（如伯努利分布中的成功概率），选择 Beta 分布作为先验分布。更新后的后验分布仍然是 Beta 分布。</li><li>对于正态分布的均值，选择正态分布作为先验分布，后验分布也保持正态分布形式。</li></ul><p>然而，<strong>一般情况下，先验分布和后验分布的形式并不一定相同</strong>。在没有选择共轭先验的情况下，后验分布可能是复杂的、非标准的分布，甚至可能需要使用数值方法（如马尔可夫链蒙特卡洛方法）来估计。</p></div><h4 id="_2-predictive-density" tabindex="-1"><a class="header-anchor" href="#_2-predictive-density"><span>2. Predictive Density</span></a></h4><p>• Each possible parameter value makes a prediction</p><p>• Some parameters more probable than others</p><img src="'+Y+'" alt="image-20241028170307868" style="zoom:80%;"><p>Make a prediction that is an infinite weighted sum (integral) of the predictions for each parameter value, where weights are the probabilities.</p><p>这句话是说，预测密度是一个加权平均，其中每个可能的参数值 θ对预测 x∗的贡献根据其后验概率 P(θ∣x1...I)进行加权。</p><h3 id="_1-21-三种预测密度的方法" tabindex="-1"><a class="header-anchor" href="#_1-21-三种预测密度的方法"><span>1.21 三种预测密度的方法</span></a></h3><p><strong>1. Maximum likelihood:</strong></p><p>Evaluate new data point under probability distribution with ML parameters</p><figure><img src="'+g+'" alt="image-20241028172139566" tabindex="0" loading="lazy"><figcaption>image-20241028172139566</figcaption></figure><p><strong>2. Maximum a posteriori:</strong></p><p>Evaluate new data point under probability distribution with MAP parameters</p><figure><img src="'+p+'" alt="image-20241028172155352" tabindex="0" loading="lazy"><figcaption>image-20241028172155352</figcaption></figure><p><strong>3. Bayesian:</strong></p><p>Calculate weighted sum of predictions from all possible values of parameters</p><figure><img src="'+m+'" alt="image-20241028172212701" tabindex="0" loading="lazy"><figcaption>image-20241028172212701</figcaption></figure><h2 id="lecture-8-review-question" tabindex="-1"><a class="header-anchor" href="#lecture-8-review-question"><span>Lecture 8 Review Question</span></a></h2><h3 id="_1-what-are-random-variables-how-many-types-of-random-variables" tabindex="-1"><a class="header-anchor" href="#_1-what-are-random-variables-how-many-types-of-random-variables"><span>1. <strong>What are random variables? How many types of random variables?</strong></span></a></h3>',30)),r(s,{id:"801",data:[{id:"GPT"},{id:"PPT"}]},{title0:t(({value:n,isActive:l})=>a[17]||(a[17]=[e("GPT")])),title1:t(({value:n,isActive:l})=>a[18]||(a[18]=[e("PPT")])),tab0:t(({value:n,isActive:l})=>a[19]||(a[19]=[i("p",null,[e("A "),i("strong",null,"random variable"),e(" is a variable whose possible values are outcomes of a random phenomenon. It's a way to quantify uncertainty by assigning numerical values to different outcomes of a random process.")],-1),i("ol",null,[i("li",null,[i("p",null,[i("strong",null,"Discrete Random Variables")]),i("ul",null,[i("li",null,[i("strong",null,"Definition"),e(": A discrete random variable takes on a countable number of distinct values. These values are often integers, such as 0, 1, 2, etc., but can also be any distinct set of outcomes.")]),i("li",null,[i("strong",null,"Probability Distribution"),e(": Discrete random variables have a probability mass function (PMF), which provides the probability of each possible outcome.")])])]),i("li",null,[i("p",null,[i("strong",null,"Continuous Random Variables")]),i("ul",null,[i("li",null,[i("strong",null,"Definition"),e(": A continuous random variable can take on an infinite number of values within a given range. These values are typically real numbers and can vary continuously without jumps.")]),i("li",null,[i("strong",null,"Probability Distribution"),e(": Continuous random variables have a probability density function (PDF), which describes the probability of the variable falling within a specific range of values. For continuous random variables, the probability of taking any exact value is zero, but we can calculate the probability over an interval.")])])])],-1)])),tab1:t(({value:n,isActive:l})=>a[20]||(a[20]=[i("ul",null,[i("li",null,"May be result of experiment (flipping a coin) or a real world measurements (measuring temperature)"),i("li",null,"If observe several instances of x we get different values."),i("li",null,[e("A random variable x denotes a quantity that is "),i("strong",null,"uncertain")]),i("li",null,[e("Some values occur more than others and this information is captured by a "),i("strong",null,"probability distribution")])],-1),i("figure",null,[i("img",{src:J,alt:"image-20241030151834269",tabindex:"0",loading:"lazy"}),i("figcaption",null,"image-20241030151834269")],-1)])),_:1}),a[25]||(a[25]=o('<h3 id="_2-the-definition-of-conditional-probability" tabindex="-1"><a class="header-anchor" href="#_2-the-definition-of-conditional-probability"><span>2. The definition of Conditional Probability</span></a></h3><p>给定 y=y1 的条件概率 P(x∣y = y1)是指在 y 固定为 y1 时，变量 x 取不同结果的相对倾向。</p><p>Conditional probability of x given that y=y1 is relative propensity of variable x to take different outcomes given that y is fixed to be equal to y1 .</p><p>In another way …</p><p>The conditional probability ( P(x | y = y_1) ) represents the probability of event ( x ) occurring given that event ( y = y_1 ) has already occurred.</p><img src="'+$+'" alt="image-20241028201747950" style="zoom:50%;"><h3 id="_3-the-definition-of-bayesian-rule" tabindex="-1"><a class="header-anchor" href="#_3-the-definition-of-bayesian-rule"><span>3. <strong>The definition of Bayesian Rule</strong></span></a></h3><p>首先我们得到：</p><img src="'+ii+'" alt="image-20241028202519998" style="zoom:50%;"><p>然后我们可以进行变换，得到 Bayes’ Rule：</p><img src="'+ai+'" alt="image-20241028202639040" style="zoom:50%;"><p>解析 Bayes‘ Rule：</p><img src="'+ei+'" alt="image-20241028194942381" style="zoom:50%;"><h3 id="_4-three-fitting-probability-distributions" tabindex="-1"><a class="header-anchor" href="#_4-three-fitting-probability-distributions"><span>4. Three Fitting probability distributions</span></a></h3><p><strong>1. Maximum likelihood:</strong></p><p>Evaluate new data point x* under probability distribution with ML parameters</p><figure><img src="'+g+'" alt="image-20241028172139566" tabindex="0" loading="lazy"><figcaption>image-20241028172139566</figcaption></figure><p><strong>2. Maximum a posteriori:</strong></p><p>Evaluate new data point x* under probability distribution with MAP parameters</p><figure><img src="'+p+'" alt="image-20241028172155352" tabindex="0" loading="lazy"><figcaption>image-20241028172155352</figcaption></figure><p><strong>3. Bayesian:</strong></p><p>Calculate weighted sum of predictions from all possible values of parameters</p><figure><img src="'+m+'" alt="image-20241028172212701" tabindex="0" loading="lazy"><figcaption>image-20241028172212701</figcaption></figure><h3 id="_5-默写一些分布" tabindex="-1"><a class="header-anchor" href="#_5-默写一些分布"><span>5. 默写一些分布</span></a></h3><h4 id="_5-1-bernoulli-likelihood" tabindex="-1"><a class="header-anchor" href="#_5-1-bernoulli-likelihood"><span>5.1 Bernoulli likelihood</span></a></h4><p>Probability Mass Function (PMF)：</p><img src="'+ti+'" alt="image-20241028211328059" style="zoom:33%;"><p>写作：</p><img src="'+ni+'" alt="image-20241028211356956" style="zoom:50%;"><h4 id="_5-2-categorical-distribution" tabindex="-1"><a class="header-anchor" href="#_5-2-categorical-distribution"><span>5.2 Categorical Distribution</span></a></h4><p>The <strong>Categorical distribution</strong> generalizes the Bernoulli distribution to handle more than two possible outcomes. It describes the probabilities of each category in a set of k categories.</p><p>Probability Mass Function：</p><img src="'+li+'" alt="image-20241028211906427" style="zoom:33%;"><p>​ <img src="'+si+'" alt="image-20241028212025558" style="zoom:50%;"></p><figure><img src="'+ri+'" alt="image-20241028213137862" tabindex="0" loading="lazy"><figcaption>image-20241028213137862</figcaption></figure><h4 id="_5-3-univariate-normal-distribution" tabindex="-1"><a class="header-anchor" href="#_5-3-univariate-normal-distribution"><span>5.3 Univariate Normal Distribution</span></a></h4><p>Probability Density Function (PDF)：</p><img src="'+oi+'" alt="image-20241028213430962" style="zoom:33%;"><img src="'+gi+'" alt="image-20241028213441225" style="zoom:50%;"><h4 id="_5-4-multivariate-normal-distribution" tabindex="-1"><a class="header-anchor" href="#_5-4-multivariate-normal-distribution"><span>5.4 Multivariate Normal Distribution</span></a></h4><p>The <strong>Multivariate Normal distribution</strong> generalizes the univariate normal distribution to multiple dimensions. It is defined by a mean vector μ\\muμ and a covariance matrix Σ.</p><figure><img src="'+pi+'" alt="image-20241028214510342" tabindex="0" loading="lazy"><figcaption>image-20241028214510342</figcaption></figure><ul><li><p>μ is the d-dimensional mean vector.</p></li><li><p>Σ is the d×d covariance matrix, which must be positive definite.</p></li></ul><p>Multivariate normal distribution describes multiple continuous variables. Takes 2 parameters</p><h2 id="lecture-9" tabindex="-1"><a class="header-anchor" href="#lecture-9"><span>Lecture 9</span></a></h2><p>这几页幻灯片介绍了视觉世界中的不确定性、计算机视觉的目标，以及解决这个问题的模型设计。</p><ol><li><p><strong>视觉世界的模糊性</strong>：视觉测量往往无法唯一确定世界状态，因为观测数据（如图像）可能含有噪声或内在的不确定性。这导致同一个观测值可能对应多个可能的世界状态。为了应对这种模糊性，最好的方法是计算观测数据下可能的世界状态的后验概率分布。</p></li><li><p><strong>计算机视觉的目标</strong>：计算机视觉的目标是基于观测值（如图像数据）输出一个关于世界状态的概率分布。由于直接求解可能非常复杂，通常可以使用近似方法或选择最大后验（MAP）估计来代表最可能的世界状态。</p></li><li><p><strong>解决方案的组成部分</strong>：要解决这个问题，需要设计一个模型来将视觉数据和世界状态联系起来，并通过学习算法拟合模型参数。最后通过推理算法在新的观测数据下估计可能的世界状态的概率。</p></li><li><p><mark><strong>模型的类型</strong>：主要有两类模型：</mark></p><ul><li>基于观测数据预测世界状态的模型 ( Pr(w|x) )。Discriminative</li><li>基于世界状态预测观测数据的模型 ( Pr(x|w) )。Generative</li></ul></li><li><p>不同的世界状态的输出对应的模型类型：</p><figure><img src="'+mi+'" alt="image-20241029120131551" tabindex="0" loading="lazy"><figcaption>image-20241029120131551</figcaption></figure></li></ol><h3 id="_1-model" tabindex="-1"><a class="header-anchor" href="#_1-model"><span>1. Model</span></a></h3><h4 id="_1-1-discriminative-—-model-contingency-of-the-world-on-the-data" tabindex="-1"><a class="header-anchor" href="#_1-1-discriminative-—-model-contingency-of-the-world-on-the-data"><span>1.1 Discriminative — Model contingency of the world on the data</span></a></h4><p>通过观测数据观测世界状态的概率 Pr(w|x)</p><p><strong>判别模型</strong>旨在直接估计给定观测数据 x 时，某一特定世界状态 w 发生的概率Pr(w∣x)。</p><ol><li><p>Choose an appropriate form for Pr(<strong>w</strong>)</p></li><li><p>Make parameters a function of <strong>x</strong></p></li><li><p>Function takes parameters θ that define its shape</p></li></ol><p>学习算法：（Learning algorithm） learn parameters θ from training data.</p><p>推理算法：（Inference algorithm）在模型学习完成后，直接计算 Pr(w∣x)，即在给定观测数据时预测特定状态的可能性。just evaluate Pr(w|x)。</p><h4 id="_1-2-generative-—-model-contingency-of-data-on-world" tabindex="-1"><a class="header-anchor" href="#_1-2-generative-—-model-contingency-of-data-on-world"><span>1.2 Generative — Model contingency of data on world</span></a></h4><p>通过观测世界状态观测数据的概率 Pr(x|w)。</p><p><strong>生成模型</strong>不直接估计 Pr(w∣x)，而是通过先估计在某个世界状态 w 下观测数据 x 的概率 Pr(x∣w)，再使用贝叶斯公式计算 Pr(w∣x)。</p><ol><li>Choose an appropriate form for Pr(<strong>x</strong>)</li><li>Make parameters a function of <strong>w</strong></li><li>Function takes parameters θ that define its shape</li></ol><p>Learning algorithm: learn parameters θ from training data.</p><p>Inference algorithm: Define prior Pr(<strong>w</strong>) and then compute Pr(<strong>w</strong>|<strong>x</strong>) using Bayes’ rule.</p><img src="'+ui+'" alt="image-20241029111612208" style="zoom:50%;"><h3 id="_2-linear-regression" tabindex="-1"><a class="header-anchor" href="#_2-linear-regression"><span>2. Linear Regression</span></a></h3><p>Consider:</p><ul><li>we make a univariate(单变量的) continuous measurement <strong>x</strong></li><li>use this to predict a univariate continuous state <strong>w</strong></li></ul><p><strong>regression as world state is continuous</strong></p><h4 id="_2-1-discriminative" tabindex="-1"><a class="header-anchor" href="#_2-1-discriminative"><span>2.1 Discriminative</span></a></h4><p>这些幻灯片讲解了如何在判别模型中使用线性回归来建模和预测变量 w 与观测变量 x 之间的关系。</p><ul><li><p>How to model Pr(<strong>w</strong>|<strong>x</strong>)?</p><ul><li>Choose an appropriate form for Pr(<strong>w</strong>)</li><li>Make parameters a function of <strong>x</strong></li><li>Function takes parameters <strong></strong> that define its shape</li></ul></li><li><p><strong>模型选择</strong>：选择一个适当的概率分布来描述 Pr(w∣x)。在这里，我们选择了正态分布 (Normal distribution) 作为分布形式。</p><ul><li>Choose normal distribution over w</li><li>Assume that the mean value μ of w is a linear function of x, i.e. μ = ϕ0 + ϕ1 .</li><li>The variance σ2 is assumed to be constant and not varying with x.</li></ul></li><li><p>整个函数可以和简写成：</p><figure><img src="'+di+'" alt="image-20241029121022435" tabindex="0" loading="lazy"><figcaption>image-20241029121022435</figcaption></figure></li><li><p>Learning algorithm</p><p>E.g. MAP</p><img src="'+ci+'" alt="image-20241029121122770" style="zoom:50%;"></li><li><p>Inference algorithm: just evaluate Pr(<strong>w</strong>|**x,**𝜃) for new data <strong>x</strong></p></li></ul><h3 id="_3-logistics-regression" tabindex="-1"><a class="header-anchor" href="#_3-logistics-regression"><span>3. Logistics regression</span></a></h3><p>离散的，同上所述。</p><h4 id="_3-1-discriminative" tabindex="-1"><a class="header-anchor" href="#_3-1-discriminative"><span>3.1 Discriminative</span></a></h4><p>这些页面详细说明了生成模型 (Generative Model) 的构建方法及其运作原理。</p><ol><li><strong>如何构建 ( P(x|w) )</strong></li></ol><ul><li>选择一个适当的形式：在这里，为了模型 ( P(x|w) )，我们选择了一个高斯分布。</li><li>使参数成为 ( w ) 的函数：因为我们希望数据 ( x ) 的分布依赖于分类 ( w ) 的值（例如，二分类情况下 ( w ) 为 0 或 1）。</li><li>定义模型的参数：模型有参数θ（比如 μ0、 μ1、 σ0 和 σ1），这些参数决定了 ( P(x|w) ) 的形状。</li></ul><ol start="2"><li><strong>学习算法</strong></li></ol><p>在学习算法部分，通过使用训练数据集 { ( x_i ), ( w_i ) } 来估计参数 (θ )。这些参数被拟合到数据中，使得对于已知 ( w ) 的情况下 ( x ) 的概率最大化。</p><ol start="3"><li><strong>推理算法</strong></li></ol><p>推理算法的目标是通过贝叶斯规则计算后验分布 ( P(w|x) )。计算后验分布时，我们会定义 ( P(w) ) 的先验分布（这里使用伯努利分布表示二分类的先验概率），然后通过贝叶斯公式：</p><figure><img src="'+hi+'" alt="image-20241029123211634" tabindex="0" loading="lazy"><figcaption>image-20241029123211634</figcaption></figure><p>在离散情况下，这可以简化为求和形式。</p><p><strong>生成模型中的例子</strong></p><p>图中展示了生成模型的典型情况：两类 ( w = 0 ) 和 ( w = 1 ) 的数据分布（分别用蓝色和红色表示）。不同类别的 ( x ) 分布在不同的均值 μ0 和 μ1周围，并具有不同的方差 σ0 和 σ1。</p><h3 id="_4-review-questions" tabindex="-1"><a class="header-anchor" href="#_4-review-questions"><span>4. Review Questions</span></a></h3><figure><img src="'+bi+'" alt="image-20241029124224566" tabindex="0" loading="lazy"><figcaption>image-20241029124224566</figcaption></figure><h4 id="_4-1" tabindex="-1"><a class="header-anchor" href="#_4-1"><span>4.1</span></a></h4><p>这张图表对比了 <strong>线性回归</strong> 和 <strong>逻辑回归</strong> 的主要区别。以下是总结和补充：</p><ol><li><mark><strong>响应变量的类型</strong></mark></li></ol><ul><li><strong>线性回归</strong>：适用于 <strong>连续值</strong> 响应变量，例如价格、高度、年龄、距离等。</li><li><strong>逻辑回归</strong>：适用于 <strong>分类值</strong> 响应变量，例如二元分类 (yes/no, win/loss)。</li></ul><ol start="2"><li><mark><strong>拟合方法</strong></mark></li></ol><ul><li><strong>线性回归</strong>：使用 <strong>最小二乘法</strong> 来找到最佳拟合直线，以最小化预测值和实际值之间的误差。</li><li><strong>逻辑回归</strong>：使用 <strong>最大似然估计</strong> 来找到最佳参数，从而最大化观测数据的概率。</li></ul><ol start="3"><li><mark><strong>监督学习类型</strong></mark></li></ol><ul><li><strong>线性回归</strong>：用于 <strong>回归任务</strong>，目标是预测一个连续的数值。</li><li><strong>逻辑回归</strong>：用于 <strong>分类任务</strong>，目标是预测类别标签。</li></ul><ol start="4"><li><mark><strong>输出</strong></mark></li></ol><ul><li><strong>线性回归</strong>：直接预测数值输出，如 $150、14 inches、2 months、1.23 miles。</li><li><strong>逻辑回归</strong>：输出的是属于某个类别的 <strong>概率值</strong>，如 40.3% 或 90%，并根据概率进行分类</li></ul>',94))])}const wi=u(fi,[["render",_i],["__file","ppt_89.html.vue"]]),ki=JSON.parse(`{"path":"/zh/Computer_Vision/ppt_89.html","title":"计算机视觉 L 8 - L 9","lang":"zh-CN","frontmatter":{"title":"计算机视觉 L 8 - L 9","icon":"python","date":"2024-10-27T20:10:05.000Z","author":"XiaoXianYue","isOriginal":true,"category":["大三上","计算机视觉"],"tag":["大三上","计算机视觉"],"sticky":false,"star":false,"article":true,"timeline":true,"image":false,"navbar":true,"sidebarIcon":true,"headerDepth":5,"lastUpdated":true,"editLink":false,"backToTop":true,"toc":true,"feed":false,"seo":false,"head":[]},"headers":[{"level":2,"title":"Lecture 8","slug":"lecture-8","link":"#lecture-8","children":[{"level":3,"title":"1.1 Random Variable","slug":"_1-1-random-variable","link":"#_1-1-random-variable","children":[]},{"level":3,"title":"1.2 Joint Probability","slug":"_1-2-joint-probability","link":"#_1-2-joint-probability","children":[]},{"level":3,"title":"1.3 Marginalization","slug":"_1-3-marginalization","link":"#_1-3-marginalization","children":[]},{"level":3,"title":"1.4 Conditional Probability","slug":"_1-4-conditional-probability","link":"#_1-4-conditional-probability","children":[]},{"level":3,"title":"1.5 Bayes' Rule","slug":"_1-5-bayes-rule","link":"#_1-5-bayes-rule","children":[]},{"level":3,"title":"1.6 Independence","slug":"_1-6-independence","link":"#_1-6-independence","children":[]},{"level":3,"title":"1.7 Expectation","slug":"_1-7-expectation","link":"#_1-7-expectation","children":[]},{"level":3,"title":"1.8 Bernoulli Distribution","slug":"_1-8-bernoulli-distribution","link":"#_1-8-bernoulli-distribution","children":[]},{"level":3,"title":"1.9 Beta Distribution","slug":"_1-9-beta-distribution","link":"#_1-9-beta-distribution","children":[]},{"level":3,"title":"1.10 Categorical Distribution","slug":"_1-10-categorical-distribution","link":"#_1-10-categorical-distribution","children":[]},{"level":3,"title":"1.11 Dirichlet Distribution","slug":"_1-11-dirichlet-distribution","link":"#_1-11-dirichlet-distribution","children":[]},{"level":3,"title":"1.13 Normal Inverse Gamma Distribution","slug":"_1-13-normal-inverse-gamma-distribution","link":"#_1-13-normal-inverse-gamma-distribution","children":[]},{"level":3,"title":"1.14 Multivariate Normal Distribution","slug":"_1-14-multivariate-normal-distribution","link":"#_1-14-multivariate-normal-distribution","children":[]},{"level":3,"title":"1.15 Normal Inverse Wishart","slug":"_1-15-normal-inverse-wishart","link":"#_1-15-normal-inverse-wishart","children":[]},{"level":3,"title":"1.16 Conjugate Distributions","slug":"_1-16-conjugate-distributions","link":"#_1-16-conjugate-distributions","children":[]},{"level":3,"title":"1.17 Importance of Conjugate in Bayes’ Rule","slug":"_1-17-importance-of-conjugate-in-bayes-rule","link":"#_1-17-importance-of-conjugate-in-bayes-rule","children":[]},{"level":3,"title":"1.18 Maximum Likelihood","slug":"_1-18-maximum-likelihood","link":"#_1-18-maximum-likelihood","children":[]},{"level":3,"title":"1.19 Maximum a posteriori (MAP)","slug":"_1-19-maximum-a-posteriori-map","link":"#_1-19-maximum-a-posteriori-map","children":[]},{"level":3,"title":"1.20 Bayesian Approach","slug":"_1-20-bayesian-approach","link":"#_1-20-bayesian-approach","children":[{"level":4,"title":"1. Fitting","slug":"_1-fitting","link":"#_1-fitting","children":[]},{"level":4,"title":"2. Predictive Density","slug":"_2-predictive-density","link":"#_2-predictive-density","children":[]}]},{"level":3,"title":"1.21 三种预测密度的方法","slug":"_1-21-三种预测密度的方法","link":"#_1-21-三种预测密度的方法","children":[]}]},{"level":2,"title":"Lecture 8 Review Question","slug":"lecture-8-review-question","link":"#lecture-8-review-question","children":[{"level":3,"title":"1. What are random variables? How many types of random variables?","slug":"_1-what-are-random-variables-how-many-types-of-random-variables","link":"#_1-what-are-random-variables-how-many-types-of-random-variables","children":[]},{"level":3,"title":"2. The definition of Conditional Probability","slug":"_2-the-definition-of-conditional-probability","link":"#_2-the-definition-of-conditional-probability","children":[]},{"level":3,"title":"3. The definition of Bayesian Rule","slug":"_3-the-definition-of-bayesian-rule","link":"#_3-the-definition-of-bayesian-rule","children":[]},{"level":3,"title":"4. Three Fitting probability distributions","slug":"_4-three-fitting-probability-distributions","link":"#_4-three-fitting-probability-distributions","children":[]},{"level":3,"title":"5. 默写一些分布","slug":"_5-默写一些分布","link":"#_5-默写一些分布","children":[{"level":4,"title":"5.1 Bernoulli likelihood","slug":"_5-1-bernoulli-likelihood","link":"#_5-1-bernoulli-likelihood","children":[]},{"level":4,"title":"5.2 Categorical Distribution","slug":"_5-2-categorical-distribution","link":"#_5-2-categorical-distribution","children":[]},{"level":4,"title":"5.3 Univariate Normal Distribution","slug":"_5-3-univariate-normal-distribution","link":"#_5-3-univariate-normal-distribution","children":[]},{"level":4,"title":"5.4 Multivariate Normal Distribution","slug":"_5-4-multivariate-normal-distribution","link":"#_5-4-multivariate-normal-distribution","children":[]}]}]},{"level":2,"title":"Lecture 9","slug":"lecture-9","link":"#lecture-9","children":[{"level":3,"title":"1. Model","slug":"_1-model","link":"#_1-model","children":[{"level":4,"title":"1.1 Discriminative — Model contingency of the world on the data","slug":"_1-1-discriminative-—-model-contingency-of-the-world-on-the-data","link":"#_1-1-discriminative-—-model-contingency-of-the-world-on-the-data","children":[]},{"level":4,"title":"1.2 Generative — Model contingency of data on world","slug":"_1-2-generative-—-model-contingency-of-data-on-world","link":"#_1-2-generative-—-model-contingency-of-data-on-world","children":[]}]},{"level":3,"title":"2. Linear Regression","slug":"_2-linear-regression","link":"#_2-linear-regression","children":[{"level":4,"title":"2.1 Discriminative","slug":"_2-1-discriminative","link":"#_2-1-discriminative","children":[]}]},{"level":3,"title":"3. Logistics regression","slug":"_3-logistics-regression","link":"#_3-logistics-regression","children":[{"level":4,"title":"3.1 Discriminative","slug":"_3-1-discriminative","link":"#_3-1-discriminative","children":[]}]},{"level":3,"title":"4. Review Questions","slug":"_4-review-questions","link":"#_4-review-questions","children":[{"level":4,"title":"4.1","slug":"_4-1","link":"#_4-1","children":[]}]}]}],"git":{"createdTime":1730256290000,"updatedTime":1730679093000,"contributors":[{"name":"Xiaoxianyue","email":"2310219843@qq.com","commits":1}]},"readingTime":{"minutes":21.49,"words":6447},"filePathRelative":"zh/Computer_Vision/ppt_89.md","localizedDate":"2024年10月27日"}`);export{wi as comp,ki as data};
