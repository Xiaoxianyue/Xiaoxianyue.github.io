import{_ as p}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as u,b as o,d as r,e as t,a as e,r as g,o as m,f as i}from"./app-OnM_VJdk.js";const c="data:image/png;base64,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",d="/assets/image-20241020233457873-CCPPAtG1.png",h="/assets/image-20241021171102863-Dzz3uBI9.png",f="/assets/image-20241021165845767-CQszbZCF.png",v="/assets/image-20241021170013570-BRCXOSH9.png",y="/assets/image-20241025175528872-BYCZseS9.png",b="/assets/image-20241025180215800-CH07NYiC.png",x="/assets/image-20241025180237444-BtYPkPgU.png",_="/assets/image-20241025203156336-rjc06SC3.png",z="/assets/image-20241025203217842-Ccyt4cAa.png",k="/assets/image-20241025210350521-CSFs4494.png",w="/assets/image-20241025210405715-BNj3D2ZM.png",S="/assets/image-20241025210635309-Bqjot7et.png",T="/assets/image-20241025210417636-CV-qF4T5.png",A="/assets/image-20241025214001835-Df9uV3lk.png",C="/assets/image-20241025214316084-fRgyVcfh.png",N="/assets/image-20241026200417775-dZCnbznG.png",X="/assets/image-20241026201504144-Cq2QzfVn.png",E="/assets/image-20241026201609469-tQBMtgUK.png",G="/assets/image-20241026201921639-D9eBldj7.png",M="/assets/image-20241026202132232-DZE8avvj.png",U="/assets/image-20241026203634591-B5GaVH3C.png",D="/assets/image-20241026212256704-BXGCe_pc.png",F="/assets/image-20241026212752131-Buv1PJde.png",H="/assets/image-20241026213551358-B6dA1Ff7.png",I="/assets/image-20241027000432976-BEjZgLXe.png",V="/assets/image-20241027000354061-BgJ1Q1IT.png",B="/assets/image-20241027000414662-DpkxywnX.png",K="/assets/image-20241026234929476-1kmguB3n.png",L="/assets/image-20241027003140666-CdJla_S-.png",O="/assets/image-20241027004515674-DNROUt9H.png",R="/assets/image-20241027002435332-OCMht0yx.png",q="/assets/image-20241027102108511-DvvbkMo1.png",Q="/assets/image-20241027105753823-D9yclVfy.png",Z="/assets/image-20241027111558260-cWHYJpfw.png",P="/assets/image-20241027112710140-BZ_WuNSt.png",Y={};function J(W,l){const n=g("Tabs");return m(),u("div",null,[l[30]||(l[30]=o('<h2 id="_1-lecture-5-affine-transformations" tabindex="-1"><a class="header-anchor" href="#_1-lecture-5-affine-transformations"><span>1. Lecture 5 : Affine Transformations</span></a></h2><ul><li><p>Affine transformations are combinations of arbitrary (4-DOF) linear transformations（线性变换）, and translations（平移）</p></li><li><p>Properties of affine transformations:</p><p>• origin does not necessarily map to origin</p><p>• lines map to lines</p><p>• parallel lines map to parallel lines</p><p>• ratios are preserved</p><p>• compositions of affine transforms are also affine transforms</p></li></ul><p><mark>计算公式：</mark></p><p>​ <strong>T(x) = xA + b</strong></p><img src="'+c+'" alt="image-20241020233033375" style="zoom:67%;"><p>![image-20241020233422399](./ppt 567.assets/image-20241020233422399.png)</p><p><mark>例子：</mark></p><img src="'+d+'" alt="image-20241020233457873" style="zoom:50%;"><h2 id="_2-lecture-6" tabindex="-1"><a class="header-anchor" href="#_2-lecture-6"><span>2. Lecture 6</span></a></h2><img src="'+h+'" alt="image-20241021171102863" style="zoom:67%;"><h3 id="_2-1-what-is-interest-point" tabindex="-1"><a class="header-anchor" href="#_2-1-what-is-interest-point"><span>2.1 What is Interest Point?</span></a></h3><ul><li><p>选择 兴趣点 的原则:</p><ul><li>Repeatability : The same feature can be found in several images despite geometric and photometric transformations.</li><li>Saliency : Each feature is distinctive</li><li>Compactness and efficiency : Many fewer features than image pixels</li><li>Locality : A feature occupies a relatively small area of the image; robust to clutter and occlusion</li></ul></li></ul><h3 id="_2-2-what-is-harris-corner-how-to-get-it" tabindex="-1"><a class="header-anchor" href="#_2-2-what-is-harris-corner-how-to-get-it"><span>2.2 What is Harris Corner? How to get it?</span></a></h3><h4 id="_1-gpt" tabindex="-1"><a class="header-anchor" href="#_1-gpt"><span>1. GPT</span></a></h4><p><mark><strong>什么是 Harris 角点检测？</strong></mark></p><p>Harris 角点检测是一种在计算机视觉中用于检测图像中<strong>角点</strong>或<strong>特征点</strong>的算法。角点是图像中在两个方向上都有显著强度变化的区域，这使它们成为图像匹配、跟踪和物体识别等任务的理想特征。</p><p>Harris 角点检测的基本思想是：在角点周围的区域中，无论向哪个方向移动观察窗口，都会导致<strong>较大的强度变化</strong>。相对而言，边缘上的强度变化只在一个方向上显著，而在平坦区域中，几乎没有强度变化。</p><p><mark><strong>关键点：</strong></mark></p><ol><li><strong>角点</strong>是图像梯度在两个方向（x 和 y）上都有显著变化的区域。</li><li><strong>边缘</strong>在一个方向上有显著变化。</li><li><strong>平坦区域</strong>几乎没有强度变化。</li></ol><p><mark><strong>如何获得 Harris 角点？</strong></mark></p><p>Harris 角点检测算法的步骤可以总结如下：</p><ol><li><p><strong>计算图像梯度</strong>：通过通常使用 Sobel 算子，计算图像的<strong>水平</strong>和<strong>垂直梯度</strong>，得到两个梯度图像：</p><ul><li>( I_x )（x 方向的梯度）</li><li>( I_y )（y 方向的梯度）</li></ul></li><li><p><strong>计算结构张量矩阵</strong>：对于图像中的每个像素，基于图像梯度构建一个<strong>结构张量矩阵</strong>： ![image-20241027170623844](./ppt 567.assets/image-20241027170623844.png) 这个矩阵包含了该像素周围区域在 x 和 y 方向上强度变化的信息。</p></li><li><p><strong>计算 Harris 响应值 (R)</strong>：通过结构张量的特征值，计算 Harris 响应函数。常用的公式为： ![image-20241027170638266](./ppt 567.assets/image-20241027170638266.png) 其中：</p><ul><li>det(M) 是矩阵 (M) 的行列式</li><li>trace(M) 是矩阵 (M) 的迹（对角线元素之和）</li><li>(k) 是一个经验常数，通常设置在 0.04 到 0.06 之间</li></ul></li><li><p><strong>阈值化</strong>：计算每个像素的 Harris 响应 (R) 之后，应用阈值化操作来确定哪些点是图像中的角点。通常，如果 Harris 响应值超过某个阈值，该点即被认为是角点。</p></li><li><p><strong>非极大值抑制</strong>：在阈值化之后，应用非极大值抑制以确保在局部区域内只保留最强的角点，从而减少相邻像素间的重复检测。</p></li></ol><p><mark><strong>Harris 角点检测公式解析</strong></mark></p><p>对于每个像素，Harris 响应矩阵 (M) 定义为：</p><p>![image-20241027170813270](./ppt 567.assets/image-20241027170813270.png)</p><p>其中：</p><ul><li>(I_x) 和 (I_y) 分别为 x 和 y 方向的梯度。</li><li>w(u, v) 是一个窗口函数（通常是高斯核），根据距离中心像素的远近为梯度加权。</li></ul><p>然后，Harris 响应函数计算为：</p><p>![image-20241027170856783](./ppt 567.assets/image-20241027170856783.png)</p><p>其中：</p><ul><li>det(M) 是矩阵 (M) 的行列式，表示梯度向量所生成的平行四边形的面积。</li><li>trace(M) 是矩阵 (M) 的迹（对角线元素的和），它提供了关于梯度整体强度的信息。</li><li>(k) 是一个常数，通常在 0.04 到 0.06 之间。</li></ul><p><mark><strong>Harris 角点检测的步骤示例：</strong></mark></p><ol><li>将图像转换为灰度图。</li><li>使用 Sobel 滤波器计算梯度图像 (I_x) 和 (I_y)。</li><li>计算每个像素的梯度乘积 (I_x<sup>2)、(I_y</sup>2) 和 (I_x I_y)。</li><li>对梯度乘积应用高斯滤波进行平滑。</li><li>计算每个像素的 Harris 响应 (R)。</li><li>对响应值进行阈值化以识别角点。</li><li>进行非极大值抑制以获得最终的角点集合。</li></ol><h4 id="_2-ppt" tabindex="-1"><a class="header-anchor" href="#_2-ppt"><span>2. ppt</span></a></h4><ol><li><p>Compute x and y derivatives of image</p><img src="'+f+'" alt="image-20241021165845767" style="zoom:80%;"></li><li><p>Compute products of derivatives at every pixel</p><p>![image-20241021165926749](./ppt 567.assets/image-20241021165926749.png)</p></li><li><p>Compute the sums of the products of derivatives at each pixel</p></li></ol><img src="'+v+'" alt="image-20241021170013570" style="zoom:80%;"><ol start="4"><li><p>Define the matrix at each pixel</p><p>![image-20241021170052622](./ppt 567.assets/image-20241021170052622.png)</p></li><li><p>Compute the response of the detector at each pixel</p><p>![image-20241021170107633](./ppt 567.assets/image-20241021170107633.png)</p></li><li><p>Threshold on value of R, compute non-max suppression.</p></li></ol><ul><li>![image-20241021171010845](./ppt 567.assets/image-20241021171010845.png)</li></ul><h3 id="_2-3-sift-detector-and-descriptor-——-scale-invanriant-feature-transform" tabindex="-1"><a class="header-anchor" href="#_2-3-sift-detector-and-descriptor-——-scale-invanriant-feature-transform"><span>2.3 SIFT Detector and Descriptor —— Scale Invanriant Feature Transform</span></a></h3><h4 id="_2-3-1-基本信息" tabindex="-1"><a class="header-anchor" href="#_2-3-1-基本信息"><span>2.3.1 基本信息</span></a></h4><ul><li><p>选取关键点后，这些关键点 invariant to scale and rotation.</p></li><li><p>Also, Lowe aimed to create a descriptor that was robust to the variations corresponding to typical viewing conditions. The descriptor is the most-used part of SIFT.</p></li><li><p>Image content is transformed into local feature coordinates that are invariant to translation, rotation, scale, and other imaging parameters. (图像的内容进行平移，旋转，放大缩小的变换)</p></li><li><p>Advantages：</p><ul><li>局部性 Locality</li><li>独特性：单一特征与整个数据库比对 Distinctive</li><li>Quantity 一个简单的关键点也可以生成大量不同特征</li><li>Efficiency</li><li>Extensibility</li></ul></li></ul><h4 id="_2-3-2-step-1-——-scale-space-extrema-detection" tabindex="-1"><a class="header-anchor" href="#_2-3-2-step-1-——-scale-space-extrema-detection"><span>2.3.2 Step 1 —— Scale-space extrema detection</span></a></h4><ul><li>Goal: Identify locations and scales that can be repeatably assigned under different views of the same scene or object.</li></ul><details class="hint-container details"><summary>Chinese</summary><p>同一个视图（物体）在不同的位置和比例也可以检测到。</p></details><ul><li>Method: search for stable features across multiple scales using a continuous function of scale.</li></ul><details class="hint-container details"><summary>详情</summary><p>用一个比例的连续函数检测在不同比例下的稳定的特征。</p></details><ul><li>Prior work has shown that under a variety of assumptions, the best function is a Gaussian function.</li></ul><details class="hint-container details"><summary>详情</summary><p>我们用一个高斯函数来做这个工作</p></details><p>The scale space of an image is a function <em>L(x,y,<strong></strong>)</em> that is producedfrom the convolution of a Gaussian kernel (at different scales) with the input image</p>',49)),r(n,{id:"419",data:[{id:"为什么选择高斯函数？"},{id:"ppt 图片解释"}]},{title0:t(({value:s,isActive:a})=>l[0]||(l[0]=[i("为什么选择高斯函数？")])),title1:t(({value:s,isActive:a})=>l[1]||(l[1]=[i("ppt 图片解释")])),tab0:t(({value:s,isActive:a})=>l[2]||(l[2]=[e("p",null,[i("高斯函数在图像处理中的一个关键作用是创建图像的"),e("strong",null,"尺度空间"),i("，即在不同尺度下搜索稳定的特征。通过高斯函数，可以实现对图像进行多尺度分析，以便找到那些在不同尺度下都稳定存在的特征。具体来说，高斯函数可以做到这一点的原因有以下几个方面：")],-1),e("ul",null,[e("li",null,[e("p",null,[e("strong",null,"高斯模糊和图像卷积")]),e("p",null,[i("高斯函数是一种平滑函数，当我们对图像进行高斯模糊（Gaussian Blur）时，其实是将图像与一个高斯核进行"),e("strong",null,"卷积"),i("操作。这个过程可以平滑图像，消除噪声，同时保留图像的主要结构。高斯模糊有一个重要的性质：它能够根据不同的尺度（即高斯函数的标准差 (\\sigma)）对图像进行平滑处理。")])]),e("li",null,[e("p",null,[e("strong",null,"尺度空间")]),e("p",null,"尺度空间的概念是为了处理图像中不同大小的特征点（例如角点、边缘等），通过对图像使用不同尺度的高斯核进行卷积，生成多个版本的图像，每个版本对应于不同的尺度。这个过程使得我们能够在不同的尺度下检测图像特征，因为在某些尺度下，特征可能会更加明显或稳定。")])],-1),e("p",null,"数学上，图像的尺度空间 ( L(x, y, \\singema) ) 表示的是通过与不同尺度的高斯核 ( G(x, y, \\singema) ) 卷积后的图像： ![image-20241025175332609](./ppt 567.assets/image-20241025175332609.png) 其中 ( I(x, y) ) 是输入图像，( G(x, y, \\sigma) ) 是高斯核，(\\sigma) 控制高斯模糊的程度（即尺度)。",-1),e("ul",null,[e("li",null,[e("p",null,[e("strong",null,"高斯函数的性质")]),e("p",null,[i("高斯函数的一个重要性质是，它的卷积操作具有"),e("strong",null,"平滑性"),i("和"),e("strong",null,"尺度变换的连续性"),i("。这意味着：")]),e("ul",null,[e("li",null,[e("p",null,"当 (\\sigma) 变小时，我们在图像中可以捕捉到细节更多的特征。")]),e("li",null,[e("p",null,"当 (\\sigma) 变大时，图像中的噪声和细小结构会被抹去，只留下大尺度的结构特征."),e("p",null,[i("通过在多个尺度下应用高斯函数，可以有效地捕捉到那些在多种尺度下都保持稳定的特征。也就是说，特征在不同尺度下“稳定”存在，这就是图像特征的"),e("strong",null,"尺度不变性"),i("。")])])])]),e("li",null,[e("p",null,[e("strong",null,"总结")]),e("p",null,"高斯函数通过在不同尺度下平滑图像，可以构建图像的尺度空间。在这个过程中，我们能够检测到那些在不同尺度下都保持稳定的特征，这就是如何通过高斯函数来实现多尺度特征提取的原理。")])],-1)])),tab1:t(({value:s,isActive:a})=>l[3]||(l[3]=[e("img",{src:y,alt:"image-20241025175528872",style:{zoom:"33%"}},null,-1),e("ol",null,[e("li",null,[e("p",null,[e("strong",null,"尺度空间（Scale Space）"),i(" 被分为 "),e("strong",null,"octaves（八度）"),i("。")]),e("p",null,"第一个八度使用尺度 σ。"),e("p",null,"第二个八度使用尺度 2σ。"),e("p",null,"后续八度依次类推。")]),e("li",null,[e("p",null,"在每个八度中，原始图像被重复地与不同尺度的高斯核进行卷积，产生一组尺度空间图像。")]),e("li",null,[e("p",null,"相邻的高斯图像相减，得到差分高斯（DOG，Difference of Gaussians）图像。")]),e("li",null,[e("p",null,"在每个八度结束时，高斯图像通过下采样，大小缩小为原图的四分之一，开始下一层八度的计算。")])],-1),e("img",{src:b,alt:"image-20241025180215800",style:{zoom:"33%"}},null,-1),e("img",{src:x,alt:"image-20241025180237444",style:{zoom:"33%"}},null,-1)])),_:1}),l[31]||(l[31]=e("h4",{id:"_2-3-3-step-2-——-key-point-localization",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2-3-3-step-2-——-key-point-localization"},[e("span",null,"2.3.3 Step 2 —— Key point localization")])],-1)),r(n,{id:"519",data:[{id:""},{id:""},{id:""}]},{title0:t(({value:s,isActive:a})=>l[4]||(l[4]=[])),title1:t(({value:s,isActive:a})=>l[5]||(l[5]=[])),title2:t(({value:s,isActive:a})=>l[6]||(l[6]=[])),tab0:t(({value:s,isActive:a})=>l[7]||(l[7]=[e("img",{src:_,alt:"image-20241025203156336",style:{zoom:"50%"}},null,-1),e("ul",null,[e("li",null,[i("差分高斯（DoG）极值检测： "),e("ul",null,[e("li",null,[i("这张图的核心是在"),e("strong",null,"尺度空间中检测DoG的极大值和极小值"),i("。")]),e("li",null,"在每一个点，它都会和当前图像中的8个邻居进行比较，以及尺度上下的9个邻居进行比较，以此来确定该点是否是一个局部极值。"),e("li",null,[i("对于每个找到的极大值或极小值，输出该点的"),e("strong",null,"位置"),i("和"),e("strong",null,"尺度"),i("，这为后续的特征点描述提供了基础")])])])],-1)])),tab1:t(({value:s,isActive:a})=>l[8]||(l[8]=[e("img",{src:z,alt:"image-20241025203217842",style:{zoom:"50%"}},null,-1),e("p",null,"图中展示了在32张经过合成变换和噪声添加的图像上进行的实验结果。",-1),e("p",null,"左侧图：表示在不同的尺度下，检测到的特征点的比例（% detected）和正确匹配的比例（% correctly matched）。",-1),e("ul",null,[e("li",null,"稳定性随着检测尺度的增加而增加，但是到了一定的尺度数量后，稳定性趋于平稳。")],-1),e("p",null,"右侧图：表示在不同的尺度下，检测到的特征点总数（average no. detected）和成功匹配的特征点数量（average no. matched）。",-1),e("ul",null,[e("li",null,"随着检测的尺度数增加，检测到的特征点总数增加，但匹配的特征点数量也有限度。")],-1),e("p",null,[i("总结："),e("strong",null,"对于效率的考虑，通常使用3个尺度采样可以找到最大数量的稳定关键点"),i("，超过或小于这个数都会影响性能。")],-1),e("p",null,null,-1)])),tab2:t(({value:s,isActive:a})=>l[9]||(l[9]=[e("ul",null,[e("li",null,[e("p",null,"Once a keypoint candidate is found, perform a detailed fit to nearby data to determine location, scale, and ratio of principal curvatures")]),e("li",null,[e("p",null,"In initial work keypoints were found at location and scale of a central sample point.")]),e("li",null,[e("p",null,"In newer work, they fit a 3D quadratic function to improve interpolation accuracy.")]),e("li",null,[e("p",null,"The Hessian matrix was used to eliminate edge responses")])],-1),e("p",null,"::: detail",-1),e("p",null,[i("一旦找到一个关键点候选点，会对邻近数据进行详细拟合，以确定该关键点的"),e("strong",null,"位置"),i("、"),e("strong",null,"尺度"),i("以及"),e("strong",null,"主曲率比"),i("。")],-1),e("p",null,[i("最初的工作中，关键点是根据中央样本点的"),e("strong",null,"位置和尺度"),i("来确定的。")],-1),e("p",null,[i("新的工作中，使用了"),e("strong",null,"3D二次函数"),i("来提高插值的准确性。")],-1),e("p",null,[i("同时，使用"),e("strong",null,"Hessian矩阵"),i("来消除边缘响应，进一步提高关键点的鲁棒性。")],-1)])),_:1}),l[32]||(l[32]=o('<h4 id="_2-3-4-step-3-——-orientation-assignment" tabindex="-1"><a class="header-anchor" href="#_2-3-4-step-3-——-orientation-assignment"><span>2.3.4 Step 3 —— Orientation assignment</span></a></h4><p>Compute the gradient magnitudes and orientations in a small window around the keypoint—at the appropriate scale.</p><img src="'+k+'" alt="image-20241025210350521" style="zoom:33%;"><img src="'+w+'" alt="image-20241025210405715" style="zoom:33%;"><img src="'+S+'" alt="image-20241025210635309" style="zoom:50%;"><img src="'+T+'" alt="image-20241025210417636" style="zoom:33%;"><div class="hint-container info"><p class="hint-container-title">相关信息</p><ul><li><p>Create histogram of local gradient directions at selected scale</p></li><li><p>Assign canonical orientation at peak of smoothed histogram</p></li><li><p>Each key specifies stable 2D coordinates (x, y, scale,orientation)</p></li></ul></div><h4 id="_2-3-5-step-4-——-keypoint-descriptors" tabindex="-1"><a class="header-anchor" href="#_2-3-5-step-4-——-keypoint-descriptors"><span>2.3.5 Step 4 —— Keypoint Descriptors</span></a></h4><p>◼ At this point, each keypoint has</p><p>❑ location</p><p>❑ scale</p><p>❑ orientation</p><p>◼ Next is to compute a descriptor for the local image region about each keypoint that is</p><p>❑ highly distinctive</p><p>❑ invariant as possible to variations such as changes in viewpoint and illumination</p><details class="hint-container details"><summary>详情</summary><ul><li><p>此时，每个关键点都有</p><ul><li>位置</li><li>比例尺</li><li>方向</li></ul></li><li><p>下一步是计算关于每个关键点的局部图像区域的描述符，即</p><ul><li>高度独特</li><li>尽可能不受视角和光照变化等因素的影响</li></ul></li></ul></details><ol><li><p>Take a 16 x16 window around interest point (i.e., at the scale detected).</p></li><li><p>Divide into a 4x4 grid of cells.</p></li><li><p>Compute histogram of image gradient directions in each cell (8 bins each).</p></li></ol><details class="hint-container details"><summary>详情</summary><p>在兴趣点周围（即检测到的比例尺处）开一个 16 x 16 的窗口。</p><p>划分为 4x4 网格的单元。</p><p>计算每个单元格中图像梯度方向的直方图（每个单元格 8 个分区）。</p></details><img src="'+A+'" alt="image-20241025214001835" style="zoom:50%;"><img src="'+C+'" alt="image-20241025214316084" style="zoom:50%;"><ul><li><p>Example: ![image-20241025215028100](./ppt 567.assets/image-20241025215028100.png)</p></li><li><p>步骤：</p><ol><li>use the normalized region about the keypoint</li><li>compute gradient magnitude and orientation at each point in the region</li><li>weight them by a Gaussian window overlaid on the circle</li><li>create an orientation histogram over the 4ⅹ4 subregions of the window</li><li>4 X 4 descriptors over 16 X 16 sample array were used in practice.</li><li>4 X 4 times 8 directions gives a vector of 128 values.</li></ol></li></ul><details class="hint-container details"><summary>详情</summary><p><strong>区域归一化</strong>：在特征点附近的归一化区域内进行特征描述符的计算，以确保特征的尺度和旋转不变性。</p><p><strong>梯度计算</strong>：在特征点周围的每个像素处计算梯度的幅度和方向，这帮助提取特征点的方向性信息。</p><p><strong>加权</strong>：将一个高斯窗口叠加在区域上，使得离特征点较远的梯度贡献较小，从而突出了特征点周围的强特征。</p><p><strong>方向直方图</strong>：将该区域划分为4×4的子区域，在每个子区域中统计8个方向的梯度方向直方图。</p><p><strong>128维向量生成</strong>：最终生成4×4×8 = 128维的特征向量，以描述该特征点的局部特性。</p></details><h3 id="_2-4-gist-descriptor" tabindex="-1"><a class="header-anchor" href="#_2-4-gist-descriptor"><span>2.4 GIST descriptor</span></a></h3><ol><li>Compute filter responses (filter bank of Gabor filters)</li><li>Divide image patch into 4 x 4 cells</li><li>Compute filter response averages for each cell</li><li>Size of descriptor is 4 x 4 x N, where N is the size of the filter bank</li></ol><details class="hint-container details"><summary>详情</summary><img src="'+N+'" alt="image-20241026200417775" style="zoom:50%;"></details><div class="hint-container info"><p class="hint-container-title">Gabor filters？</p><img src="'+X+'" alt="image-20241026201504144" style="zoom:50%;"><img src="'+E+'" alt="image-20241026201609469" style="zoom:33%;"></div><h3 id="_2-5-histogram-of-gradient-orientations-hog" tabindex="-1"><a class="header-anchor" href="#_2-5-histogram-of-gradient-orientations-hog"><span>2.5 Histogram of Gradient Orientations(HOG)</span></a></h3><h4 id="_2-5-1-step-1-image-processing" tabindex="-1"><a class="header-anchor" href="#_2-5-1-step-1-image-processing"><span>2.5.1 Step 1 Image Processing</span></a></h4><p>• Patches can be any size, but there is a fixed ratio, for example, when the patch aspect ratio is 1:2, the patch size can be 100<em>200, 128</em>256 or 1000<em>2000 but not 101</em>205.</p><p>• Here is a picture of 720<em>475, we choose a patch of 100</em>200 size to calculate the HOG feature, cut out this patch from the image, and then adjust the size to 64*128.</p><img src="'+G+'" alt="image-20241026201921639" style="zoom:50%;"><h4 id="_2-5-2-step-2" tabindex="-1"><a class="header-anchor" href="#_2-5-2-step-2"><span>2.5.2 Step 2</span></a></h4><p>Find horizontal and vertical gradients, gradient magnitude and orientations</p><img src="'+M+'" alt="image-20241026202132232" style="zoom:50%;"><h4 id="_2-5-3-step-3" tabindex="-1"><a class="header-anchor" href="#_2-5-3-step-3"><span>2.5.3 Step 3</span></a></h4><ol><li><p><strong>图像划分与滑动窗口</strong>：将图像分成 8x8 像素的单元格，然后使用 2x2 的块作为滑动窗口。这样做的目的是对图像进行局部区域的分析，每个区域能够反映该区域的梯度信息。（“8x8 的单元格”代表图像被分成小区域，“2x2 的滑动窗口”表示在8x8区域内的更细粒度滑动，这样可以获得更丰富的梯度信息并保持计算的局部一致性。）</p></li><li><p><strong>梯度方向与幅值的计算</strong>：对于每个像素，计算其梯度的幅值和方向。每个像素的梯度方向和幅值可以帮助捕捉图像边缘和纹理的方向。</p></li><li><p><strong>量化梯度方向</strong>：将梯度方向量化成 9 个方向区间（20°到160°），每个区间代表一个方向“桶”（bin），然后根据幅值分布对方向区间内的像素进行统计。量化后的方向信息被记录在直方图中，以便后续进行特征描述。</p></li><li><p><strong>特征直方图的压缩表示</strong>：每个块（2x2的单元格）内的像素梯度方向和幅值被量化并统计成一个方向直方图。这样的表示方式既能够简化数据，同时保留了足够的边缘和轮廓信息。举例来说，如果一个图像是 8x8 大小，每个像素有三通道值，那么最终的HOG特征将会形成一个包含128维度的特征向量。</p></li><li><p><strong>可视化</strong>：图中用箭头表示了梯度方向，箭头的长度表示梯度的幅值。右下角的直方图展示了量化后的梯度方向和幅值，这种表示方式能够帮助HOG特征在不同尺度下获得鲁棒性。</p></li></ol><h4 id="_2-5-4-step-4-5" tabindex="-1"><a class="header-anchor" href="#_2-5-4-step-4-5"><span>2.5.4 Step 4，5</span></a></h4><img src="'+U+'" alt="image-20241026203634591" style="zoom:33%;"><p>step 4：16*16 block normalization</p><p>step 5：Concatenate histograms into a feature of : 15 x 7 x 4 x 9 = 3780 dimensions （7：horizontal；15：vertical）</p><h4 id="_2-5-5-summary" tabindex="-1"><a class="header-anchor" href="#_2-5-5-summary"><span>2.5.5 Summary</span></a></h4><p>![image-20241026203906992](./ppt 567.assets/image-20241026203906992.png)</p><h2 id="_3-lecture-7" tabindex="-1"><a class="header-anchor" href="#_3-lecture-7"><span>3. Lecture 7</span></a></h2><h3 id="_3-1-what-is-bag-of-features-what-about-the-steps-of-bog" tabindex="-1"><a class="header-anchor" href="#_3-1-what-is-bag-of-features-what-about-the-steps-of-bog"><span>3.1 What is bag of features? What about the steps of BOG?</span></a></h3><h4 id="_3-1-1-step" tabindex="-1"><a class="header-anchor" href="#_3-1-1-step"><span>3.1.1 Step</span></a></h4><ol><li>Extract local features</li><li>Learn “visual vocabulary”</li><li>Quantize local features using visual vocabulary</li><li>Represent images by frequencies of “visual words</li></ol><h4 id="_3-1-2-local-feature-extraction" tabindex="-1"><a class="header-anchor" href="#_3-1-2-local-feature-extraction"><span>3.1.2 Local feature extraction</span></a></h4><p>![image-20241027102254983](./ppt 567.assets/image-20241027102254983.png)</p><h4 id="_3-1-3-learning-the-visual-vocabulary" tabindex="-1"><a class="header-anchor" href="#_3-1-3-learning-the-visual-vocabulary"><span>3.1.3 Learning the visual vocabulary</span></a></h4><p>视觉词汇就是通过聚类提取的局部特征集合，每个视觉单词对应一个聚类中心。通过这些视觉单词的频率，可以用直方图的形式来表示图像的特征。</p><ul><li>具体步骤</li></ul><img src="'+D+'" alt="image-20241026212256704" style="zoom:33%;"><p>Extracted descriptors from the training set （左上角）</p><div class="hint-container info"><p class="hint-container-title">相关信息</p><p>在提取的描述子上应用聚类算法，将相似的特征聚集在一起，以创建视觉词汇表。每个聚类中心就代表了一个“视觉单词”。</p><p>右图中的彩色点表示将描述子经过聚类后，形成了不同类别的视觉单词。</p></div><h4 id="_3-1-4-quantize-local-features-using-visual-vocabulary-and" tabindex="-1"><a class="header-anchor" href="#_3-1-4-quantize-local-features-using-visual-vocabulary-and"><span>3.1.4 <strong>Quantize local features using visual vocabulary</strong> and</span></a></h4><ul><li><p>怎么聚类？ K-means clustering</p><p><strong>Goal</strong>: minimize sum of squared Euclidean distances between features <strong>x</strong>i and their nearest cluster centers <strong>m</strong>k</p><img src="'+F+'" alt="image-20241026212752131" style="zoom:50%;"></li></ul><p>​ <img src="'+H+'" alt="image-20241026213551358" style="zoom:33%;"></p><h4 id="_3-1-5-represent-images-by-frequencies-of-visual-words" tabindex="-1"><a class="header-anchor" href="#_3-1-5-represent-images-by-frequencies-of-visual-words"><span>3.1.5 <strong>Represent images by frequencies of “visual words”</strong></span></a></h4>',58)),e("ul",null,[e("li",null,[l[16]||(l[16]=e("p",null,[e("mark",null,[e("strong",null,"Spatial pyramid")])],-1)),r(n,{id:"991",data:[{id:"Level 0"},{id:"Level 1"},{id:"Level 2"}]},{title0:t(({value:s,isActive:a})=>l[10]||(l[10]=[i("Level 0")])),title1:t(({value:s,isActive:a})=>l[11]||(l[11]=[i("Level 1")])),title2:t(({value:s,isActive:a})=>l[12]||(l[12]=[i("Level 2")])),tab0:t(({value:s,isActive:a})=>l[13]||(l[13]=[e("img",{src:I,alt:"image-20241027000432976",style:{zoom:"67%"}},null,-1)])),tab1:t(({value:s,isActive:a})=>l[14]||(l[14]=[e("img",{src:V,alt:"image-20241027000354061",style:{zoom:"67%"}},null,-1)])),tab2:t(({value:s,isActive:a})=>l[15]||(l[15]=[e("img",{src:B,alt:"image-20241027000414662",style:{zoom:"67%"}},null,-1)])),_:1}),l[17]||(l[17]=o('<div class="hint-container info"><p class="hint-container-title">相关信息</p><p>将聚类得到的视觉词汇转换为图像的直方图，一般按照以下步骤来进行：</p><p><mark>步骤</mark></p><ol><li><p><strong>构建视觉词汇表</strong>：</p><ul><li>使用特征提取算法（例如SIFT）从一组图像中提取局部特征。</li><li>将所有图像的特征汇总，并使用聚类算法（通常是K-means）将这些特征分成若干个簇。每个簇的中心代表一个“视觉单词”。</li><li>这样得到一个视觉词汇表，包含所有“视觉单词”。</li></ul></li><li><p><strong>量化图像特征</strong>：</p><ul><li>对于每一张待量化的图像，再次提取其局部特征（例如SIFT特征）。</li><li>将图像中的每个特征与视觉词汇表中的视觉单词进行比较，找到最近的视觉单词（即距离最近的簇中心）。</li><li>记录每个视觉单词出现的次数。</li></ul></li><li><p><strong>生成直方图</strong>：</p><ul><li>初始化一个直方图，其维度等于视觉词汇表中视觉单词的个数，每个维度的初始值为0。</li><li>对于图像中的每个特征，根据找到的最近视觉单词的编号，将该编号对应的直方图位置的值加1。</li><li>最后，得到的直方图就是该图像的“词频”表示，即每个视觉单词在该图像中的出现频率。</li></ul></li><li><p><strong>归一化</strong>（可选）：</p><ul><li>为了避免图像大小对直方图的影响，可以将直方图归一化，使其所有值的和为1。</li></ul></li></ol><p><mark>示例</mark></p><p>假设我们有一个视觉词汇表，共10个视觉单词编号为0到9，表示一个图像中的特征量化结果为[1, 2, 3, 3, 5, 8, 8]。我们构建一个10维的直方图，将相应编号的计数加1：</p><ul><li>初始化直方图：<code>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</code></li><li>填充直方图： <ul><li>视觉单词1出现1次 -&gt; <code>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</code></li><li>视觉单词2出现1次 -&gt; <code>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0]</code></li><li>视觉单词3出现2次 -&gt; <code>[0, 1, 1, 2, 0, 0, 0, 0, 0, 0]</code></li><li>视觉单词5出现1次 -&gt; <code>[0, 1, 1, 2, 0, 1, 0, 0, 0, 0]</code></li><li>视觉单词8出现2次 -&gt; <code>[0, 1, 1, 2, 0, 1, 0, 0, 2, 0]</code></li></ul></li></ul><p>最终得到的直方图表示为<code>[0, 1, 1, 2, 0, 1, 0, 0, 2, 0]</code>。</p><p><mark>结果</mark></p><p>通过这种方式，我们将图像转换为视觉词汇的直方图表示，形成固定长度的特征向量，可以用于进一步的分类和比较。</p><p>可以根据图片内容生成一些直方图。每个层级的特征通过直方图表示，随着层次的增加，特征的分辨率变高，可以捕捉更细粒度的图像信息。</p></div>',1))])]),l[33]||(l[33]=e("p",null,"![image-20241026234103011](./ppt 567.assets/image-20241026234103011.png)",-1)),l[34]||(l[34]=e("ul",null,[e("li",null,"这个过程就是，首先从图像中提取特征（一般是手工设计的特征），然后通过可以训练的分类器进行分类。常见的特征表示方法包括边缘、纹理等手工特征，而分类器可以是支持向量机（SVM）、K-近邻（KNN）等。")],-1)),l[35]||(l[35]=e("h4",{id:"_3-1-6-对直方图进行分类处理",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_3-1-6-对直方图进行分类处理"},[e("span",null,"3.1.6 对直方图进行分类处理")])],-1)),l[36]||(l[36]=e("p",null,"分类器：",-1)),r(n,{id:"1165",data:[{id:"K-nearest neighbor classifier"},{id:"Linear classifier"},{id:"Nearest neighbor VS Linear classifiers"},{id:"Support Vector Machines"},{id:"如何训练一个分类器"}]},{title0:t(({value:s,isActive:a})=>l[18]||(l[18]=[i("K-nearest neighbor classifier")])),title1:t(({value:s,isActive:a})=>l[19]||(l[19]=[i("Linear classifier")])),title2:t(({value:s,isActive:a})=>l[20]||(l[20]=[i("Nearest neighbor VS Linear classifiers")])),title3:t(({value:s,isActive:a})=>l[21]||(l[21]=[i("Support Vector Machines")])),title4:t(({value:s,isActive:a})=>l[22]||(l[22]=[i("如何训练一个分类器")])),tab0:t(({value:s,isActive:a})=>l[23]||(l[23]=[e("ul",null,[e("li",null,[e("p",null,[e("mark",null,[e("strong",null,"K-nearest neighbor classifier 工作原理：")])]),e("p",null,"给定一个测试样本，通过计算它与训练集中每个样本的距离，找到与之距离最近的训练样本，并将最近样本的标签赋予测试样本。这种方法不需要训练，只需要一个距离或相似度度量。"),e("img",{src:K,alt:"image-20241026234929476",style:{zoom:"50%"}}),e("p",null,"我们这里用来比对的是（之一）欧几里得距离和余弦距离："),e("img",{src:L,alt:"image-20241027003140666",style:{zoom:"50%"}}),e("details",{class:"hint-container details"},[e("summary",null,"详情"),e("p",null,"K-最近邻（KNN）分类器 是一种基于实例的学习方法。它不需要训练模型，而是在分类时直接使用训练数据。KNN通过计算新数据点与训练集中所有数据点之间的距离来找到最近的kkk个点，然后通过这些点的标签来决定新数据点的类别。KNN通常使用欧几里得距离，但也可以使用其他距离度量。"),e("img",{src:O,alt:"image-20241027004515674",style:{zoom:"33%"}})]),e("div",{class:"hint-container info"},[e("p",{class:"hint-container-title"},"相关信息"),e("p",null,"如果我们用直方图来进行比对，有一些特定的公式可以把从图片上提取的直方图和一下特定样本比对："),e("img",{src:R,alt:"image-20241027002435332",style:{zoom:"50%"}})])])],-1)])),tab1:t(({value:s,isActive:a})=>l[24]||(l[24]=[e("ul",null,[e("li",null,[e("p",null,[e("mark",null,[e("strong",null,"线性分类器—— Linear classifiers")])]),e("p",null,"线性分类器的主要步骤如下："),e("ol",null,[e("li",null,[e("p",null,[e("strong",null,"定义线性函数"),i("：假设线性分类器的形式为 f(x) = w * x + b 其中，( w ) 是权重向量，( x ) 是输入样本特征向量，( b ) 是偏置。目标是找到最优的( w )和( b )，以便将不同类别的数据分开。")])]),e("li",null,[e("p",null,[e("strong",null,"选择损失函数"),i("：根据具体的分类器选择损失函数。例如：")]),e("ul",null,[e("li",null,"感知机：基于误分类损失。"),e("li",null,"支持向量机（SVM）：基于最大化分类间隔的 hinge 损失。"),e("li",null,"线性回归：可以用于分类，但通常通过最小化均方误差来优化。")])]),e("li",null,[e("p",null,[e("strong",null,"优化权重和偏置"),i("：使用梯度下降或其他优化算法最小化损失函数，找到最优的权重( w )和偏置( b )。例如，在支持向量机中，优化的目标是找到一个最大化分类间隔的分隔超平面。")])]),e("li",null,[e("p",null,[e("strong",null,"预测"),i("：对于一个新的样本( x )，计算其分类分数")]),e("p",null,"​ f(x) = w * x + b"),e("p",null,"根据分数的符号来确定类别。通常，正数表示一种类别，负数表示另一种类别。")]),e("li",null,[e("p",null,[e("strong",null,"评估"),i("：使用验证数据集或交叉验证等方法来评估分类器的性能，并根据需要调整参数以提高模型的泛化能力。")])])]),e("p",null,[e("strong",null,"总结")]),e("p",null,"线性分类器的基本步骤可以归纳为：定义线性模型，选择损失函数，优化模型参数（权重和偏置），然后利用优化后的模型进行预测。")])],-1)])),tab2:t(({value:s,isActive:a})=>l[25]||(l[25]=[e("img",{src:q,alt:"image-20241027102108511",style:{zoom:"50%"}},null,-1)])),tab3:t(({value:s,isActive:a})=>l[26]||(l[26]=[e("img",{src:Q,alt:"image-20241027105753823",style:{zoom:"33%"}},null,-1),e("p",null,"这些分隔线都能够将红色点和棕色点正确分类，因为数据是线性可分的。",-1),e("p",null,[i("但问题是，哪一条分隔线才是最佳的？SVM的目标是找到一个"),e("strong",null,"最优超平面"),i("，这个超平面不仅能分隔两类数据，还能最大化与两类样本之间的间隔（称为“间隔最大化”）。选择间隔更大的分隔线可以提高模型的鲁棒性，对未来数据分类的准确性也更高。")],-1),e("p",null,[i("SVM可以通过在高维空间中寻找线性分隔面，进而在原始空间形成非线性边界来解决这个问题。此方法利用了"),e("strong",null,"核函数"),i("，将原始的特征映射到更高维的特征空间，使得在该空间内可以找到一个简单的线性决策边界。")],-1),e("ul",null,[e("li",null,[e("p",null,"Example："),e("img",{src:Z,alt:"image-20241027111558260",style:{zoom:"33%"}}),e("p",null,"图中展示了一个较为复杂的分类问题。在这个例子中，决策边界是曲线的，而不是简单的直线，这是因为数据不是线性可分的。图中展示的是对“人脸”与“非人脸”分类的问题，经过复杂特征的学习后，形成了一个复杂的决策边界，从而可以在原始的空间中精确地区分两类数据。")])],-1)])),tab4:t(({value:s,isActive:a})=>l[27]||(l[27]=[e("p",null,[i("• Goal: obtain a classifier with "),e("strong",null,"good generalization"),i(" or performance on never before seen data")],-1),e("ol",null,[e("li",null,[e("p",null,[i("Learn "),e("em",null,"parameters"),i(" on the "),e("strong",null,"training set")])]),e("li",null,[e("p",null,[i("Tune "),e("em",null,"hyperparameters"),i(" (implementation choices) on the"),e("em",null,"held out"),i(),e("strong",null,"validation set")])]),e("li",null,[e("p",null,[i("Evaluate performance on the "),e("strong",null,"test set")])])],-1),e("p",null,"​ • – Crucial: do not peek at the test set when iterating",-1),e("p",null,"​ steps 1 and 2!",-1),e("p",null,[i("这张幻灯片讲解了机器学习模型的训练和评估流程，目的是得到一个"),e("strong",null,"具有良好泛化能力的分类器"),i("，即在从未见过的数据上也能保持较好的性能。具体步骤如下：")],-1),e("ol",null,[e("li",null,[e("p",null,[e("strong",null,"在训练集上学习参数"),i("：")]),e("ul",null,[e("li",null,"**训练集（Training Set）**用于模型的训练。模型会在这个数据集上学习到特定的参数，以便能够尽可能准确地拟合训练数据。"),e("li",null,"在这个过程中，模型内部的参数（如权重）会根据数据进行调整，以最小化预测误差。")])]),e("li",null,[e("p",null,[e("strong",null,"在验证集上调整超参数"),i("：")]),e("ul",null,[e("li",null,[e("strong",null,[i("验证集（Validation Set）"),e("strong",null,"通常是从训练数据中分离出的一部分数据，不用于直接的参数学习，而是用于调整"),i("超参数")]),i("。")]),e("li",null,"超参数是模型构建时的某些设置选项，比如树的深度、学习率等。这些超参数不在训练过程中自动调整，而是由用户通过实验来手动优化。"),e("li",null,"验证集用于检查模型在不同超参数下的表现，从而帮助选择最佳的超参数组合。"),e("li",null,"需要注意的是，验证集数据不会影响模型的参数，而仅用于选择模型结构。")])]),e("li",null,[e("p",null,[e("strong",null,"在测试集上评估模型性能"),i("：")]),e("ul",null,[e("li",null,"**测试集（Test Set）**是用来评估模型最终性能的数据。它是完全隔离的数据集，模型在之前的训练和验证过程中都没有见过这部分数据。"),e("li",null,"测试集的主要目的是提供一个客观的标准来衡量模型的泛化能力。"),e("li",null,[e("strong",null,"重要提醒"),i("：在整个模型的训练和验证过程中，绝对不能提前查看测试集的结果，因为这样会导致数据泄露，使得测试结果的客观性受到影响。")])])])],-1),e("img",{src:P,alt:"image-20241027112710140",style:{zoom:"50%"}},null,-1),e("p",null,"这几张图片展示了模型训练过程中数据的分割和验证过程，特别强调了如何通过合理的数据集划分来优化模型的泛化能力和防止过拟合。",-1),e("ol",null,[e("li",null,[e("p",null,[e("strong",null,"不建议直接使用测试集调参"),i("：")]),e("ul",null,[e("li",null,"第一张图表明，用测试集来调试和选择超参数是一种不好的做法，因为测试集应该作为模型最终性能的验证，不应该参与模型的训练过程。测试集是模型最终评估的代理，用于反映模型对全新数据的泛化性能，只有在最终评估时才少量使用。")])]),e("li",null,[e("p",null,[e("strong",null,"分割训练集和验证集"),i("：")]),e("ul",null,[e("li",null,"第二张图展示了一种更好的做法，即将训练数据进一步分割为不同的“折”（fold）用于交叉验证。通常，我们将一部分数据保留为验证集，其他数据用于训练。然后使用该验证集来优化模型的超参数，而测试集仅在最后用于最终模型的性能评估。")])]),e("li",null,[e("p",null,[e("strong",null,"交叉验证（Cross-validation）"),i("：")]),e("ul",null,[e("li",null,"第三张图展示了交叉验证的方法。交叉验证将训练数据集划分为多个部分（例如五折交叉验证）。每次选择一个“折”作为验证集，其他折用于训练模型，依次轮换，直到每个折都做过一次验证集。最后将各折的结果平均，得到模型的整体性能。这样可以更好地评估模型的稳定性和泛化能力。")])])],-1),e("p",null,"这套流程确保了模型的性能评估尽量客观，使得模型不仅在当前数据上表现良好，也能够在全新数据上具有良好的泛化性能。",-1),e("p",null,null,-1)])),_:1}),l[37]||(l[37]=o('<h4 id="_3-1-5-summar" tabindex="-1"><a class="header-anchor" href="#_3-1-5-summar"><span>3.1.5 Summar</span></a></h4><ol><li><strong>Extract Local Features</strong> from images.</li><li><strong>Cluster Features</strong> to form a visual vocabulary (create visual words).</li><li><strong>Quantize Features</strong> of each image based on the visual vocabulary.</li><li><strong>Build Histograms</strong> of visual words to represent each image.</li><li>Optionally, <strong>Classify</strong> images using these histograms as feature vectors.</li></ol><p>This approach allows images to be represented by fixed-size vectors, making them suitable for machine learning models that require fixed-length inputs, regardless of the number of keypoints detected in each image.</p><h3 id="_3-2-choose-recognize-model" tabindex="-1"><a class="header-anchor" href="#_3-2-choose-recognize-model"><span>3.2 Choose recognize model</span></a></h3><p><strong>We have many images, these images contains grass, person,</strong> <strong>building. Now we want to classify them into three classes.</strong> <strong>How can we design the recognition model? What is the steps?</strong></p>',5)),r(n,{id:"1476",data:[{id:"自己话总结"}]},{title0:t(({value:s,isActive:a})=>l[28]||(l[28]=[i("自己话总结")])),tab0:t(({value:s,isActive:a})=>l[29]||(l[29]=[e("ol",null,[e("li",null,"首先提取兴趣点，做标签（通常是手动的）"),e("li",null,"提取特征向量描述（兴趣点附近）"),e("li",null,"我们可以用 SIFT 算法生成一个描述符，使其经过平移，旋转，放大缩小，也能检测出来。"),e("li",null,"通过 SIFT 检测出的特征描述符聚类（用一些聚类算法）成“视觉单词”，并生成词频直方图表示图像。"),e("li",null,"再用 SVM 或 KNN 把生成的不同图片的直方图与元特征进行比对，进行分类。")],-1)])),_:1})])}const ee=p(Y,[["render",J],["__file","ppt 567.html.vue"]]),le=JSON.parse('{"path":"/zh/Computer_Vision/ppt%20567.html","title":"计算机视觉 L 5 - L 7","lang":"zh-CN","frontmatter":{"title":"计算机视觉 L 5 - L 7","icon":"python","date":"2024-10-20T14:52:40.000Z","author":"XiaoXianYue","isOriginal":true,"category":["大三上","计算机视觉"],"tag":["大三上","计算机视觉"],"sticky":false,"star":false,"article":true,"timeline":true,"image":false,"navbar":true,"sidebarIcon":true,"headerDepth":5,"lastUpdated":true,"editLink":false,"backToTop":true,"toc":true,"feed":false,"seo":false,"head":[]},"headers":[{"level":2,"title":"1. Lecture 5 : Affine Transformations","slug":"_1-lecture-5-affine-transformations","link":"#_1-lecture-5-affine-transformations","children":[]},{"level":2,"title":"2. Lecture 6","slug":"_2-lecture-6","link":"#_2-lecture-6","children":[{"level":3,"title":"2.1 What is Interest Point?","slug":"_2-1-what-is-interest-point","link":"#_2-1-what-is-interest-point","children":[]},{"level":3,"title":"2.2 What is Harris Corner? How to get it?","slug":"_2-2-what-is-harris-corner-how-to-get-it","link":"#_2-2-what-is-harris-corner-how-to-get-it","children":[{"level":4,"title":"1. GPT","slug":"_1-gpt","link":"#_1-gpt","children":[]},{"level":4,"title":"2. ppt","slug":"_2-ppt","link":"#_2-ppt","children":[]}]},{"level":3,"title":"2.3 SIFT Detector and Descriptor —— Scale Invanriant Feature Transform","slug":"_2-3-sift-detector-and-descriptor-——-scale-invanriant-feature-transform","link":"#_2-3-sift-detector-and-descriptor-——-scale-invanriant-feature-transform","children":[{"level":4,"title":"2.3.1 基本信息","slug":"_2-3-1-基本信息","link":"#_2-3-1-基本信息","children":[]},{"level":4,"title":"2.3.2 Step 1 —— Scale-space extrema detection","slug":"_2-3-2-step-1-——-scale-space-extrema-detection","link":"#_2-3-2-step-1-——-scale-space-extrema-detection","children":[]},{"level":4,"title":"2.3.3 Step 2 —— Key point localization","slug":"_2-3-3-step-2-——-key-point-localization","link":"#_2-3-3-step-2-——-key-point-localization","children":[]},{"level":4,"title":"2.3.4 Step 3 —— Orientation assignment","slug":"_2-3-4-step-3-——-orientation-assignment","link":"#_2-3-4-step-3-——-orientation-assignment","children":[]},{"level":4,"title":"2.3.5 Step 4 —— Keypoint Descriptors","slug":"_2-3-5-step-4-——-keypoint-descriptors","link":"#_2-3-5-step-4-——-keypoint-descriptors","children":[]}]},{"level":3,"title":"2.4 GIST descriptor","slug":"_2-4-gist-descriptor","link":"#_2-4-gist-descriptor","children":[]},{"level":3,"title":"2.5 Histogram of Gradient Orientations(HOG)","slug":"_2-5-histogram-of-gradient-orientations-hog","link":"#_2-5-histogram-of-gradient-orientations-hog","children":[{"level":4,"title":"2.5.1 Step 1 Image Processing","slug":"_2-5-1-step-1-image-processing","link":"#_2-5-1-step-1-image-processing","children":[]},{"level":4,"title":"2.5.2 Step 2","slug":"_2-5-2-step-2","link":"#_2-5-2-step-2","children":[]},{"level":4,"title":"2.5.3 Step 3","slug":"_2-5-3-step-3","link":"#_2-5-3-step-3","children":[]},{"level":4,"title":"2.5.4 Step 4，5","slug":"_2-5-4-step-4-5","link":"#_2-5-4-step-4-5","children":[]},{"level":4,"title":"2.5.5 Summary","slug":"_2-5-5-summary","link":"#_2-5-5-summary","children":[]}]}]},{"level":2,"title":"3.  Lecture 7","slug":"_3-lecture-7","link":"#_3-lecture-7","children":[{"level":3,"title":"3.1 What is bag of features? What about the steps of BOG?","slug":"_3-1-what-is-bag-of-features-what-about-the-steps-of-bog","link":"#_3-1-what-is-bag-of-features-what-about-the-steps-of-bog","children":[{"level":4,"title":"3.1.1  Step","slug":"_3-1-1-step","link":"#_3-1-1-step","children":[]},{"level":4,"title":"3.1.2  Local feature extraction","slug":"_3-1-2-local-feature-extraction","link":"#_3-1-2-local-feature-extraction","children":[]},{"level":4,"title":"3.1.3 Learning the visual vocabulary","slug":"_3-1-3-learning-the-visual-vocabulary","link":"#_3-1-3-learning-the-visual-vocabulary","children":[]},{"level":4,"title":"3.1.4 Quantize local features using visual vocabulary and","slug":"_3-1-4-quantize-local-features-using-visual-vocabulary-and","link":"#_3-1-4-quantize-local-features-using-visual-vocabulary-and","children":[]},{"level":4,"title":"3.1.5 Represent images by frequencies of “visual words”","slug":"_3-1-5-represent-images-by-frequencies-of-visual-words","link":"#_3-1-5-represent-images-by-frequencies-of-visual-words","children":[]},{"level":4,"title":"3.1.6 对直方图进行分类处理","slug":"_3-1-6-对直方图进行分类处理","link":"#_3-1-6-对直方图进行分类处理","children":[]},{"level":4,"title":"3.1.5 Summar","slug":"_3-1-5-summar","link":"#_3-1-5-summar","children":[]}]},{"level":3,"title":"3.2 Choose recognize model","slug":"_3-2-choose-recognize-model","link":"#_3-2-choose-recognize-model","children":[]}]}],"git":{"createdTime":1729564384000,"updatedTime":1730078462000,"contributors":[{"name":"Xiaoxianyue","email":"2310219843@qq.com","commits":1}]},"readingTime":{"minutes":25.6,"words":7681},"filePathRelative":"zh/Computer_Vision/ppt 567.md","localizedDate":"2024年10月20日"}');export{ee as comp,le as data};
