import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as s,b as n,o as t}from"./app-84lBMjzT.js";const i="/assets/image-20250627083106716-B_hBX4RI.png",l={};function r(o,e){return t(),s("div",null,e[0]||(e[0]=[n('<h2 id="自我介绍" tabindex="-1"><a class="header-anchor" href="#自我介绍"><span>自我介绍</span></a></h2><p>您好！我叫肖羡月，是中国传媒大学（海南国际学院）智能科学与技术专业的大三学生，系统学习了机器学习、计算机视觉、高级算法、人工智能导论等核课程，打下了坚实的技术基础。</p><p>针对<strong>AI图像大数据与识别</strong>这个实习岗位，我认为我具备以下几方面的优势：</p><p><strong>首先，我具备一定的机器学习算法应用能力。<strong>在中文分词模型优化项目中，我基于BERT构建并调参分词模型，把F1分数提升到了98%左右，同时还做了Dropout率的消融实验，探索正则化与模型泛化的关系，锻炼了我对深度学习模型的理解和工程实现能力。另外</strong>在社交媒体用户行为分析项目中</strong>，我结合随机森林、XGBoost和K-means等算法完成了用户画像和互动预测，比较熟悉模型选择和评估。</p><p><strong>其次， 我具备完整的数据处理与建模实践经验</strong>。在“体育视觉传播大模型”与“新闻舆情智能体”等项目中，我负责数据清洗、合并、异常值处理等工作，参与模型微调及初步训练过程，对大规模数据流的处理流程和数据建构逻辑有了一定的思考和理解。</p><p>在技能方面，我熟练使用Python、SQL、HTML、Flask、Hive、MySQL、Matplotlib、PyTorch等工具进行软件开发和数据分析，有较好的团队协作意识和自学能力，同时具备一定英语能力，能够胜任跨文化项目。</p><p>我期待能够作为AI图像大数据与识别实习生加入团队，结合我的项目经验和技术积累，为团队发展贡献力量。</p><p>谢谢！</p><h2 id="中文分词模型优化" tabindex="-1"><a class="header-anchor" href="#中文分词模型优化"><span>中文分词模型优化：</span></a></h2><h3 id="过程和调参" tabindex="-1"><a class="header-anchor" href="#过程和调参"><span>过程和调参</span></a></h3><ol><li><p>Paddlenlp.transform该模块属于 PaddleNLP（百度开源的自然语言处理库），提供了对各种预训练模型（如 BERT、ERNIE、RoBERTa 等）的封装。其功能类似于 Hugging Face 的 transformers 库。</p></li><li><p>然后token，分词之后进行tagging（SBME）</p></li><li><p>这样之后你只需要传入 example，其它参数（tokenizer, label2id, max_seq_len）都已经默认绑定。</p></li><li><p>再使用padding使他们长度一致</p></li><li><p>封装成batch</p></li></ol><figure><img src="'+i+`" alt="image-20250627083106716" tabindex="0" loading="lazy"><figcaption>image-20250627083106716</figcaption></figure><p>① <strong>训练轮数（Epochs）</strong></p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="background-color:#272822;color:#F8F8F2;"><pre class="shiki monokai vp-code"><code><span class="line"><span>num_epochs = 3</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>表示完整训练集会被遍历 <strong>3 次</strong></li><li>如果你发现模型欠拟合（loss 高），可以适当增加轮数</li></ul><hr><p>② <strong>学习率（learning rate）</strong></p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="background-color:#272822;color:#F8F8F2;"><pre class="shiki monokai vp-code"><code><span class="line"><span>learning_rate = 3e-5</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>这是你设定的初始学习率，是微调 BERT 时的常用值（10⁻⁵～10⁻⁴ 之间）</li><li>适合稳定更新预训练模型参数</li></ul><hr><p>③ <strong>评估和日志步数</strong></p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="background-color:#272822;color:#F8F8F2;"><pre class="shiki monokai vp-code"><code><span class="line"><span>eval_steps = 100</span></span>
<span class="line"><span>log_steps = 10</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>每隔 100 个训练 step 评估一次模型指标（如 F1）</li><li>每 10 步输出一次训练 loss、进度等日志</li></ul><hr><p>④ <strong>模型保存路径</strong></p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="background-color:#272822;color:#F8F8F2;"><pre class="shiki monokai vp-code"><code><span class="line"><span>save_dir = &quot;./checkpoints&quot;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>模型在训练过程中的 checkpoint 会保存在这个目录</li></ul><hr><p>⑤ <strong>权重衰减（L2 正则化）</strong></p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="background-color:#272822;color:#F8F8F2;"><pre class="shiki monokai vp-code"><code><span class="line"><span>weight_decay = 0.01</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>用于抑制过拟合，尤其适用于大模型</li><li>设置了 0.01 的权重衰减系数，典型值（通常设在 0.01～0.1）</li></ul><hr><p>⑥ <strong>学习率预热（Warmup）比例</strong></p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="background-color:#272822;color:#F8F8F2;"><pre class="shiki monokai vp-code"><code><span class="line"><span>warmup_proportion = 0.1</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>表示训练前 10% 的 step 使用线性 warm-up，使学习率从 0 慢慢上升至设定值</li><li>这样避免一开始震荡过大，适合 BERT 这种预训练大模型</li></ul><hr><p>⑦ <strong>参数选择性权重衰减</strong></p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="background-color:#272822;color:#F8F8F2;"><pre class="shiki monokai vp-code"><code><span class="line"><span>decay_params = [</span></span>
<span class="line"><span>    p.name for n, p in model.named_parameters()</span></span>
<span class="line"><span>    if not any(nd in n for nd in [&quot;bias&quot;, &quot;norm&quot;])</span></span>
<span class="line"><span>]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>你排除了 <code>&quot;bias&quot;</code> 和 <code>&quot;LayerNorm&quot;</code> 中的参数不进行 <code>weight_decay</code>，这是非常合理的： <ul><li><code>bias</code> 和归一化层参数若正则化可能会影响模型性能</li></ul></li><li>使用 <code>apply_decay_param_fun</code> 精准控制哪些参数加 L2 惩罚</li></ul><hr><p>⑧ <strong>优化器设置</strong></p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="background-color:#272822;color:#F8F8F2;"><pre class="shiki monokai vp-code"><code><span class="line"><span>optimizer = paddle.optimizer.AdamW(</span></span>
<span class="line"><span>    learning_rate=learning_rate,</span></span>
<span class="line"><span>    parameters=model.parameters(),</span></span>
<span class="line"><span>    weight_decay=weight_decay,</span></span>
<span class="line"><span>    apply_decay_param_fun=lambda x: x in decay_params</span></span>
<span class="line"><span>)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>使用了 <code>AdamW</code>，适合 Transformer/BERT 系列</li><li>可结合 warmup 和学习率调度器使用（你后面可能会定义 <code>lr_scheduler</code>）</li></ul><hr><p>⑨ <strong>损失函数</strong></p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="background-color:#272822;color:#F8F8F2;"><pre class="shiki monokai vp-code"><code><span class="line"><span>loss_fn = nn.CrossEntropyLoss()</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>标准的 token 分类任务损失函数</li><li>可配合 <code>ignore_index</code> 屏蔽 padding label</li></ul><hr><p>⑩ <strong>评估指标设置</strong></p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="background-color:#272822;color:#F8F8F2;"><pre class="shiki monokai vp-code"><code><span class="line"><span>metric = ChunkEvaluator(label_list=label2id.keys())</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>使用 <code>ChunkEvaluator</code> 评估 NER 分词任务</li><li>支持输出 \`Precision / Recall / F1</li></ul><h3 id="✅-面试问题-你是用什么方法做这个项目的-怎么调参-做了哪些消融实验" tabindex="-1"><a class="header-anchor" href="#✅-面试问题-你是用什么方法做这个项目的-怎么调参-做了哪些消融实验"><span>✅ 面试问题：你是用什么方法做这个项目的？怎么调参？做了哪些消融实验？</span></a></h3><p><strong>答：</strong></p><p>我这个项目是基于 BERT 的中文分词任务，采用了 BERT + token-level linear 分类器的结构，用 <code>SBME</code> 标签进行序列标注。具体做法是将输入文本先用 PaddleNLP 的 <code>bert-base-chinese</code> 分词器进行 token 化，然后用一个 <code>BertForTokenClassification</code> 模型对每个 token 进行分类。</p><p>在调参方面，我使用了：</p><ul><li>微调学习率 <code>3e-5</code>，结合 <code>AdamW</code> 优化器；</li><li>加入 <code>weight decay = 0.01</code> 抑制过拟合，并排除了 <code>bias</code> 和 <code>LayerNorm</code> 参数不参与正则化；</li><li>设置 <code>warmup_proportion=0.1</code> 来缓冲前期训练；</li><li>使用 <code>ChunkEvaluator</code> 评估 <code>P/R/F1</code>，并设置了 <code>eval_steps=100</code> 进行验证集监控；</li><li>dropout 概率使用了 <code>0.1</code> 和 <code>0.3</code> 两组对比进行验证。</li></ul><p>消融实验方面，我主要做了：</p><ol><li><strong>移除 Dropout 层</strong>：观察泛化性能变化，验证其抑制过拟合的效果；</li><li><strong>只使用前几层 BERT</strong>：验证浅层特征是否足够；</li><li><strong>替换分词器（如 <code>word</code> 切分 vs <code>char</code> 切分）</strong>：对比 token 粒度对性能的影响；</li><li><strong>不做 label padding</strong>：观察 loss 计算是否受影响；</li><li><strong>去掉 warmup</strong>：学习曲线震荡更明显，验证 warmup 有助收敛稳定。</li></ol><p>这些实验帮助我确认了 dropout 和 warmup 是模型泛化能力的重要因素，最终模型在验证集上达到了较优的 F1 分数。</p><h2 id="社交媒体用户" tabindex="-1"><a class="header-anchor" href="#社交媒体用户"><span>社交媒体用户</span></a></h2><p>在这个项目中，我用 Kaggle 上的社交媒体情绪数据集，通过机器学习对用户互动行为进行了建模和用户画像分析。</p><p><strong>监督学习方面</strong>，我用了随机森林和 XGBoost 两个模型来预测贴文的互动量（点赞和转发数），主要是因为它们对特征重要性有良好的解释能力，且对离散变量友好。<br> 我做了深入的特征工程，包括提取时间（小时/是否周末）、文本长度、用户历史行为（平均点赞、转发）、平台信息等特征，并通过 one-hot 编码和标准化处理。<br> 在调参上，我主要调整了 <code>n_estimators</code> 和 <code>max_depth</code>，并对 XGBoost 启用了 <code>early_stopping</code> 来防止过拟合。</p><p><strong>无监督学习方面</strong>，我用 K-means 聚类对用户进行分群，通过 Elbow Method 和 Silhouette Score 来选择聚类数量。虽然最佳聚类数是 2-3，但我使用了 4 类来获得更细粒度的用户画像。特征包括文本长度、点赞数、是否周末发帖等。最后用雷达图展示了各群体的行为差异。</p><p>特征重要性分析显示，用户的历史互动行为是最强的预测指标，这也验证了平台“用户影响力驱动分发”的机制。这个项目让我更深入理解了如何将监督与非监督模型结合，用数据驱动的方式刻画用户行为和偏好。</p><h3 id="_1️⃣-你用随机森林和-xgboost-做了什么-为什么用这两个模型-它们的区别是什么" tabindex="-1"><a class="header-anchor" href="#_1️⃣-你用随机森林和-xgboost-做了什么-为什么用这两个模型-它们的区别是什么"><span>1️⃣ <strong>你用随机森林和 XGBoost 做了什么？为什么用这两个模型？它们的区别是什么？</strong></span></a></h3><p>我用随机森林和 XGBoost 来预测用户帖子的互动数（点赞和转发），任务是典型的回归问题。选择这两个模型主要是因为：</p><ul><li>都是集成学习方法，对非线性关系建模能力强；</li><li>对类别型和连续型特征都兼容；</li><li>都支持特征重要性分析，利于解释模型。</li></ul><p>区别在于：</p><ul><li>随机森林是 <strong>Bagging</strong> 框架，训练多个互相独立的决策树取平均，偏差小但方差大；</li><li>XGBoost 是 <strong>Boosting</strong> 框架，迭代训练新树来修正前面模型的误差，具有更强的拟合能力与收敛速度，调参更加灵活。</li></ul><h3 id="_2️⃣-你是怎么调参的-有什么技巧" tabindex="-1"><a class="header-anchor" href="#_2️⃣-你是怎么调参的-有什么技巧"><span>2️⃣ <strong>你是怎么调参的？有什么技巧？</strong></span></a></h3><p>我主要调整的超参数包括：</p><ul><li><code>n_estimators</code>（树的数量）；</li><li><code>max_depth</code>（树深）；</li><li>对 XGBoost 还使用了 <code>early_stopping_rounds</code> 以避免过拟合；</li><li>学习率（XGBoost）设置为默认的 0.3，没有明显波动时未进一步调整。</li></ul><p>调参策略：</p><ul><li>先用较大树深快速拟合看是否欠拟合；</li><li>使用交叉验证或固定验证集评估；</li><li>分析特征重要性后剔除无效特征，简化模型。</li></ul><h3 id="_3️⃣-k-means-聚类是如何用在这个项目中的-聚类的效果如何评估" tabindex="-1"><a class="header-anchor" href="#_3️⃣-k-means-聚类是如何用在这个项目中的-聚类的效果如何评估"><span>3️⃣ <strong>K-means 聚类是如何用在这个项目中的？聚类的效果如何评估？</strong></span></a></h3><p>我使用 K-means 聚类来对社交媒体用户进行画像建模。输入特征包括：</p><ul><li>平均点赞、转发数量；</li><li>文本长度；</li><li>是否喜欢在周末发帖等。</li></ul><p>为了保证各维度对聚类的影响均衡，先进行了 <strong>标准化处理</strong>。</p><p>评估方法：</p><ul><li>使用 <strong>Elbow 方法</strong> 和 <strong>轮廓系数（Silhouette Score）</strong> 来确定最优聚类数量；</li><li>聚类结果通过可视化分析，包括 <strong>雷达图</strong> 和 <strong>箱线图</strong>，从内容长度、活跃时间、互动强度等维度描述每一类用户的特征差异。</li></ul><h3 id="_4️⃣-哪些特征对模型预测最关键-为什么" tabindex="-1"><a class="header-anchor" href="#_4️⃣-哪些特征对模型预测最关键-为什么"><span>4️⃣ <strong>哪些特征对模型预测最关键？为什么？</strong></span></a></h3><p>最关键的特征是：</p><ul><li>用户历史平均点赞数（UserAvgLikes）</li><li>平均转发数（UserAvgRetweets）</li></ul><p>这两个特征的累计重要性达到 97%，说明：</p><ul><li><strong>历史互动行为高度可预测未来行为</strong>，体现出社交平台的“影响力驱动分发机制”；</li><li>与此同时，时间（IsWeekend）和文本长度（TextLength）也有次要影响，说明内容和发布时间仍有边际作用。</li></ul><h3 id="_5️⃣-你做过模型结果的可视化或解释吗-用了什么方法" tabindex="-1"><a class="header-anchor" href="#_5️⃣-你做过模型结果的可视化或解释吗-用了什么方法"><span>5️⃣ <strong>你做过模型结果的可视化或解释吗？用了什么方法？</strong></span></a></h3><p>是的。我使用了：</p><ul><li><strong>特征重要性条形图</strong>：对 Random Forest / XGBoost 输出的 <code>feature_importances_</code> 进行可视化，直观展示各特征贡献；</li><li><strong>预测 vs 实际值散点图</strong>：判断回归误差是否集中，检验模型是否偏差；</li><li><strong>情绪类型和互动的关系图</strong>：用于理解情绪对传播效果的影响；</li><li><strong>聚类雷达图 / 箱线图</strong>：用于分析用户群体之间的差异。</li></ul><h3 id="_7️⃣-项目中有没有遇到模型过拟合-你是怎么处理的" tabindex="-1"><a class="header-anchor" href="#_7️⃣-项目中有没有遇到模型过拟合-你是怎么处理的"><span>7️⃣ <strong>项目中有没有遇到模型过拟合？你是怎么处理的？</strong></span></a></h3><p>有遇到模型拟合度过高的情况，比如在训练集上 R² 接近 1，RMSE 极低。为了防止过拟合，我做了：</p><ul><li><strong>启用早停机制</strong>（XGBoost <code>early_stopping_rounds=10</code>）；</li><li><strong>设置较小的树深度（max_depth=5）</strong>；</li><li>进行 <strong>特征选择和降维</strong>；</li><li>保持训练/验证集 80:20 分割，并使用随机种子固定划分。</li></ul><h2 id="体育传播大模型" tabindex="-1"><a class="header-anchor" href="#体育传播大模型"><span>体育传播大模型</span></a></h2><h4 id="_1️⃣-图像识别数据的清洗与构建" tabindex="-1"><a class="header-anchor" href="#_1️⃣-图像识别数据的清洗与构建"><span>1️⃣ <strong>图像识别数据的清洗与构建</strong></span></a></h4><ul><li>从比赛视频中抽帧，提取出图像数据；</li><li>过滤掉低质量帧（如模糊、遮挡），剔除无效样本；</li><li>对视频画面中的 <strong>运动员、球、球拍等目标物体</strong>，协助完成图像标注或验证数据一致性；</li><li>清洗过程中注意统一图像尺寸、通道数，并处理无标签或错误标签数据，保证模型训练数据干净可靠。</li></ul><hr><h4 id="_2️⃣-结构化标签对齐与数据融合" tabindex="-1"><a class="header-anchor" href="#_2️⃣-结构化标签对齐与数据融合"><span>2️⃣ <strong>结构化标签对齐与数据融合</strong></span></a></h4><ul><li>将识别结果（如运动员位置、动作类型、事件时间点）格式化为 JSON 或表格格式；</li><li>融合视频帧识别输出和文本解说、历史比分等多源数据，构建训练样本；</li><li>处理异常值，比如识别输出坐标超出画面、动作持续时间异常等，通过设阈值、滑窗统计剔除。</li></ul><hr><h4 id="_3️⃣-数据驱动模型微调准备" tabindex="-1"><a class="header-anchor" href="#_3️⃣-数据驱动模型微调准备"><span>3️⃣ <strong>数据驱动模型微调准备</strong></span></a></h4><ul><li>为动作识别模型（如用于识别发球、扣杀等）准备带标签的图像序列；</li><li>对每类动作平衡样本分布，防止模型偏向主流类别；</li><li>为多模态模型准备图像 + 文本 + 元数据（如时间戳、比分）的联合输入。</li></ul><h3 id="✅-面试回答示范-你们图像识别抽取关键帧用了什么技术" tabindex="-1"><a class="header-anchor" href="#✅-面试回答示范-你们图像识别抽取关键帧用了什么技术"><span>✅ 面试回答示范：你们图像识别抽取关键帧用了什么技术？</span></a></h3><p><strong>答：</strong></p><p>我们在图像识别阶段，采用了 <strong>YOLO 系列目标检测模型</strong>（如 YOLOv5 / YOLOv8）来对视频帧中的关键目标进行实时检测，例如运动员、球拍、球、赛场元素等。</p><p>针对体育视频，我们的处理流程大致如下：</p><ol><li><strong>视频抽帧</strong>：首先用 OpenCV 以固定帧率（如 5fps 或 10fps）从直播视频中提取图像帧；</li><li><strong>帧级目标检测</strong>：对每一帧图像输入 YOLO 模型，检测是否包含有效目标（如运动员或球）；</li><li><strong>动作判断/目标过滤</strong>： <ul><li>若帧中无目标或目标置信度过低（如置信度 &lt; 0.5），自动丢弃；</li><li>若连续帧中目标框变化剧烈，认为是发生了关键动作（如击球），保留关键帧；</li></ul></li><li><strong>关键帧选取</strong>： <ul><li>利用目标检测 + 位置变化（例如球员位置、球运行轨迹突变）来提取“动作转折点帧”；</li><li>部分场景我们也通过“事件识别模型”辅助判断（如识别“得分瞬间”）。</li></ul></li></ol><p>通过这种方式，我们实现了 <strong>从原始视频中自动抽取高价值帧</strong>，为后续的动作识别、事件分类、技术分析等任务提供干净、有效的输入。</p><h3 id="✅-1-图像、文本、元数据是否都能转化为向量-写入向量数据库" tabindex="-1"><a class="header-anchor" href="#✅-1-图像、文本、元数据是否都能转化为向量-写入向量数据库"><span>✅ <strong>1. 图像、文本、元数据是否都能转化为向量，写入向量数据库？</strong></span></a></h3><p><strong>是的，可以，而且这是构建多模态大模型应用的关键一步。</strong></p><p>在你参与的“体育视觉传播大模型”项目中，图像、文本、元数据可以统一向量化后写入向量数据库（如 FAISS、Milvus、Weaviate），用于 <strong>多模态检索、RAG（检索增强生成）</strong>、知识召回等任务：</p><table><thead><tr><th>数据类型</th><th>转向量方式</th><th>说明</th></tr></thead><tbody><tr><td>图像帧</td><td>使用视觉编码器（如 CLIP 的视觉模型、ResNet、ViT）提取图像向量</td><td>得到图像的语义特征</td></tr><tr><td>文本描述</td><td>使用语言模型（如 BERT、RoBERTa、Sentence-BERT、text-embedding-ada-002）提取文本向量</td><td>适合直播解说、评论、规则等文本</td></tr><tr><td>元数据（时间、比分、位置）</td><td>结构化编码 + MLP + 拼接向量化 或 数值归一化后直接编码</td><td>可以与图像/文本拼接组成 joint embedding</td></tr></tbody></table><p>这些向量可以写入向量数据库，用于：</p><ul><li>相似视频片段召回；</li><li>多模态问答；</li><li>比赛事件快速检索；</li><li>构建“体育知识图谱”中的节点表示。</li></ul><h3 id="✅-2-这个项目可能用什么大模型-api" tabindex="-1"><a class="header-anchor" href="#✅-2-这个项目可能用什么大模型-api"><span>✅ <strong>2. 这个项目可能用什么大模型 API？</strong></span></a></h3><p>结合你项目中提到的内容（RAG + NLP + 视频分析 + 内容生成 + 问答），可能会用到以下几类大模型或 API：</p><hr><h4 id="📘-文本生成-问答类大模型-api" tabindex="-1"><a class="header-anchor" href="#📘-文本生成-问答类大模型-api"><span>📘 文本生成/问答类大模型 API</span></a></h4><table><thead><tr><th>模型 / API</th><th>作用</th><th>备注</th></tr></thead><tbody><tr><td><strong>OpenAI GPT-4/GPT-3.5</strong></td><td>比赛播报生成、问答、摘要</td><td>支持多轮对话、多模态（gpt-4-vision）</td></tr><tr><td><strong>ChatGLM</strong></td><td>国产 LLM 替代，问答/摘要</td><td>更适合私有化部署</td></tr><tr><td><strong>T5 / FLAN-T5 / LLaMA2</strong></td><td>报道生成、摘要</td><td>训练自有内容生成模型时使用</td></tr><tr><td><strong>Baichuan / Qwen / DeepSeek</strong></td><td>中文赛事内容理解与生成</td><td>与国内平台兼容性好</td></tr></tbody></table><hr><h4 id="🖼-图像-文本-多模态大模型" tabindex="-1"><a class="header-anchor" href="#🖼-图像-文本-多模态大模型"><span>🖼 图像+文本 多模态大模型</span></a></h4><table><thead><tr><th>模型</th><th>功能</th><th>场景</th></tr></thead><tbody><tr><td><strong>CLIP</strong>（OpenAI）</td><td>图像编码，图文对齐</td><td>提取图像语义向量，配合文本进行相似搜索</td></tr><tr><td><strong>BLIP-2 / Flamingo / MiniGPT-4</strong></td><td>图文问答 / 生成</td><td>输入图像生成文字、问答</td></tr><tr><td><strong>GPT-4-Vision API</strong></td><td>视觉问答，图片转描述</td><td>比如“这帧是扣杀动作吗？”</td></tr></tbody></table><h2 id="✅-flask-是干什么的" tabindex="-1"><a class="header-anchor" href="#✅-flask-是干什么的"><span>✅ Flask 是干什么的？</span></a></h2><p><strong>Flask 是一个轻量级的 Python Web 应用框架</strong>，用于快速搭建后端服务，特别适合做：</p><table><thead><tr><th>应用场景</th><th>说明</th></tr></thead><tbody><tr><td>✅ 接收前端请求</td><td>接收网页、App、curl 发来的 HTTP 请求（GET、POST 等）</td></tr><tr><td>✅ 返回模型结果</td><td>用于部署机器学习模型，把预测结果通过接口返回</td></tr><tr><td>✅ 构建 API 接口</td><td>提供 RESTful API，比如 <code>/predict</code>, <code>/query</code>, <code>/upload</code> 等</td></tr><tr><td>✅ 搭建原型系统</td><td>非常适合快速搭建演示/测试系统，支持与前端联动</td></tr></tbody></table><h2 id="✅-hive-的主要作用" tabindex="-1"><a class="header-anchor" href="#✅-hive-的主要作用"><span>✅ Hive 的主要作用：</span></a></h2><table><thead><tr><th>功能</th><th>说明</th></tr></thead><tbody><tr><td>✅ 数据仓库</td><td>用来存储、管理、分析 PB 级别的结构化数据</td></tr><tr><td>✅ SQL 查询引擎</td><td>提供类 SQL 的查询语法（HQL），让不懂 MapReduce 的人也能查询 Hadoop 数据</td></tr><tr><td>✅ 和 Hadoop 集成</td><td>背后实际运行的是 MapReduce 或 Spark 作业，但你只用写 SQL</td></tr><tr><td>✅ 支持表、分区、视图</td><td>像传统数据库一样管理数据结构，适合离线数仓分析</td></tr></tbody></table><h2 id="✅-pytorch-是什么-怎么读-体现在你哪段经历" tabindex="-1"><a class="header-anchor" href="#✅-pytorch-是什么-怎么读-体现在你哪段经历"><span>✅ PyTorch 是什么，怎么读，体现在你哪段经历？</span></a></h2><h3 id="📖-怎么读" tabindex="-1"><a class="header-anchor" href="#📖-怎么读"><span>📖 <strong>怎么读：</strong></span></a></h3><blockquote><p>/ˈpaɪ.tɔːrtʃ/，读作 “派托奇”</p></blockquote><h3 id="🧠-是做什么的" tabindex="-1"><a class="header-anchor" href="#🧠-是做什么的"><span>🧠 <strong>是做什么的？</strong></span></a></h3><p>PyTorch 是一个<strong>开源的深度学习框架</strong>，由 Facebook 开发，广泛用于：</p><ul><li>构建神经网络（如 CNN、RNN、BERT）；</li><li>执行 GPU 加速的模型训练与推理；</li><li>支持动态图，调试灵活、可扩展性强；</li><li>学术研究、工业部署都用得非常多。</li></ul><h3 id="📌-在你简历中的体现" tabindex="-1"><a class="header-anchor" href="#📌-在你简历中的体现"><span>📌 <strong>在你简历中的体现：</strong></span></a></h3><p>你在以下经历中显然有 PyTorch 实操：</p><ul><li>✅ <strong>“基于BERT的中文分词模型优化研究”</strong>：你提到了模型微调、GPU加速训练、消融实验等，这些一般是用 PyTorch + HuggingFace 实现；</li><li>✅ <strong>“体育视觉传播大模型”项目</strong>：涉及视觉识别、模型训练、后端部署，这种项目往往也是基于 PyTorch 实现深度视觉模型如 YOLO、CLIP 等。</li></ul><hr><h2 id="✅-matplotlib-是什么-怎么读-体现在你哪段经历" tabindex="-1"><a class="header-anchor" href="#✅-matplotlib-是什么-怎么读-体现在你哪段经历"><span>✅ Matplotlib 是什么，怎么读，体现在你哪段经历？</span></a></h2><h3 id="📖-怎么读-1" tabindex="-1"><a class="header-anchor" href="#📖-怎么读-1"><span>📖 <strong>怎么读：</strong></span></a></h3><blockquote><p>/ˈmæt.plə.tlɪb/，读作 “麦特普洛特利布”</p></blockquote><h3 id="🧠-是做什么的-1" tabindex="-1"><a class="header-anchor" href="#🧠-是做什么的-1"><span>🧠 <strong>是做什么的？</strong></span></a></h3><p>Matplotlib 是 Python 中最常用的<strong>科学绘图库</strong>，用于：</p><ul><li>绘制图表（柱状图、折线图、散点图、热力图等）；</li><li>可视化模型结果（如 loss 曲线、特征重要性）；</li><li>辅助数据分析报告生成。</li></ul><h3 id="📌-在你简历中的体现-1" tabindex="-1"><a class="header-anchor" href="#📌-在你简历中的体现-1"><span>📌 <strong>在你简历中的体现：</strong></span></a></h3><ul><li>✅ <strong>“社交媒体用户行为分析项目”</strong>：你提到了可视化分析和特征重要性评估，这些结果一般就是用 Matplotlib（或 Seaborn）画图实现的；</li><li>✅ 你简历的“计算机能力”也标出了 Matplotlib，合理对应了你做模型分析、情绪传播图、雷达图等工作。</li></ul><hr><h3 id="🧑‍💼-面试中你可以这么说" tabindex="-1"><a class="header-anchor" href="#🧑‍💼-面试中你可以这么说"><span>🧑‍💼 面试中你可以这么说：</span></a></h3><blockquote><p>我在模型训练和数据分析项目中都使用过 PyTorch 和 Matplotlib。比如在中文分词任务中，我基于 PyTorch 微调了 BERT，并在 GPU 上完成训练；在社交媒体行为分析项目中，我用 Matplotlib 绘制了特征重要性条形图、情绪互动散点图和用户聚类雷达图，辅助模型解释和策略制定。</p></blockquote><h2 id="会问什么问题" tabindex="-1"><a class="header-anchor" href="#会问什么问题"><span>会问什么问题？</span></a></h2><h3 id="✅-问题-1-你在-体育视觉传播大模型-项目中提到参与模型微调-请问你们是基于什么模型微调的-为什么需要微调" tabindex="-1"><a class="header-anchor" href="#✅-问题-1-你在-体育视觉传播大模型-项目中提到参与模型微调-请问你们是基于什么模型微调的-为什么需要微调"><span>✅ 问题 1：你在“体育视觉传播大模型”项目中提到参与模型微调，请问你们是基于什么模型微调的？为什么需要微调？</span></a></h3><p><strong>参考回答：</strong><br> 我们主要是基于预训练的目标检测模型（比如 YOLO 或 CLIP）进行微调，用于识别体育赛事中的特定对象（如乒乓球、球员、球拍等）。<br> 之所以进行微调，是因为虽然这些模型在通用数据集上训练过，但在实际比赛画面中存在视角、遮挡、动作模糊等情况，所以需要用我们采集的有标注数据对模型进行微调，提升它在特定场景下的识别准确率。</p><h3 id="✅-问题-2-你在项目中提到参与图像数据清洗-具体做了哪些工作-如何判断一帧是否是-有效帧" tabindex="-1"><a class="header-anchor" href="#✅-问题-2-你在项目中提到参与图像数据清洗-具体做了哪些工作-如何判断一帧是否是-有效帧"><span>✅ 问题 2：你在项目中提到参与图像数据清洗，具体做了哪些工作？如何判断一帧是否是“有效帧”？</span></a></h3><p><strong>参考回答：</strong><br> 我主要参与了图像帧的筛选与清洗工作，包括：</p><ul><li>用 OpenCV 对比赛视频进行抽帧；</li><li>利用 YOLO 对每帧检测是否包含目标（如运动员或球），只保留置信度高的帧；</li><li>剔除重复帧和模糊帧（通过图像清晰度评估或帧差法）；</li><li>最后统一图像尺寸并格式化标签文件（如转换成 YOLO 标注格式）。</li></ul><p>我们判断“有效帧”主要依据目标置信度、目标完整性和帧间变化等标准。</p><h3 id="✅-问题-3-你提到熟悉-pytorch-在图像模型训练中你一般怎么组织数据集-怎么处理图片和标签" tabindex="-1"><a class="header-anchor" href="#✅-问题-3-你提到熟悉-pytorch-在图像模型训练中你一般怎么组织数据集-怎么处理图片和标签"><span>✅ 问题 3：你提到熟悉 PyTorch，在图像模型训练中你一般怎么组织数据集？怎么处理图片和标签？</span></a></h3><p><strong>参考回答：</strong><br> 我会使用 PyTorch 的 <code>Dataset</code> 和 <code>DataLoader</code> 类来组织图像数据，流程包括：</p><ul><li>读取图片路径与标签（如 bounding box、类别）；</li><li>用paddle.vision <code>transform</code> 对图像进行 Resize、Normalize归一化、ToTensor转化张量；</li><li>标签可以是 <code>.txt</code> 文件、COCO 格式或其他结构化 json，需要转换成统一格式；</li><li>如果是检测任务，返回图像 tensor 和对应 bbox对应边界框 标签；如果是分类任务，就返回图像 + 类别 index。</li></ul><h3 id="✅-问题-4-你说你用过-hadoop-和-hive-在图像识别项目中它们是做什么的" tabindex="-1"><a class="header-anchor" href="#✅-问题-4-你说你用过-hadoop-和-hive-在图像识别项目中它们是做什么的"><span>✅ 问题 4：你说你用过 Hadoop 和 Hive，在图像识别项目中它们是做什么的？</span></a></h3><p><strong>参考回答：</strong><br> Hive 和 Hadoop 在我们项目中用于管理和处理大规模结构化数据，比如从图像识别中导出的结构化标签、动作事件、比赛信息等。<br> 具体做法包括：</p><ul><li>用 Hive 存储分析后的识别结果（如时间戳 + 动作类型 + 坐标）；</li><li>通过 HQL 查询选手某场比赛的全部动作序列；</li><li>Hadoop 用来支持数据的离线分布式处理，比如将数百场视频的标签数据统一归档处理。</li></ul><p>这对后续的技战术分析和内容生成非常关键。</p><h3 id="✅-问题-5-你提到做过可视化-用-matplotlib-做了哪些图-用在什么场景" tabindex="-1"><a class="header-anchor" href="#✅-问题-5-你提到做过可视化-用-matplotlib-做了哪些图-用在什么场景"><span>✅ 问题 5：你提到做过可视化，用 Matplotlib 做了哪些图？用在什么场景？</span></a></h3><p><strong>参考回答：</strong><br> 在社交媒体用户分析项目中，我用 Matplotlib 做了如下几种图：</p><ul><li>条形图：展示不同情绪类型对应的平均互动数；</li><li>散点图：情绪 vs 点赞数量的分布；</li><li>雷达图：展示聚类后用户的多维行为特征（如互动量、内容长度、发布时间等）；</li><li>折线图：训练过程中的 loss 曲线，用于监控模型是否过拟合。</li></ul><p>这些图有助于我们理解模型、解释特征，并用于写报告和展示结果。</p>`,164)]))}const c=a(l,[["render",r]]),h=JSON.parse('{"path":"/zh/internship/self_introduce.html","title":"自我介绍","lang":"zh-CN","frontmatter":{"title":"自我介绍","icon":"python","date":"2025-01-22T14:52:39.000Z","author":"XiaoXianYue","isOriginal":true,"category":["智慧星光实习笔记"],"tag":["智慧星光实习笔记"],"sticky":false,"star":false,"article":true,"timeline":true,"image":false,"navbar":true,"sidebarIcon":true,"headerDepth":5,"lastUpdated":true,"editLink":false,"backToTop":true,"toc":true,"feed":false,"seo":false,"head":[]},"git":{"createdTime":1753061132000,"updatedTime":1753061132000,"contributors":[{"name":"Xiaoxianyue","username":"Xiaoxianyue","email":"2310219843@qq.com","commits":1,"url":"https://github.com/Xiaoxianyue"}]},"readingTime":{"minutes":18.41,"words":5524},"filePathRelative":"zh/internship/self_introduce.md","localizedDate":"2025年1月22日"}');export{c as comp,h as data};
