<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.20" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.74" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme="dark"] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://bougiemoonintaurus/zh/NLP/bert_essay.html"><meta property="og:site_name" content="奶酪奶酪"><meta property="og:title" content="基于BERT的分词模型"><meta property="og:description" content="image-20250414113933731image-20250414113933731 1. Experiment Design Since Chinese lacks clear word boundaries, word segmentation is crucial in natural language processing (NLP)...."><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2025-07-21T01:25:32.000Z"><meta property="article:author" content="XiaoXianYue"><meta property="article:tag" content="大三下"><meta property="article:tag" content="NLP"><meta property="article:published_time" content="2025-04-07T16:57:04.000Z"><meta property="article:modified_time" content="2025-07-21T01:25:32.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"基于BERT的分词模型","image":[""],"datePublished":"2025-04-07T16:57:04.000Z","dateModified":"2025-07-21T01:25:32.000Z","author":[{"@type":"Person","name":"XiaoXianYue"}]}</script><title>基于BERT的分词模型 | 奶酪奶酪</title><meta name="description" content="image-20250414113933731image-20250414113933731 1. Experiment Design Since Chinese lacks clear word boundaries, word segmentation is crucial in natural language processing (NLP)....">
    <link rel="preload" href="/assets/style-D3YEg4E9.css" as="style"><link rel="stylesheet" href="/assets/style-D3YEg4E9.css">
    <link rel="modulepreload" href="/assets/app-84lBMjzT.js"><link rel="modulepreload" href="/assets/bert_essay.html-69-9Q689.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-DlAUqK2U.js">
    <link rel="prefetch" href="/assets/index.html-QUQgzYbo.js" as="script"><link rel="prefetch" href="/assets/bolg.html-BDvoOvFv.js" as="script"><link rel="prefetch" href="/assets/intro.html-3QE0wG9e.js" as="script"><link rel="prefetch" href="/assets/MQTT.html--bPQC7PM.js" as="script"><link rel="prefetch" href="/assets/index.html-CSaaY76_.js" as="script"><link rel="prefetch" href="/assets/intro.html-BQI3pyL1.js" as="script"><link rel="prefetch" href="/assets/ch03.html-ykfqB9Ex.js" as="script"><link rel="prefetch" href="/assets/ch07.html-BRpEBR0T.js" as="script"><link rel="prefetch" href="/assets/notes.html-D1RXx2nF.js" as="script"><link rel="prefetch" href="/assets/question.html-BNB-SJoJ.js" as="script"><link rel="prefetch" href="/assets/s_ass.html-CbGOoTcu.js" as="script"><link rel="prefetch" href="/assets/s_ass01.html-Cuny9FB9.js" as="script"><link rel="prefetch" href="/assets/s_ass02.html-DQ_eNEnc.js" as="script"><link rel="prefetch" href="/assets/s_ass03.html-C161D25E.js" as="script"><link rel="prefetch" href="/assets/s_ass04.html-DgMatPAw.js" as="script"><link rel="prefetch" href="/assets/s_ass05.html-K2n9HzNW.js" as="script"><link rel="prefetch" href="/assets/s_ass06.html-D5a3zdoG.js" as="script"><link rel="prefetch" href="/assets/s_ass07.html-sRspMkaT.js" as="script"><link rel="prefetch" href="/assets/s_ass08.html-CPSPFVVO.js" as="script"><link rel="prefetch" href="/assets/s_ass09.html-Dhbc0i7C.js" as="script"><link rel="prefetch" href="/assets/s_ass10.html-gzT6L_cB.js" as="script"><link rel="prefetch" href="/assets/s_ass11.html-BfP_2pQe.js" as="script"><link rel="prefetch" href="/assets/s_ass12.html-rFMTJqdd.js" as="script"><link rel="prefetch" href="/assets/Assessed_work.html-D0yovSG3.js" as="script"><link rel="prefetch" href="/assets/Report.html-CsCRl8-7.js" as="script"><link rel="prefetch" href="/assets/Week01_ass.html-Do1vHNAK.js" as="script"><link rel="prefetch" href="/assets/Week02.html-gw-5grzF.js" as="script"><link rel="prefetch" href="/assets/Week0203.html-vzLin1NU.js" as="script"><link rel="prefetch" href="/assets/Week02_ass.html-DLHt6wWO.js" as="script"><link rel="prefetch" href="/assets/Week03.html-CxcG7Sqx.js" as="script"><link rel="prefetch" href="/assets/Week04.html-D5mxZYpn.js" as="script"><link rel="prefetch" href="/assets/Week0405.html-C6JlHReG.js" as="script"><link rel="prefetch" href="/assets/Week05.html-Ch0vgVIW.js" as="script"><link rel="prefetch" href="/assets/Week06.html-aRixCMqy.js" as="script"><link rel="prefetch" href="/assets/Week0607.html-CyfiqqF3.js" as="script"><link rel="prefetch" href="/assets/Week07.html-B1udnIfh.js" as="script"><link rel="prefetch" href="/assets/Week08.html-CjuTXKAA.js" as="script"><link rel="prefetch" href="/assets/Week080910.html-BCIe6uBQ.js" as="script"><link rel="prefetch" href="/assets/Week09.html-Mp2yLc2A.js" as="script"><link rel="prefetch" href="/assets/Week0910.html-C97dcD3T.js" as="script"><link rel="prefetch" href="/assets/Week10.html-CQyh2Uq5.js" as="script"><link rel="prefetch" href="/assets/mistakebook.html-C4YFq3f3.js" as="script"><link rel="prefetch" href="/assets/summary.html-BeKP8aJ1.js" as="script"><link rel="prefetch" href="/assets/CS_Class_01.html-Ch-y_FBu.js" as="script"><link rel="prefetch" href="/assets/CS_Class_02.html-DHzxv3DC.js" as="script"><link rel="prefetch" href="/assets/CS_Method.html-RNl8rocu.js" as="script"><link rel="prefetch" href="/assets/CS_array_string.html-D6OMPkIw.js" as="script"><link rel="prefetch" href="/assets/CS_basic_conception.html-CDZuPSId.js" as="script"><link rel="prefetch" href="/assets/CS_condition_loop.html-BJvXYcVV.js" as="script"><link rel="prefetch" href="/assets/CS_delegate.html-D0zmAA3t.js" as="script"><link rel="prefetch" href="/assets/1.html-CFpNK6c_.js" as="script"><link rel="prefetch" href="/assets/2.html-BCoE_LtN.js" as="script"><link rel="prefetch" href="/assets/3.html-hFtoy3g_.js" as="script"><link rel="prefetch" href="/assets/4.html-DLUvqtQw.js" as="script"><link rel="prefetch" href="/assets/01.html-BgJryf42.js" as="script"><link rel="prefetch" href="/assets/02.html-DOA2JYH5.js" as="script"><link rel="prefetch" href="/assets/03.html-BJbIeyUF.js" as="script"><link rel="prefetch" href="/assets/04.html-CWat5Evp.js" as="script"><link rel="prefetch" href="/assets/05.html-BDKfbPUu.js" as="script"><link rel="prefetch" href="/assets/DS_review.html-BFvQ5hvc.js" as="script"><link rel="prefetch" href="/assets/01.html-CN61-ZuU.js" as="script"><link rel="prefetch" href="/assets/02.html-Dj4pJVhU.js" as="script"><link rel="prefetch" href="/assets/03.html-CeIGUZ_N.js" as="script"><link rel="prefetch" href="/assets/ass_408.html-CTb5GOWw.js" as="script"><link rel="prefetch" href="/assets/bottleneck.html-D3Y-pPzx.js" as="script"><link rel="prefetch" href="/assets/cnn.html-Dzifm-hi.js" as="script"><link rel="prefetch" href="/assets/dp_api.html-CN6ffcFo.js" as="script"><link rel="prefetch" href="/assets/early_stop.html-CNHvtwhB.js" as="script"><link rel="prefetch" href="/assets/mlp_exe.html-BIITKQ_T.js" as="script"><link rel="prefetch" href="/assets/rnn.html-DC-M_Nu0.js" as="script"><link rel="prefetch" href="/assets/01-03.html-BVEkhQSU.js" as="script"><link rel="prefetch" href="/assets/CNN.html-CX5xpY92.js" as="script"><link rel="prefetch" href="/assets/OA.html-CcmlFhpJ.js" as="script"><link rel="prefetch" href="/assets/RNN.html-DiNXVdvF.js" as="script"><link rel="prefetch" href="/assets/Daily_assignment.html-BT1ECiDs.js" as="script"><link rel="prefetch" href="/assets/math.html-OSykxTWf.js" as="script"><link rel="prefetch" href="/assets/Week01.html-med7BKV3.js" as="script"><link rel="prefetch" href="/assets/Week02.html-DaD4SCmK.js" as="script"><link rel="prefetch" href="/assets/Week03.html-DrQjXlK0.js" as="script"><link rel="prefetch" href="/assets/Week04.html-0gdmekB-.js" as="script"><link rel="prefetch" href="/assets/Week05.html-DFuy358P.js" as="script"><link rel="prefetch" href="/assets/final_report.html-D84H8_sJ.js" as="script"><link rel="prefetch" href="/assets/assignment.html-DJLolgLr.js" as="script"><link rel="prefetch" href="/assets/ch02.html-fG4YxIHU.js" as="script"><link rel="prefetch" href="/assets/ch03.html-DHLTK4xn.js" as="script"><link rel="prefetch" href="/assets/ch04.html-CkZ-mwCg.js" as="script"><link rel="prefetch" href="/assets/ch05.html-Cmm8zMQI.js" as="script"><link rel="prefetch" href="/assets/ch06.html-oZKUX5c-.js" as="script"><link rel="prefetch" href="/assets/ch07.html-CPYgigXy.js" as="script"><link rel="prefetch" href="/assets/ch08.html-D8B5W61f.js" as="script"><link rel="prefetch" href="/assets/ch09.html-GCDWuoce.js" as="script"><link rel="prefetch" href="/assets/ch10.html-DU4TaHHZ.js" as="script"><link rel="prefetch" href="/assets/ch11.html-36FV_4MN.js" as="script"><link rel="prefetch" href="/assets/ch12.html-DcOStlOd.js" as="script"><link rel="prefetch" href="/assets/enreport.html-BF_qzgXE.js" as="script"><link rel="prefetch" href="/assets/re_ve.html-CfZtHCyf.js" as="script"><link rel="prefetch" href="/assets/report.html-CsnuD1co.js" as="script"><link rel="prefetch" href="/assets/sum.html-D8hoibjI.js" as="script"><link rel="prefetch" href="/assets/01.html-Bsyjw0BP.js" as="script"><link rel="prefetch" href="/assets/02.html-CydOjJ0y.js" as="script"><link rel="prefetch" href="/assets/03.html-CoD1V-Ns.js" as="script"><link rel="prefetch" href="/assets/04.html-C8-zrKiF.js" as="script"><link rel="prefetch" href="/assets/05.html-DUOfRWIU.js" as="script"><link rel="prefetch" href="/assets/Review_question.html-B6s8yZYd.js" as="script"><link rel="prefetch" href="/assets/exercise.html-BAARS1l_.js" as="script"><link rel="prefetch" href="/assets/question02.html-ClBUhqJC.js" as="script"><link rel="prefetch" href="/assets/task.html-DQYHt-oX.js" as="script"><link rel="prefetch" href="/assets/A_L.html-DOhvt5aB.js" as="script"><link rel="prefetch" href="/assets/Coventry_Class_01.html-BM7MlK5A.js" as="script"><link rel="prefetch" href="/assets/Coventry_Class_02.html-ZmZ2Rqtt.js" as="script"><link rel="prefetch" href="/assets/Coventry_Method.html-B803M_4s.js" as="script"><link rel="prefetch" href="/assets/Delegates_and_Events.html-uHvxia9O.js" as="script"><link rel="prefetch" href="/assets/Generics.html-DvaSraEl.js" as="script"><link rel="prefetch" href="/assets/Inheritance.html-yVuBB6_b.js" as="script"><link rel="prefetch" href="/assets/Interfaces.html-BzS4S1nw.js" as="script"><link rel="prefetch" href="/assets/review.html-BwFY5W7k.js" as="script"><link rel="prefetch" href="/assets/Agile_01.html-xIFRuz7x.js" as="script"><link rel="prefetch" href="/assets/Agile_02.html-DP75L8ZP.js" as="script"><link rel="prefetch" href="/assets/Data_modelling.html-DlvPjA1V.js" as="script"><link rel="prefetch" href="/assets/Ethics.html-PO0vHuAJ.js" as="script"><link rel="prefetch" href="/assets/Gof.html-BfGp-nbW.js" as="script"><link rel="prefetch" href="/assets/Gof_02.html-skqbQ_aD.js" as="script"><link rel="prefetch" href="/assets/OO_conception.html-CNZ-k-wz.js" as="script"><link rel="prefetch" href="/assets/OO_principle.html-C-INbLPa.js" as="script"><link rel="prefetch" href="/assets/Software_Design.html-CzVG-xMk.js" as="script"><link rel="prefetch" href="/assets/Software_Testing.html-0Tb4XXYA.js" as="script"><link rel="prefetch" href="/assets/TOEFL_speaking1.html-CHq4_tTd.js" as="script"><link rel="prefetch" href="/assets/TOEFL_speaking2.html-CxaJCG_r.js" as="script"><link rel="prefetch" href="/assets/TOEFL_speaking3.html-BTCyy3s5.js" as="script"><link rel="prefetch" href="/assets/TOEFL_speaking4.html-O9p4RIhb.js" as="script"><link rel="prefetch" href="/assets/TOEFL_writing.html-CQdcnBOF.js" as="script"><link rel="prefetch" href="/assets/accumulation.html-D1bx1-tJ.js" as="script"><link rel="prefetch" href="/assets/ad_wr.html-DaI35klt.js" as="script"><link rel="prefetch" href="/assets/assi_dairy.html-DJdACvqB.js" as="script"><link rel="prefetch" href="/assets/correction_notebook.html-DgB011eq.js" as="script"><link rel="prefetch" href="/assets/listen_class.html-DysSitIf.js" as="script"><link rel="prefetch" href="/assets/logical_chain.html-DqJ-sQXx.js" as="script"><link rel="prefetch" href="/assets/question.html-DOidLUxg.js" as="script"><link rel="prefetch" href="/assets/reading.html-B47G7Iqj.js" as="script"><link rel="prefetch" href="/assets/reading01.html-Df5LkEBN.js" as="script"><link rel="prefetch" href="/assets/reading02.html-Dt491qcH.js" as="script"><link rel="prefetch" href="/assets/spclass.html-BVJwyjZx.js" as="script"><link rel="prefetch" href="/assets/spk_mtrl.html-Bs-vvJX1.js" as="script"><link rel="prefetch" href="/assets/translation_training.html-DF3d723r.js" as="script"><link rel="prefetch" href="/assets/wr_class.html-BkT8Gfyu.js" as="script"><link rel="prefetch" href="/assets/wr_mistake.html-DD2A8ME9.js" as="script"><link rel="prefetch" href="/assets/stock.html-v3-18EWZ.js" as="script"><link rel="prefetch" href="/assets/Agricultural_Bank_of_China.html-CfGalI-E.js" as="script"><link rel="prefetch" href="/assets/Agriculture_robot.html-BD-f9BPV.js" as="script"><link rel="prefetch" href="/assets/istarshine.html-DqdR0pjn.js" as="script"><link rel="prefetch" href="/assets/istarshine01.html-CzFor8Rn.js" as="script"><link rel="prefetch" href="/assets/self_introduce.html-B-u6Yd1T.js" as="script"><link rel="prefetch" href="/assets/Data-type.html-CymMaRCK.js" as="script"><link rel="prefetch" href="/assets/Dictionary.html-o3TWDNIw.js" as="script"><link rel="prefetch" href="/assets/Document.html-DpTnUNnC.js" as="script"><link rel="prefetch" href="/assets/For.html-DtZq30mG.js" as="script"><link rel="prefetch" href="/assets/Homework1.html-Cc065WMN.js" as="script"><link rel="prefetch" href="/assets/If_Homework.html-DuSmLJGE.js" as="script"><link rel="prefetch" href="/assets/List.html-DpItdwAN.js" as="script"><link rel="prefetch" href="/assets/List_Homework.html-Ql2jORkj.js" as="script"><link rel="prefetch" href="/assets/Numeric.html-x2rwFYcK.js" as="script"><link rel="prefetch" href="/assets/Set.html-BdDQlCH1.js" as="script"><link rel="prefetch" href="/assets/String.html-DrMVnxiv.js" as="script"><link rel="prefetch" href="/assets/String_Homework.html-Dk9WSRKQ.js" as="script"><link rel="prefetch" href="/assets/Variable.html-BEyRAFp-.js" as="script"><link rel="prefetch" href="/assets/While.html-BAe1bASv.js" as="script"><link rel="prefetch" href="/assets/bool.html-BG2r7SdZ.js" as="script"><link rel="prefetch" href="/assets/function.html-D9z7tiP6.js" as="script"><link rel="prefetch" href="/assets/if.html-Cfdjev33.js" as="script"><link rel="prefetch" href="/assets/recursion.html-oMdG0Rm0.js" as="script"><link rel="prefetch" href="/assets/tuple.html-PeVjy3a_.js" as="script"><link rel="prefetch" href="/assets/404.html-D76wtL2M.js" as="script"><link rel="prefetch" href="/assets/index.html-D9iSJbzp.js" as="script"><link rel="prefetch" href="/assets/index.html-CaNPnU8Y.js" as="script"><link rel="prefetch" href="/assets/index.html-CpR_grgH.js" as="script"><link rel="prefetch" href="/assets/index.html-CXjTjI6t.js" as="script"><link rel="prefetch" href="/assets/index.html-DOD-5aN6.js" as="script"><link rel="prefetch" href="/assets/index.html-ClVrfs5I.js" as="script"><link rel="prefetch" href="/assets/index.html-C1n0NAqC.js" as="script"><link rel="prefetch" href="/assets/index.html-C-lpVN8J.js" as="script"><link rel="prefetch" href="/assets/index.html-B2cX_Q17.js" as="script"><link rel="prefetch" href="/assets/index.html-BXqaqhws.js" as="script"><link rel="prefetch" href="/assets/index.html-DYtM_Z4T.js" as="script"><link rel="prefetch" href="/assets/index.html-BH7EkY35.js" as="script"><link rel="prefetch" href="/assets/index.html-D5qVg5Il.js" as="script"><link rel="prefetch" href="/assets/index.html-mKwv4N4P.js" as="script"><link rel="prefetch" href="/assets/index.html-CzNc8Z2i.js" as="script"><link rel="prefetch" href="/assets/index.html-Dwr-4FA5.js" as="script"><link rel="prefetch" href="/assets/index.html-BcnrIDQW.js" as="script"><link rel="prefetch" href="/assets/index.html-CgkmyniY.js" as="script"><link rel="prefetch" href="/assets/index.html-8n1mya6-.js" as="script"><link rel="prefetch" href="/assets/index.html-D2gXR6lL.js" as="script"><link rel="prefetch" href="/assets/index.html-CQZsZ6nE.js" as="script"><link rel="prefetch" href="/assets/index.html-r5UmHYc8.js" as="script"><link rel="prefetch" href="/assets/index.html-Cp0xZzup.js" as="script"><link rel="prefetch" href="/assets/index.html-CGtnl6OT.js" as="script"><link rel="prefetch" href="/assets/index.html-JWtzarEt.js" as="script"><link rel="prefetch" href="/assets/index.html-C_WTLa6p.js" as="script"><link rel="prefetch" href="/assets/index.html-Cyl5yutW.js" as="script"><link rel="prefetch" href="/assets/index.html-CesX3mUY.js" as="script"><link rel="prefetch" href="/assets/index.html-DjnqdHHD.js" as="script"><link rel="prefetch" href="/assets/index.html-Hs7TGvLk.js" as="script"><link rel="prefetch" href="/assets/index.html-DMgeipi_.js" as="script"><link rel="prefetch" href="/assets/index.html-KW0hXAOn.js" as="script"><link rel="prefetch" href="/assets/index.html-Xkk7pgLT.js" as="script"><link rel="prefetch" href="/assets/index.html-Dju5aeU9.js" as="script"><link rel="prefetch" href="/assets/index.html-TNtLHm9F.js" as="script"><link rel="prefetch" href="/assets/index.html-UeE0l6qh.js" as="script"><link rel="prefetch" href="/assets/index.html-D5ezi8EG.js" as="script"><link rel="prefetch" href="/assets/index.html-Ci2Dg8W2.js" as="script"><link rel="prefetch" href="/assets/index.html-H844SkH1.js" as="script"><link rel="prefetch" href="/assets/index.html-Dpjqe0YM.js" as="script"><link rel="prefetch" href="/assets/index.html-B1ijz358.js" as="script"><link rel="prefetch" href="/assets/index.html-B3uBtze_.js" as="script"><link rel="prefetch" href="/assets/index.html-teMZSwx0.js" as="script"><link rel="prefetch" href="/assets/index.html-D_lwJkWG.js" as="script"><link rel="prefetch" href="/assets/index.html-DHEmKqc-.js" as="script"><link rel="prefetch" href="/assets/index.html-OVH6GD2t.js" as="script"><link rel="prefetch" href="/assets/index.html-M_mbE1HK.js" as="script"><link rel="prefetch" href="/assets/index.html-DZBpgM7U.js" as="script"><link rel="prefetch" href="/assets/index.html-CIiwCl9H.js" as="script"><link rel="prefetch" href="/assets/index.html-CMY-kAv9.js" as="script"><link rel="prefetch" href="/assets/index.html-D0Cb8_83.js" as="script"><link rel="prefetch" href="/assets/index.html-hmPXxEpy.js" as="script"><link rel="prefetch" href="/assets/index.html-CSWLS2NC.js" as="script"><link rel="prefetch" href="/assets/index.html-veiL62Yy.js" as="script"><link rel="prefetch" href="/assets/index.html-Dv6wr6mZ.js" as="script"><link rel="prefetch" href="/assets/index.html-C2ftVb58.js" as="script"><link rel="prefetch" href="/assets/index.html-20rDLDqt.js" as="script"><link rel="prefetch" href="/assets/index.html-CUgzeY46.js" as="script"><link rel="prefetch" href="/assets/index.html-WfSNltCO.js" as="script"><link rel="prefetch" href="/assets/index.html-C3DKcH_t.js" as="script"><link rel="prefetch" href="/assets/index.html-DKyFuAtB.js" as="script"><link rel="prefetch" href="/assets/index.html-XGoAIgUX.js" as="script"><link rel="prefetch" href="/assets/index.html-4CfiZ8ST.js" as="script"><link rel="prefetch" href="/assets/index.html-CSUKz5r1.js" as="script"><link rel="prefetch" href="/assets/index.html-CYd43ot4.js" as="script"><link rel="prefetch" href="/assets/index.html-C11yr8rm.js" as="script"><link rel="prefetch" href="/assets/index.html-BA2ssZDP.js" as="script"><link rel="prefetch" href="/assets/index.html-CadKbx0D.js" as="script"><link rel="prefetch" href="/assets/index.html-DyGak3Po.js" as="script"><link rel="prefetch" href="/assets/index.html-CFWw8jv4.js" as="script"><link rel="prefetch" href="/assets/index.html-CrTPa984.js" as="script"><link rel="prefetch" href="/assets/index.html-hzstyiWf.js" as="script"><link rel="prefetch" href="/assets/index.html-DG30Yigo.js" as="script"><link rel="prefetch" href="/assets/index.html-Dy5Tlwp3.js" as="script"><link rel="prefetch" href="/assets/index.html-CwHU9LcX.js" as="script"><link rel="prefetch" href="/assets/index.html-4ok7tNjU.js" as="script"><link rel="prefetch" href="/assets/index.html-DpO-yTAZ.js" as="script"><link rel="prefetch" href="/assets/index.html-B6CHm0sV.js" as="script"><link rel="prefetch" href="/assets/index.html-C_ryqy7q.js" as="script"><link rel="prefetch" href="/assets/index.html-BIpc_w8X.js" as="script"><link rel="prefetch" href="/assets/index.html-WI8XoBCe.js" as="script"><link rel="prefetch" href="/assets/index.html-L3YDa12o.js" as="script"><link rel="prefetch" href="/assets/index.html-DUDwGF-u.js" as="script"><link rel="prefetch" href="/assets/index.html-B3js6QV3.js" as="script"><link rel="prefetch" href="/assets/index.html-Cr8IgsYR.js" as="script"><link rel="prefetch" href="/assets/index.html-CB-_4uvn.js" as="script"><link rel="prefetch" href="/assets/index.html-CL-EXufL.js" as="script"><link rel="prefetch" href="/assets/index.html-gZeX01yz.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-DXWKOczD.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container external-link-icon has-toc" vp-container><!--[--><header id="navbar" class="vp-navbar" vp-navbar><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><a class="route-link vp-brand" href="/zh/" aria-label="带我回家"><img class="vp-nav-logo" src="/logo.jpg" alt><!----><span class="vp-site-name hide-in-pad">奶酪奶酪</span></a><!--]--></div><div class="vp-navbar-center"><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/zh/" aria-label="MMoon" iconsizing="height"><!--[--><i class="vp-icon iconfont icon-home" sizing="height"></i><!--]-->MMoon<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/#" aria-label="学校科目" iconsizing="height"><!--[--><i class="vp-icon iconfont icon-style" sizing="height"></i><!--]-->学校科目<!----></a></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="编程语言"><!--[--><i class="vp-icon iconfont icon-editor" sizing="height"></i>编程语言<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Python</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/" aria-label="课堂笔记" iconsizing="both"><!---->课堂笔记<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">c#</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/#" aria-label="oop" iconsizing="both"><!--[--><i class="vp-icon iconfont icon-xxx" sizing="both"></i><!--]-->oop<!----></a></li></ul></li></ul></button></div></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/" aria-label="小猫" iconsizing="height"><!--[--><i class="vp-icon iconfont icon-github" sizing="height"></i><!--]-->小猫<!----></a></div></nav><!--]--></div><div class="vp-navbar-end"><!--[--><div class="vp-nav-item"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="选择语言"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon i18n-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="i18n icon" name="i18n" style="width:1rem;height:1rem;vertical-align:middle;"><path d="M379.392 460.8 494.08 575.488l-42.496 102.4L307.2 532.48 138.24 701.44l-71.68-72.704L234.496 460.8l-45.056-45.056c-27.136-27.136-51.2-66.56-66.56-108.544h112.64c7.68 14.336 16.896 27.136 26.112 35.84l45.568 46.08 45.056-45.056C382.976 312.32 409.6 247.808 409.6 204.8H0V102.4h256V0h102.4v102.4h256v102.4H512c0 70.144-37.888 161.28-87.04 210.944L378.88 460.8zM576 870.4 512 1024H409.6l256-614.4H768l256 614.4H921.6l-64-153.6H576zM618.496 768h196.608L716.8 532.48 618.496 768z"></path></svg><!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link auto-link" href="/" aria-label="English" iconsizing="both"><!---->English<!----></a></li><li class="vp-dropdown-item"><a class="route-link route-link-active auto-link" href="/zh/NLP/bert_essay.html" aria-label="简体中文" iconsizing="both"><!---->简体中文<!----></a></li></ul></button></div></div><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/Xiaoxianyue/Xiaoxianyue.github.io" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-color-mode-switch" id="color-mode-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" name="auto" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" name="dark" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" name="light" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><form class="search-box" role="search"><input type="search" placeholder="搜索" autocomplete="off" spellcheck="false" value><!----></form><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar" vp-sidebar><!----><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/zh/" aria-label="MMoon" iconsizing="both"><!--[--><i class="vp-icon iconfont icon-home" sizing="both"></i><!--]-->MMoon<!----></a></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Advanced Algorithms</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Big Data</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">CS</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Data Analysis</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Data Structure</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Deep Learning</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Deep Thinking</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Diary</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">GRE</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Internship</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Java Homework</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Machine Learning</span><span class="vp-arrow end"></span></button><!----></section></li><li><a class="route-link auto-link vp-sidebar-link" href="/zh/MQTT.html" aria-label="MQTT" iconsizing="both"><!--[--><i class="vp-icon iconfont icon-alias" sizing="both"></i><!--]-->MQTT<!----></a></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><!----><span class="vp-sidebar-title">NLP</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/zh/NLP/task.html" aria-label="NLP 课堂实践 01" iconsizing="both"><!--[--><i class="vp-icon iconfont icon-alias" sizing="both"></i><!--]-->NLP 课堂实践 01<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/zh/NLP/01.html" aria-label="NLP 课堂笔记 01" iconsizing="both"><!--[--><i class="vp-icon iconfont icon-alias" sizing="both"></i><!--]-->NLP 课堂笔记 01<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/zh/NLP/exercise.html" aria-label="NLP习题课" iconsizing="both"><!--[--><i class="vp-icon iconfont icon-alias" sizing="both"></i><!--]-->NLP习题课<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/zh/NLP/05.html" aria-label="图模型" iconsizing="both"><!--[--><i class="vp-icon iconfont icon-alias" sizing="both"></i><!--]-->图模型<!----></a></li><li><a class="route-link route-link-active auto-link vp-sidebar-link active" href="/zh/NLP/bert_essay.html" aria-label="基于BERT的分词模型" iconsizing="both"><!--[--><i class="vp-icon iconfont icon-alias" sizing="both"></i><!--]-->基于BERT的分词模型<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/zh/NLP/Review_question.html" aria-label="复习问题" iconsizing="both"><!--[--><i class="vp-icon iconfont icon-alias" sizing="both"></i><!--]-->复习问题<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/zh/NLP/02.html" aria-label="形式语言与自动机及其在NLP中的应用" iconsizing="both"><!--[--><i class="vp-icon iconfont icon-alias" sizing="both"></i><!--]-->形式语言与自动机及其在NLP中的应用<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/zh/NLP/03.html" aria-label="语料库和语言学" iconsizing="both"><!--[--><i class="vp-icon iconfont icon-alias" sizing="both"></i><!--]-->语料库和语言学<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/zh/NLP/04.html" aria-label="语言模型" iconsizing="both"><!--[--><i class="vp-icon iconfont icon-alias" sizing="both"></i><!--]-->语言模型<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/zh/NLP/question02.html" aria-label="语言模型练习题" iconsizing="both"><!--[--><i class="vp-icon iconfont icon-alias" sizing="both"></i><!--]-->语言模型练习题<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">OOP</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Python1v1</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Software Design</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">TOEFL</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Wuhan Uni</span><span class="vp-arrow end"></span></button><!----></section></li><li><a class="route-link auto-link vp-sidebar-link" href="/zh/intro.html" aria-label="介绍页" iconsizing="both"><!--[--><i class="vp-icon iconfont icon-circle-info" sizing="both"></i><!--]-->介绍页<!----></a></li></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><i class="vp-icon iconfont icon-alias" sizing="height"></i>基于BERT的分词模型</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">XiaoXianYue</span></span><span property="author" content="XiaoXianYue"></span></span><span class="page-original-info">原创</span><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span data-allow-mismatch="text">2025年4月7日</span><meta property="datePublished" content="2025-04-07T16:57:04.000Z"></span><!----><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 7 分钟</span><meta property="timeRequired" content="PT7M"></span><span class="page-category-info" aria-label="分类🌈" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon" name="category"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item color0 clickable" role="navigation">大三下</span><span class="page-category-item color2 clickable" role="navigation">NLP</span><!--]--><meta property="articleSection" content="大三下,NLP"></span><span class="page-tag-info" aria-label="标签🏷" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon" name="tag"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item color0 clickable" role="navigation">大三下</span><span class="page-tag-item color2 clickable" role="navigation">NLP</span><!--]--><meta property="keywords" content="大三下,NLP"></span></div><hr></div><!----><!----><div class="theme-hope-content" vp-content><figure><img src="/assets/image-20250414113933731-BtDrn5_P.png" alt="image-20250414113933731" tabindex="0" loading="lazy"><figcaption>image-20250414113933731</figcaption></figure><h2 id="_1-experiment-design" tabindex="-1"><a class="header-anchor" href="#_1-experiment-design"><span>1. Experiment Design</span></a></h2><p>Since Chinese lacks clear word boundaries, word segmentation is crucial in natural language processing (NLP). This article explores how to implement Chinese word segmentation tasks based on the BERT model.</p><p>This experiment uses BERT&#39;s context modeling to improve word segmentation accuracy. Unlike traditional methods, BERT can consider contextual information before and after words at the same time. BERT is a mature word segmentation model that has been trained on a large amount of corpus. The goal of this experiment is to fine-tune the BERT-based model, evaluate it on a benchmark dataset, and show the advantages of the pre-trained model in this task.</p><h3 id="_1-1-experiment-procedure" tabindex="-1"><a class="header-anchor" href="#_1-1-experiment-procedure"><span>1.1 Experiment Procedure</span></a></h3><p>The experiment follows the following key steps, each corresponding to a key implementation phase:</p><ul><li>**This module content shows the code version without the ablation study part (taken from the file <code>BERT.ipynb</code>). The version including the ablation study part is explained in detail in the ablation study section. **</li></ul><h4 id="_1-1-1-data-preprocessing" tabindex="-1"><a class="header-anchor" href="#_1-1-1-data-preprocessing"><span>1.1.1 Data Preprocessing</span></a></h4><img src="/assets/image-20250410165533644-DgJ15ziw.png" alt="image-20250410165533644" style="zoom:50%;"><ul><li>Using the icwb2-pku dataset, each word has four labels for annotation: <strong>B</strong> represents (start), <strong>M</strong> represents (middle), <strong>E</strong> represents (end), <strong>S</strong> represents (single word). Below is the cell that imports the dataset.</li></ul><figure><img src="/assets/image-20250414082134433-GruQH3ub.png" alt="image-20250414082134433" tabindex="0" loading="lazy"><figcaption>image-20250414082134433</figcaption></figure><ul><li>Output an example:</li></ul><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="background-color:#272822;color:#F8F8F2;"><pre class="shiki monokai vp-code"><code><span class="line"><span style="color:#F8F8F2;">{</span><span style="color:#E6DB74;">&#39;text&#39;</span><span style="color:#F8F8F2;">: [</span><span style="color:#E6DB74;">&#39;共 同 创 造 美 好 的 新 世 纪 — — 二 ○ ○ 一 年 新 年 贺 词&#39;</span><span style="color:#F8F8F2;">, </span><span style="color:#E6DB74;">&#39;( 二 ○ ○ ○ 年 十 二 月 三 十 一 日 ) ( 附 图 片 1 张 )&#39;</span><span style="color:#F8F8F2;">, </span><span style="color:#E6DB74;">&#39;女 士 们 , 先 生 们 , 同 志 们 , 朋 友 们 :&#39;</span><span style="color:#F8F8F2;">], </span><span style="color:#E6DB74;">&#39;label&#39;</span><span style="color:#F8F8F2;">: [</span><span style="color:#E6DB74;">&#39;B E B E B E S S B E B E B M M M E B E B E&#39;</span><span style="color:#F8F8F2;">, </span><span style="color:#E6DB74;">&#39;S B M M M E B M E B M M E S S S B E S S S&#39;</span><span style="color:#F8F8F2;">, </span><span style="color:#E6DB74;">&#39;B E S S B E S S B E S S B E S S&#39;</span><span style="color:#F8F8F2;">]}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h4 id="_1-1-2-tokenization" tabindex="-1"><a class="header-anchor" href="#_1-1-2-tokenization"><span>1.1.2 Tokenization</span></a></h4><ul><li>BERT&#39;s WordPiece tokenizer encodes each Chinese character. In this experiment, each Chinese character is usually regarded as an independent token.</li></ul><img src="/assets/image-20250414082651504-Do2xSo_I.png" alt="image-20250414082651504" style="zoom:33%;"><ul><li>Output an example: “中文分词是一项重要的自然语言处理领域任务”</li></ul><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="background-color:#272822;color:#F8F8F2;"><pre class="shiki monokai vp-code"><code><span class="line"><span style="color:#F8F8F2;">{</span><span style="color:#E6DB74;">&#39;input_ids&#39;</span><span style="color:#F8F8F2;">: [</span><span style="color:#AE81FF;">101</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">704</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">3152</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">1146</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">6404</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">3221</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">671</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">7555</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">7028</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">6206</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">4638</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">5632</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">4197</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">6427</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">6241</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">1905</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">4415</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">7566</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">1818</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">818</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">1218</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">102</span><span style="color:#F8F8F2;">], </span><span style="color:#E6DB74;">&#39;token_type_ids&#39;</span><span style="color:#F8F8F2;">: [</span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">, </span><span style="color:#AE81FF;">0</span><span style="color:#F8F8F2;">]}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>Optionally keep empty labels for rare subwords. Add tokens [CLS] and [SEP] at the beginning and end of the sequence, and add placeholder labels for alignment. The final input format includes <code>input_ids</code>, <code>attention_mask</code>, and <code>label_ids</code>.</li></ul><img src="/assets/image-20250414084333595-Ck2CRDQ_.png" alt="image-20250414084333595" style="zoom:33%;"><h4 id="_1-1-3-model-construction" tabindex="-1"><a class="header-anchor" href="#_1-1-3-model-construction"><span>1.1.3 Model Construction</span></a></h4><ul><li>The pretrained BERT-Base Chinese model serves as the encoder,</li></ul><figure><img src="/assets/image-20250413154801349-C33g7p6I.png" alt="image-20250413154801349" tabindex="0" loading="lazy"><figcaption>image-20250413154801349</figcaption></figure><ul><li>topped with a linear classification layer to predict segmentation labels.</li></ul><img src="/assets/image-20250408171821717-D0xjvhX2.png" alt="image-20250408171821717" style="zoom:33%;"><ul><li>The model takes a token sequence as input, generates a contextual embedding for each token, and maps label probabilities via Softmax.</li></ul><img src="/assets/image-20250413154824922-BXblhPUC.png" alt="image-20250413154824922" style="zoom:50%;"><ul><li>Both BERT&#39;s parameters and the classifier&#39;s weights are fine-tuned during training.</li></ul><img src="/assets/image-20250414152920262-ngIzLyK1.png" alt="image-20250414152920262" style="zoom:50%;"><h4 id="_1-1-4-model-training" tabindex="-1"><a class="header-anchor" href="#_1-1-4-model-training"><span>1.1.4 Model Training</span></a></h4><img src="/assets/image-20250410165604371-DuGF6vY3.png" alt="image-20250410165604371" style="zoom:50%;"><ul><li>Cross-entropy loss is used, with the Adam optimizer (learning rate = 3e-5).</li></ul><img src="/assets/image-20250413155635822-Ct0QCBgE.png" alt="image-20250413155635822" style="zoom:50%;"><ul><li>Special tokens and padding positions are masked to focus training on valid character labels.</li><li>Training runs for multiple epochs, with validation metrics (Precision, Recall, F1) monitored to prevent overfitting.</li></ul><figure><img src="/assets/image-20250414153340381-DTWtmmMP.png" alt="image-20250414153340381" tabindex="0" loading="lazy"><figcaption>image-20250414153340381</figcaption></figure><h4 id="_1-1-5-model-evaluation" tabindex="-1"><a class="header-anchor" href="#_1-1-5-model-evaluation"><span>1.1.5 Model Evaluation</span></a></h4><ul><li>What the trained model does is to predict the label sequence of the test sentence and convert it into a segmented word sequence. This completes our word segmentation task.</li></ul><img src="/assets/image-20250408185009377-DpvhzFIs.png" alt="image-20250408185009377" style="zoom:33%;"><ul><li>The performance is measured by using parameters such as precision, recall, and F1 score.</li></ul><img src="/assets/image-20250408185225218-vqnM22ss.png" alt="image-20250408185225218" style="zoom:33%;"><h4 id="_1-1-6-model-prediction" tabindex="-1"><a class="header-anchor" href="#_1-1-6-model-prediction"><span>1.1.6 Model Prediction</span></a></h4><ul><li>As can be seen in the prediction module output, the model successfully splits any input sentence into space-delimited words, demonstrating its generalization ability.</li></ul><img src="/assets/image-20250408185451879-BV8nZeiv.png" alt="image-20250408185451879" style="zoom:33%;"><h2 id="_2-local-deployment" tabindex="-1"><a class="header-anchor" href="#_2-local-deployment"><span>2. Local Deployment</span></a></h2><p>Initially, the experimental prototype was run in a local CPU environment. However, due to the high computational overhead of the BERT model, training the full model on the CPU was extremely inefficient and training could not be completed after several hours. To improve performance, the experiment was migrated to the AutoDL platform that supports GPU acceleration. By leveraging the parallel computing power of GPUs, the training time was reduced from hours to minutes. During the migration process, the model was adapted to take advantage of GPU-based operations and parameters such as batch size were adjusted based on GPU memory limitations.</p><blockquote><p>Note: Part of screenshots of the platform during training have been placed in the Experimental Flow section shown above. Screenshot with imported GPU (see the first 3 cells of the .zip file for code details)</p></blockquote><img src="/assets/image-20250410165715402-BjsqLscU.png" alt="image-20250410165715402" style="zoom:50%;"><h2 id="_3-module-descriptions" tabindex="-1"><a class="header-anchor" href="#_3-module-descriptions"><span>3. Module Descriptions</span></a></h2><ul><li><p><strong>Tokenizer</strong>:</p><p>Use BERT&#39;s WordPiece tokenizer to split the sentence into subwords. If a character is not recognized by the model, it will be specially marked.</p></li><li><p><strong>Feature Construction</strong>:</p><p>Convert the tokenization results and labels into fixed-length tensors. This process includes adding [CLS] and [SEP] tags, mapping words and labels to IDs, and finally generating input_ids, attention_mask, and label_ids.</p></li><li><p><strong>Data Loader</strong>:</p><p>Used to batch and shuffle the training data for more efficient training on GPUs.</p></li><li><p><strong>Model Architecture</strong>:</p><p>Use BERT as the encoder and connect its output word vector to a linear classifier to predict the label of each word.</p></li><li><p><strong>Loss Function</strong>:</p><p>Use the masked cross entropy loss function to ignore the impact of special symbols (such as [CLS], [SEP]) and padding positions on model training.</p></li><li><p><strong>Evaluation function</strong>:</p><p>Compare the model prediction results with the true labels and calculate the precision, recall and F1 score to evaluate the performance of the model on the sequence labeling task.</p></li></ul><h2 id="_4-experiment-results" tabindex="-1"><a class="header-anchor" href="#_4-experiment-results"><span>4. Experiment Results</span></a></h2><blockquote><p>The image below shows the training results of the original experiment. A comprehensive comparison with the ablation experiment results is provided in the ablation study section.</p></blockquote><img src="/assets/image-20250407164849422-BqTfWQY6.png" alt="image-20250407164849422" style="zoom:33%;"><img src="/assets/image-20250407164903944-DzKlqnQ5.png" alt="image-20250407164903944" style="zoom:33%;"><p><strong>Training Dynamics</strong>:</p><ul><li>The training loss curve dropped to about 0.1 after the third round of training and remained stable, indicating that the model convergence was effective. This monotonous downward trend reflects the model&#39;s increasing ability to learn Chinese word formation patterns.</li><li>The validation set F1 score rose rapidly to about 94% after the first round of training, and approached 97.8% after the third round of training, and then stabilized, indicating that the model was fine-tuned and converged successfully.</li></ul><h2 id="_5-conclusion" tabindex="-1"><a class="header-anchor" href="#_5-conclusion"><span>5. Conclusion</span></a></h2><p>This experiment successfully developed a Chinese word segmentation model based on BERT and achieved near-optimal performance (F1 ≈ 98%). BERT&#39;s context vector effectively solves ambiguous and out-of-vocabulary problems, surpassing traditional methods.</p><h2 id="_6-ablation-study" tabindex="-1"><a class="header-anchor" href="#_6-ablation-study"><span>6. Ablation study</span></a></h2><p>In order to further explore the impact of some model parameters on model performance, this paper selects the dropout rate in the model parameters and designs an ablation experiment. Dropout is a common regularization technique that refers to randomly deactivating some neurons during training to prevent overfitting. Although BERT usually uses the default dropout rate of 0.1, this paper studies whether this value is the optimal value and the impact of changing this value on word segmentation performance.</p><h3 id="_6-1-experiment-design" tabindex="-1"><a class="header-anchor" href="#_6-1-experiment-design"><span>6.1 Experiment Design</span></a></h3><p>This study increased the dropout rate from 0.1 to 0.5, which means that 50% of neurons are randomly discarded during each forward propagation. The hidden layer dropout rate (&quot;hidden_dropout_prob&quot;) and the attention mechanism dropout rate (&quot;attention_probs_dropout_prob&quot;) were adjusted in the code. Both values were set to 0.1 by default in the original experiment.</p><p>To control variables, the two models used the same training steps, evaluation process, and test set.</p><h3 id="_6-2-code-modifications" tabindex="-1"><a class="header-anchor" href="#_6-2-code-modifications"><span>6.2 Code Modifications</span></a></h3><p><strong>This module presents the added and modified code from the file <code>BERT.ipynb</code> to <code>BERT_ablation.ipynb</code>, where ablation experiments were integrated into the training process to compare experimental results.</strong></p><h4 id="_6-2-1-modified-dropout-parameters" tabindex="-1"><a class="header-anchor" href="#_6-2-1-modified-dropout-parameters"><span>6.2.1 Modified Dropout Parameters</span></a></h4><ul><li>The original model used the default BERT dropout rate (about 0.1).</li><li>In the ablation experiment, the dropout rate was increased to 0.5. This was implemented with the following code:</li></ul><p>cell[22]</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="background-color:#272822;color:#F8F8F2;"><pre class="shiki monokai vp-code"><code><span class="line"><span style="color:#F8F8F2;">higher_dropout_rate </span><span style="color:#F92672;">=</span><span style="color:#AE81FF;"> 0.5</span><span style="color:#88846F;">  # 原始BERT的dropout率通常为0.1</span></span>
<span class="line"><span style="color:#F8F8F2;">ablation_model </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> AutoModelForTokenClassification.from_pretrained(</span></span>
<span class="line"><span style="color:#F8F8F2;">    model_name, </span></span>
<span class="line"><span style="color:#FD971F;font-style:italic;">    num_classes</span><span style="color:#F92672;">=</span><span style="color:#AE81FF;">5</span><span style="color:#F8F8F2;">, </span></span>
<span class="line"><span style="color:#FD971F;font-style:italic;">    hidden_dropout_prob</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">higher_dropout_rate,  </span><span style="color:#88846F;"># 修改隐藏层dropout率</span></span>
<span class="line"><span style="color:#FD971F;font-style:italic;">    attention_probs_dropout_prob</span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;">higher_dropout_rate  </span><span style="color:#88846F;"># 修改注意力机制dropout率</span></span>
<span class="line"><span style="color:#F8F8F2;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><img src="/assets/image-20250413163416906-CQHpTqwO.png" alt="image-20250413163416906" style="zoom:33%;"><h4 id="_6-2-2-modified-training-function" tabindex="-1"><a class="header-anchor" href="#_6-2-2-modified-training-function"><span>6.2.2 Modified Training Function</span></a></h4><ul><li>The modified training function ensures that both the original and ablation models use the same inputs, with the only difference being the dropout rate inside the model.</li></ul><img src="/assets/image-20250413163537980-D0KltEKE.png" alt="image-20250413163537980" style="zoom:33%;"><h4 id="_6-2-3-use-the-same-evaluation-function-to-ensure-a-single-variable" tabindex="-1"><a class="header-anchor" href="#_6-2-3-use-the-same-evaluation-function-to-ensure-a-single-variable"><span>6.2.3 Use the same evaluation function to ensure a single variable</span></a></h4><ul><li>Since the input format was not changed, a specialized evaluation function for the ablation model was no longer needed.</li><li>The original evaluation function can be reused to evaluate both models.</li></ul><h4 id="_6-2-4-added-garbage-collection" tabindex="-1"><a class="header-anchor" href="#_6-2-4-added-garbage-collection"><span>6.2.4 Added Garbage Collection</span></a></h4><ul><li>During training, garbage collection is performed every 100 steps.</li><li>It is also triggered once at the end of training.</li></ul><img src="/assets/image-20250413163831390-C7NMZIpY.png" alt="image-20250413163831390" style="zoom:33%;"><h4 id="_6-2-5-added-model-performance-and-segmentation-comparison" tabindex="-1"><a class="header-anchor" href="#_6-2-5-added-model-performance-and-segmentation-comparison"><span>6.2.5 Added Model Performance and Segmentation Comparison</span></a></h4><ul><li>The same test data was used to evaluate both the original and ablation models.</li><li>Word segmentation results were compared across multiple example sentences.</li></ul><img src="/assets/image-20250413235940299-O3OefeVS.png" alt="image-20250413235940299" style="zoom:33%;"><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="background-color:#272822;color:#F8F8F2;"><pre class="shiki monokai vp-code"><code><span class="line"><span style="color:#F8F8F2;">test_texts </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> [</span></span>
<span class="line"><span style="color:#E6DB74;">    &quot;北京大学是中国最著名的高等学府之一&quot;</span><span style="color:#F8F8F2;">,</span></span>
<span class="line"><span style="color:#E6DB74;">    &quot;今天天气真好我们一起去公园散步吧&quot;</span><span style="color:#F8F8F2;">,</span></span>
<span class="line"><span style="color:#E6DB74;">    &quot;人工智能技术正在快速发展并改变我们的生活方式&quot;</span></span>
<span class="line"><span style="color:#F8F8F2;">]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F92672;">for</span><span style="color:#F8F8F2;"> i, test_text </span><span style="color:#F92672;">in</span><span style="color:#66D9EF;"> enumerate</span><span style="color:#F8F8F2;">(test_texts):</span></span>
<span class="line"><span style="color:#F8F8F2;">    original_result </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> infer(model, test_text, tokenizer, id2label)</span></span>
<span class="line"><span style="color:#F8F8F2;">    ablation_result </span><span style="color:#F92672;">=</span><span style="color:#F8F8F2;"> infer(ablation_model, test_text, tokenizer, id2label)</span></span>
<span class="line"><span style="color:#88846F;">    # 打印对比结果</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><img src="/assets/image-20250413163954522-vjfPm6c3.png" alt="image-20250413163954522" style="zoom:33%;"><h4 id="_6-2-6-added-time-recording-and-comparison" tabindex="-1"><a class="header-anchor" href="#_6-2-6-added-time-recording-and-comparison"><span>6.2.6 Added Time Recording and Comparison</span></a></h4><ul><li><p>Recorded training time for both the original model and the ablation model.</p></li><li><p>Compared time differences between the two at the end of the experiment.</p></li><li><p>This is a partial screenshot of the running results of the model trained using GPU on the AutoDL platform.</p></li></ul><img src="/assets/image-20250413235740928-DCkcqQ1k.png" alt="image-20250413235740928" style="zoom:33%;"><img src="/assets/image-20250413235811537-DAkLV6yO.png" alt="image-20250413235811537" style="zoom:33%;"><img src="/assets/image-20250414155008670-BdiyGzun.png" alt="image-20250414155008670" style="zoom:50%;"><h3 id="_6-3-ablation-experiment-results" tabindex="-1"><a class="header-anchor" href="#_6-3-ablation-experiment-results"><span>6.3 Ablation Experiment Results</span></a></h3><ul><li><p>The training curves clearly demonstrate the negative impact of an excessively high dropout rate on model performance. As shown in the first plot, the low-dropout model (in pink) rapidly converges, with training loss dropping close to zero. In contrast, the high-dropout model (in blue) maintains a consistently high loss throughout training, suggesting that learning is hindered.</p></li><li><p>The second plot further confirms this observation, showing that the low-dropout model steadily improves and maintains a high development score, while the high-dropout model fails to achieve meaningful performance gains. These results highlight the importance of properly tuning the dropout rate to ensure effective training and generalization.</p></li></ul><img src="/assets/image-20250413235639985-d--DdIed.png" alt="image-20250413235639985" style="zoom:50%;"><img src="/assets/image-20250413235655792-2ZlaIx98.png" alt="image-20250413235655792" style="zoom:50%;"><h3 id="_6-4-analysis-and-discussion" tabindex="-1"><a class="header-anchor" href="#_6-4-analysis-and-discussion"><span>6.4 Analysis and Discussion</span></a></h3><p>The results of this study reveal the role of dropout settings in the fine-tuning process. A higher dropout rate (0.5) discards too much contextual information, while excessive regularization during fine-tuning hinders the model from learning contextual connections, resulting in underfitting and reduced generalization ability.</p><p>While dropout can prevent overfitting, this study shows that in a transfer learning setting like fine-tuning BERT, milder and smaller regularization is often more effective in retaining pre-training knowledge.</p><h2 id="_7-references" tabindex="-1"><a class="header-anchor" href="#_7-references"><span>7. References</span></a></h2><ol><li>AI Studio. (2025). <em>基于BERT实现中文分词</em> [BERT-based Chinese Word Segmentation]. Retrieved from <a href="https://aistudio.baidu.com/projectdetail/9002250" target="_blank" rel="noopener noreferrer">https://aistudio.baidu.com/projectdetail/9002250</a></li><li>Emerson, T. (2005). <em>The Second International Chinese Word Segmentation Bakeoff</em>. In <em>Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing</em> (pp. 123–133).</li></ol></div><!----><footer class="vp-page-meta"><!----><div class="vp-meta-item git-info"><div class="update-time"><span class="vp-meta-label">上次编辑于: </span><span class="vp-meta-info" data-allow-mismatch="text">2025/7/21 01:25:32</span></div><div class="contributors"><span class="vp-meta-label">贡献者: </span><!--[--><!--[--><span class="vp-meta-info" title="email: 2310219843@qq.com">Xiaoxianyue</span><!--]--><!--]--></div></div></footer><nav class="vp-page-nav"><a class="route-link auto-link prev" href="/zh/NLP/05.html" aria-label="图模型" iconsizing="both"><div class="hint"><span class="arrow start"></span>上一页</div><div class="link"><i class="vp-icon iconfont icon-alias" sizing="height"></i>图模型</div></a><a class="route-link auto-link next" href="/zh/NLP/Review_question.html" aria-label="复习问题" iconsizing="both"><div class="hint">下一页<span class="arrow end"></span></div><div class="link">复习问题<i class="vp-icon iconfont icon-alias" sizing="height"></i></div></a></nav><!----><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper" vp-footer><div class="vp-footer">默认页脚</div><div class="vp-copyright">Copyright © 2025 XiaoXianYue </div></footer></div><!--]--><!--[--><!----><!--[--><!--]--><!--]--><!--]--></div>
    <script type="module" src="/assets/app-84lBMjzT.js" defer></script>
  </body>
</html>
